Non-linear Metric Learning

Dor Kedem, Stephen Tyree, Kilian Q. Weinberger
Dept. of Comp. Sci. & Engi.
Washington U.
St. Louis, MO 63130
kedem.dor,swtyree,kilian@wustl.edu

Fei Sha
Dept. of Comp. Sci.
U. of Southern California
Los Angeles, CA 90089
feisha@usc.edu

Gert Lanckriet
Dept. of Elec. & Comp. Engineering
U. of California
La Jolla, CA 92093
gert@ece.ucsd.edu

Abstract
In this paper, we introduce two novel metric learning algorithms, χ2 -LMNN and
GB-LMNN, which are explicitly designed to be non-linear and easy-to-use. The
two approaches achieve this goal in fundamentally different ways: χ2 -LMNN
inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear χ2 -distance to explicitly capture similarities within histogram data sets; GB-LMNN applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach’s robustness, speed, parallelizability and insensitivity towards the single additional hyperparameter. On various benchmark data sets, we demonstrate these methods not
only match the current state-of-the-art in terms of kNN classification error, but in
the case of χ2 -LMNN, obtain best results in 19 out of 20 learning settings.

1

Introduction

How to compare examples is a fundamental question in machine learning. If an algorithm could
perfectly determine whether two examples were semantically similar or dissimilar, most subsequent
machine learning tasks would become trivial (i.e., a nearest neighbor classifier will achieve perfect
results). Guided by this motivation, a surge of recent research [10, 13, 15, 24, 31, 32] has focused on
Mahalanobis metric learning. The resulting methods greatly improve the performance of metric dependent algorithms, such as k-means clustering and kNN classification, and have gained popularity
in many research areas and applications within and beyond machine learning.
One reason for this success is the out-of-the-box usability and robustness of several popular methods
to learn these linear metrics. So far, non-linear approaches [6, 18, 26, 30] to metric learning have
not managed to replicate this success. Although more expressive, the optimization problems are
often expensive to solve and plagued by sensitivity to many hyper-parameters. Ideally, we would
like to develop easy-to-use black-box algorithms that learn new data representations for the use
of established metrics. Further, non-linear transformations should be applied depending on the
specifics of a given data set.
In this paper, we introduce two novel extensions to the popular Large Margin Nearest Neighbors
(LMNN) framework [31] which provide non-linear capabilities and are applicable out-of-the-box.
The two algorithms follow different approaches to achieve this goal:
(i) Our first algorithm, χ2 -LMNN is specialized for histogram data. It generalizes the non-linear
χ2 -distance and learns a metric that strictly preserve the histogram properties of input data on a
probability simplex. It successfully combines the simplicity and elegance of the LMNN objective
and the domain-specific expressiveness of the χ2 -distance.
1

(ii) Our second algorithm, gradient boosted LMNN (GB-LMNN) employs a non-linear mapping
combined with a traditional Euclidean distance function. It is a natural extension of LMNN from
linear to non-linear mappings. By training the non-linear transformation directly in function space
with gradient-boosted regression trees (GBRT) [11] the resulting algorithm inherits the positive
aspects of GBRT—its insensitivity to hyper-parameters, robustness against overfitting, speed and
natural parallelizability [28].
Both approaches scale naturally to medium-sized data sets, can be optimized using standard techniques and only introduce a single additional hyper-parameter. We demonstrate the efficacy of both
algorithms on several real-world data sets and observe two noticeable trends: i) GB-LMNN (with
default settings) achieves state-of-the-art k-nearest neighbor classification errors with high consistency across all our data sets. For learning tasks where non-linearity is not required, it reduces to
LMNN as a special case. On more complex data sets it reliably improves over linear metrics and
matches or out-performs previous work on non-linear metric learning. ii) For data sampled from a
simplex, χ2 -LMNN is strongly superior to alternative approaches that do not explicitly incorporate
the histogram aspect of the data—in fact it obtains best results in 19/20 learning settings.

2

Background and Notation

Let {(x1 , y1 ), . . . , (xn , yn )} ∈ Rd ×C be labeled training data with discrete labels C = {1, . . . , c}.
Large margin nearest neighbors (LMNN) [30, 31] is an algorithm to learn a Mahalanobis metric
specifically to improve the classification error of k-nearest neighbors (kNN) [7] classification. As
the kNN rule relies heavily on the underlying metric (a test input is classified by a majority vote
amongst its k nearest neighbors), it is a good indicator for the quality of the metric in use. The
Mahalanobis metric can be viewed as a straight-forward generalization of the Euclidean metric,
DL (xi , xj ) = kL(xi − xj )k2 ,
(1)
parameterized by a matrix L ∈ Rd×d , which in the case of LMNN is learned such that the linear
transformation x → Lx better represents similarity in the target domain. In the remainder of this
section we briefly review the necessary terminology and basic framework behind LMNN and refer
the interested reader to [31] for more details.
Local neighborhoods. LMNN identifies two types of neighborhood relations between an input
xi and other inputs in the data set: For each xi , as a first step, k dedicated target neighbors are
identified prior to learning. These are the inputs that should ideally be the actual nearest neighbors
after applying the transformation (we use the notation j i to indicate that xj is a target neighbor
of xi ). A common heuristic for choosing target neighbors is picking the k closest inputs (according
to the Euclidean distance) to a given xi within the same class. The second type of neighbors are
impostors. These are inputs that should not be among the k-nearest neighbors — defined to be all
inputs from a different class that are within the local neighborhood of xi .
LMNN optimization. The LMNN objective has two terms, one for each neighborhood objective:
First, it reduces the distance between an instance and its target neighbors, thus pulling them closer
and making the input’s local neighborhood smaller. Second, it moves impostor neighbors (i.e.,
differently labeled inputs) farther away so that the distances to impostors should exceed the distances
to target neighbors by a large margin. Weinberger et. al [31] combine these two objectives into a
single unconstrained optimization problem:
X
X

min
DL (xi , xj )2
(2)
1 + DL (xi , xj )2 − DL (xi , xk )2 +
+ µ
L
|
{z
}
i,j:j i
k : yi 6=yk
pull target neighbor xj closer
|
{z
}
push impostor xk away, beyond target neighbor xj by a large margin `

The parameter µ defines a trade-off between the two objectives and [x]+ is defined as the hinge-loss
[x]+ = max(0, x). The optimization (2) can be transformed into a semidefinite program (SDP) [31]
for which a global solution can be found efficiently. The large margin in (2) is set to 1 as its exact
value only impacts the scale of L and not the kNN classifier.
Dimensionality reduction. As an extension to the original LMNN formulation, [26, 30] show that
with L ∈ Rr×d with r < d, LMNN learns a projection into a lower-dimensional space Rr that still
represents domain specific similarities. While this low-rank constraint breaks the convexity of the
optimization problem, significant speed-ups [30] can be obtained when the kNN classifier is applied
in the r-dimensional space — especially when combined with special-purpose data structures [33].
2

3

χ2 -LMNN: Non-linear Distance Functions on the Probability Simplex

The original LMNN algorithm learns a linear transformation L ∈ Rd×d that captures semantic similarity for kNN
classification on data in some Euclidean vector space
Rd . In this section we extend this formulation to settings in which data are sampled from a probability simplex S d = {x ∈ Rd |x ≥ 0, x> 1 = 1}, where 1 ∈ Rd denotes the vector of all-ones. Each input xi ∈ S d can be
interpreted as a histogram over d buckets. Such data are
ubiquitous in computer vision where the histograms can
be distributions over visual codebooks [27] or colors [25],
in text-data as normalized bag-of-words or topic assign- Figure2 1: A schematic illustration of
the χ -LMNN optimization. The mapments [3], and many other fields [9, 17, 21].
ping is constrained to preserve all inputs
Histogram distances. The abundance of such data has on the simplex S 3 (grey surface). The
sparked the development of several specialized distance arrows indicate the push (red and yelmetrics designed to compare histograms. Examples are low) and pull (blue) forces from the χ2 Quadratic-Form distance [16], Earth Mover’s Distance LMNN objective.
[21], Quadratic-Chi distance family [20] and χ2 histogram distance [16]. We focus explicitly on the latter. Transforming the inputs with a linear
transformation learned with LMNN will almost certainly result in a loss of their histogram properties — and the ability to use such distances. In this section, we introduce our first non-linear
extension for LMNN, to address this issue. In particular, we propose two significant changes to the
original LMNN formulation: i) we learn a constrained mapping that keeps the transformed data on
the simplex (illustrated in Figure 1), and ii) we optimize the kNN classification performance with
respect to the non-linear χ2 histogram distance directly.
χ2 histogram distance. We focus on the χ2 histogram distance, whose origin is the χ2 statistical
hypothesis test [19], and which has successfully been applied in many domains [8, 27, 29]. The χ2
distance is a bin-to-bin distance measurement, which takes into account the size of the bins and their
differences. Formally, the χ2 distance is a well-defined metric χ2 : S d → [0, 1] defined as [20]
χ2 (xi , xj ) =

d
1 X ([xi ]f − [xj ]f )2
,
2
[xi ]f + [xj ]f

(3)

f =1

where [xi ]f indicates the f th feature value of the vector xi .
Generalized χ2 distance. First, analogous to the generalized Euclidean metric in (1), we generalize
the χ2 distance with a linear transformation and introduce the pseudo-metric χ2L (xi , xj ), defined as
χ2L (xi , xj ) = χ2 (Lxi , Lxj ).

(4)

The χ2 distance is only a well-defined metric within the simplex S d and therefore we constrain
L to map any x onto S d . We define the set of such simplex-preserving linear transformations as
P = {L ∈ Rd×d : ∀x ∈ S d , Lx ∈ S d }.
χ2 -LMNN Objective. To optimize the transformation L with respect to the χ2 histogram distance
directly, we replace the Mahalanobis distance DL in (2) with χ2L and obtain the following:
X
X 

min
χ2L (xi , xj ) + µ
` + χ2L (xi , xj ) − χ2L (xi , xk ) + .
(5)
L∈P

i,j: j

i

k: yi 6=yk

Besides the substituted distance function, there are two important changes in the optimization problem (5) compared to (2). First, as mentioned before, we have an additional constraint L ∈ P. Second,
because (4) is not linear in L> L, different values for the margin parameter ` lead to truly different
solutions (which differ not just up to a scaling factor as before). We therefore can no longer arbitrarily set ` = 1. Instead, ` becomes an additional hyper-parameter of the model. We refer to this
algorithm as χ2 -LMNN.
Optimization. To learn (5), it can be shownP
L ∈ P if and only if L is element-wise non-negative, i.e.,
L ≥ 0, and each column is normalized, i.e., i Lij = 1, ∀j. These constraints are linear with respect
3

to L and we can optimize (5) efficiently with a projected sub-gradient method [2]. As an even faster
optimization method, we propose a simple change of variables to generate an unconstrained version
of (5). Let us define f : Rd×d → P to be the column-wise soft-max operator
eAij
[f (A)]ij = P Akj .
ke

(6)

By design, all columns of f (A) are normalized and every matrix entry is non-negative. The function
f (·) is continuous and differentiable. By defining L = f (A) we obtain L ∈ P for any choice of A ∈
Rd×d . This allows us to minimize (5) with respect to A using unconstrained sub-gradient descent1 .
We initialize the optimization with A = 10 I + 0.01 11> (where I denotes the identity matrix) to
approximate the non-transformed χ2 histogram distance after the change of variable (f (A) ≈ I).
Dimensionality Reduction. Analogous to the original LMNN formulation (described in Section 2),
we can restrict from a square matrix to L ∈ Rr×d with r < d. In this case χ2 -LMNN learns a
projection into a lower dimensional simplex L : S d → S r . All other parts of the algorithm change
analogously. This extension can be very valuable to enable faster nearest neighbor search [33]
especially for time-sensitive applications, e.g., object recognition tasks in computer vision [27]. In
section 6 we evaluate this version of χ2 -LMNN under a range of settings for r.

4

GB-LMNN: Non-linear Transformations with Gradient Boosting

Whereas section 3 focuses on the learning scenario where a linear transformation is too general, in
this section we explore the opposite case where it is too restrictive. Affine transformations preserve
collinearity and ratios of distances along lines — i.e., inputs on a straight line remain on a straight
line and their relative distances are preserved. This can be too restrictive for data where similarities
change locally (e.g., because similar data lie on non-linear sub-manifolds). Chopra et al. [6] pioneered non-linear metric learning, using convolutional neural networks to learn embeddings for faceverification tasks. Inspired by their work, we propose to optimize the LMNN objective (2) directly
in function space with gradient boosted CART trees [11]. Combining the learned transformation
φ(x) : Rd → Rd with a Euclidean distance function has the capability to capture highly non-linear
similarity relations. It can be optimized using standard techniques, naturally scales to large data sets
while only introducing a single additional hyper-parameter in comparison with LMNN.
Generalized LMNN. To generalize the LMNN objective 2 to a non-linear transformation φ(·), we
denote the Euclidean distance after the transformation as
Dφ (xi , xj ) = kφ(xi ) − φ(xj )k2 ,

(7)

which satisfies all properties of a well-defined pseudo-metric in the original input space. To optimize
the LMNN objective directly with respect to Dφ , we follow the same steps as in Section 3 and
substitute Dφ for DL in (2). The resulting unconstrained loss function becomes
X
X

L(φ) =
1 + kφ(xi )−φ(xj )k22 − kφ(xi )−φ(xk )k22 + . (8)
kφ(xi )−φ(xj )k22 + µ
i,j: j

i

k: yi 6=yk

In its most general form, with an unspecified mapping φ, (8) unifies most of the existing variations of
LMNN metric learning. The original linear LMNN mapping [31] is a special case where φ(x) = Lx.
Kernelized versions [5, 12, 26] are captured by φ(x) = Lψ(x), producing the kernel K(xi , xj ) =
φ(xi )> φ(xj ) = ψ(xi )> L> Lψ(xj ). The embedding of Globerson and Roweis [14] corresponds to
the most expressive mapping function φ(xi ) = zi , where each input xi is transformed independently
to a new location zi to satisfy similarity constraints — without out-of-sample extensions.
GB-LMNN. The previous examples vary widely in expressiveness, scalability, and generalization,
largely as a consequence of the mapping function φ. It is important to find the right non-linear form
for φ, and we believe an elegant solution lies in gradient boosted regression trees.
Our method, termed GB-LMNN, learns a global non-linear mapping. The construction of the mapping, an ensemble of multivariate regression trees selected by gradient boosting [11], minimizes the
general LMNN objective (8) directly in function space. Formally, the GB-LMNN transformation
1
The set of all possible matrices f (A) is slightly more restricted than P, as it reaches zero entries only in
the limit. However, given finite computational precision, this does not seem to be a problem in practice.

4

True
Gradient
Approximated
Gradient
Itera/on	  1

Itera/on	  10

Itera/on	  20

Itera/on	  40

Itera/on	  100

Figure 2: GB-LMNN illustrated on a toy data set sampled from two concentric circles of different
classes (blue and red dots). The figure depicts the true gradient (top row) with respect to each input
and its least squares approximation (bottom row) with a multi-variate regression tree (depth, p = 4).
PT
is an additive function φ = φ0 + α t=1 ht initialized by φ0 and constructed by iteratively adding
regression trees ht of limited depth p [4], each weighted by a learning rate α. Individually, the trees
are weak learners and are capable of learning only simple functions, but additively they form powerful ensembles with good generalization to out-of-sample data. In iteration t, the tree ht is selected
greedily to best minimize the objective upon its addition to the ensemble,
φt (·) = φt−1 (·) + αht (·), where ht ≈ argmin L(φt−1 + αh).

(9)

h∈T p

Here, T p denotes the set of all regression trees of depth p. The (approximately) optimal tree ht is
found by a first-order Taylor approximation of L. This makes the optimization akin to a steepest
descent step in function space, where ht is selected to approximate the negative gradient gt of the
objective L(φt−1 ) with respect to the transformation learned at the previous iteration φt−1 . Since
we learn an approximation of gt as a function of the training data, sub-gradients are computed with
respect to each training input xi , and approximated by the tree ht (·) in the least-squared sense,
ht (·) = argmin
h∈T p

n
X
∂L(φt−1 )
.
(gt (xi ) − ht (xi ))2 , where: gt (xi ) =
∂φ
t−1 (xi )
i=1

(10)

Intuitively, at each iteration, the tree ht (·) of depth p splits the input space into 2p axis-aligned
regions. All inputs that fall into one region are translated by a constant vector — consequently,
the inputs in different regions are shifted in different directions. We learn the trees greedily with a
modified version of the public-domain CART implementation pGBRT [28].
Optimization details. Since (8) is non-convex with respect to φ, we initialize with the linear
transformation learned by LMNN, φ0 = Lx, making our method a non-linear refinement of LMNN.
The only additional hyperparameter to the optimization is the maximum tree depth p to which the
algorithm is not particularly sensitive (we set p = 6). 2
Figure 2 depicts a simple toy-example with concentric circles of inputs from two different classes.
By design, the inputs are sampled such that the nearest neighbor for any given input is from the
other class. A linear transformation is incapable of separating the two classes. However GB-LMNN
produces a mapping with the desired separation. The figure illustrates the actual gradient (top row)
and its approximation (bottom row). The limited-depth regression trees are unable to capture the
gradient for all inputs in a single iteration. But by greedily focusing on inputs with the largest
gradients or groups of inputs with the most easily encoded gradients, the gradient boosting process
additively constructs the transformation function. At iteration 100, the gradients with respect to
most inputs vanish, indicating that a local minimum of L(φ) is almost reached — the inputs from
the two classes are separated by a large margin.
2

Here, we set the step-size, a common hyper-parameter across all variations of LMNN, to α = 0.01.

5

Dimensionality reduction. Like linear LMNN and χ2 -LMNN, it is possible to learn a non-linear
transformation to a lower dimensional space, φ(x) : Rd → Rr , r ≤ d. Initialization is made with
the rectangular matrix output of the dimensionality-reduced LMNN transformation, φ0 = Lx with
L ∈ Rr×d . Training proceeds by learning trees with r- rather than d-dimensional outputs.

5

Related Work

There have been previous attempts to generalize learning linear distances to nonlinear metrics. The
nonlinear mapping φ(x) of eq. (7) can be implemented with kernels [5, 12, 18, 26]. These extensions have the advantages of maintaining computational tractability as convex optimization problems. However, their utility is limited inherently by the sizes of kernel matrices .Weinberger et. al
[30] propose M 2 -LMNN, a locally linear extension to LMNN. They partition the space into multiple
regions, and jointly learn a separate metric for each region—however, these local metrics do not give
rise to a global metric and distances between inputs within different regions are not well-defined.
Neural network-based approaches offer the flexibility of learning arbitrarily complex nonlinear mappings [6]. However, they often demand high computational expense, not only in parameter fitting but
also in model selection and hyper-parameter tuning. Of particular relevance to our GB-LMNN work
is the use of boosting ensembles to learn distances between bit-vectors [1, 23]. Note that their goals
are to preserve distances computed by locality sensitive hashing to enable fast search and retrieval.
Ours are very different: we alter the distances discriminatively to minimize classification error.
Our work on χ2 -LMNN echoes the recent interest in learning earth-mover-distance (EMD) which
is also frequently used in measuring similarities between histogram-type data [9]. Despite its name,
EMD is not necessarily a metric [20]. Investigating the link between our work and those new advances is a subject for future work.

6

Experimental Results

We evaluate our non-linear metric learning algorithms against several competitive methods. The effectiveness of learned metrics is assessed by kNN classification error. Our open-source implementations are available for download at http://www.cse.wustl.edu/˜kilian/code/code.html.
GB-LMNN We compare the non-linear global metric learned by GB-LMNN to three linear metrics:
the Euclidean metric and metrics learned by LMNN [31] and Information-Theoretic Metric Learning
(ITML) [10]. Both optimize similar discriminative loss functions. We also compare to the metrics
learned by Multi-Metric LMNN (M 2 -LMNN) [30]. M 2 -LMNN learns |C| linear metrics, one for
each the input labels.
We evaluate these methods and our GB-LMNN on several medium-sized data sets: ISOLET, USPS
and Letters from the UCI repository. ISOLET and USPS have predefined test sets, otherwise results
are averaged over 5 train/test splits (80%/20%). A hold-out set of 25% of the training set3 is used to
assign hyper-parameters and to determine feature pre-processing (i.e., feature-wise normalization).
We set k = 3 for kNN classification, following [31]. Table 1 reports the means and standard errors
of each approach (standard error is omitted for data with pre-defined test sets), with numbers in bold
font indicating the best results up to one standard error.
On all three datasets, GB-LMNN outperforms methods of learning linear metrics. This shows the
benefit of learning nonlinear metrics. On Letters, GB-LMNN outperforms the second-best method
M 2 -LMNN by significant margins. On the other two, GB-LMNN is as good as M 2 -LMNN.
We also apply GB-LMNN to four datasets with histogram data — setting the stage for an interesting
comparison to χ2 -LMNN below. The results are displayed on the right side of the table. These
datasets are popularly used in computer vision for object recognition [22]. Data instances are 800bin histograms of visual codebook entries. There are ten common categories to the four datasets and
we use them for multiway classification with kNN.
Neither method evaluated so far is specifically adapted to histogram features. Especially linear
models, such as LMNN and ITML, are expected to fumble over the intricate similarities that such
3
In the case of ISOLET, which consists of audio signals of spoken letters by different individuals, the holdout set consisted of one speaker.

6

!"#$%&

-./0123456784/3
'"("

$%&&%)"

*"$)

89:;4;101<=671>803?
+%,-./
./.0#1

-.$&%-2

1344546*3748 1358596*38:; 138<<<<6*37; 137:46*39<< 1385:6*39<< 135:96*39<< 13778=6*39<<

>'-$!*%.1
BCDE
EDFF
D8GEDFF
HIGEDFF
J8
KLM
KLF
J8GEDFF

9?@
:?=A<?<
!"#$%"!
!"&$%"!
!"&$%"%
G
G
G
G

;?8
:?4
8?;
'"#
'"#
G
G
G
G

;?<A<?8
;?<A<?8
=?9A<?=
=?9A<?8
!"($%"!
G
G
G
G

;<?;A=?7
!"#$%&#$
89?5A7?;
84?@A8?7
!!#*%!#+
88?8A7?9
8:?;A8?4
84?9A@?7
'%")$!"!

@=?9A7?4
'!#(%'#)
7:?9A=?<
7:?4A=?8
'!#(%$#*
7=?<A7?8
75?@A7?7
74?:A8?7
*"+$%"(

==?4A<?4
=7?;A7?8
=7?9A7?@
&'#!%'#'
!*#)%'#+
=@?=A7?<
==?5A8?<
=@?:A7?:
'+",$%"*

:=?9A7?=
:8?8A8?7
"$#*%'#(
:7?:A7?:
(*#,%'#$
:9?9A7?7
:4?8A7?8
:;?7A7?8
&)"#$!"!

Table 1: kNN classification error (in %, ± standard error where applicable), for general methods
(top section) and histogram methods (bottom section). Best results up to one standard error in bold.
Best results among general methods for simplex data in red italics.
data types may encode. As shown in the table, GB-LMNN consistently outperforms the linear
methods and M 2 -LMNN.
χ2 -LMNN In Table 1, we compare χ2 -LMNN to other methods for computing distances on histogram features: χ2 -distance without transformation (equivalent to our parameterized distance χ2L
distance with the transformation L being the identity matrix), Quadratic-Chi-Squared (QCS) and
Quadratic-Chi-Normalized (QCN) distances, defined in [20]. For QCS and QCN, we use histogram
intersection as the ground distance. Unlike our approach, none of these is discriminatively learned
from data. χ2 -LMNN outperforms all other methods significantly.
It is also instructive to compare the results to the performance of non-histogram specific methods.
We observe that LMNN performs better than the standard χ2 -distance on Amazon and Caltech. This
seems to suggest that for those two datasets, linear metrics may be adequate and GB-LMNN’s nonlinear mapping might not be able to provide extra expressiveness and benefits. This is confirmed
in Table 1: GB-LMNN improves performance less significantly for Amazon and Caltech than for
the other two datasets, DSLR and Webcam. For the latter two, on the contrary, LMNN performs
worse than χ2 -distance. In such cases, GB-LMNN’s nonlinear mapping seems more beneficial. It
provides a significant performance boost, and matches the performance of χ2 -distance (up to one
standard-error). Nonetheless, despite learning a nonlinear mapping, GB-LMNN still underperforms
χ2 -LMNN. In other words, it is possible that no matter how flexible a nonlinear mapping could be,
it is still best to use metrics that respect the semantic features of the data.
Dimensionality reduction. GB-LMNN and χ2 -LMNN are both capable of performing dimensionality reduction. We compare these with three dimensionality reduction methods (PCA, LMNN, and
M 2 -LMNN) on the histogram datasets and the larger UCI datasets. Each dataset is reduced to an
output dimensionality of r = 10, 20, 40, 80 features. As we can see from the results in Table 6, it is
fair to say that GB-LMNN performs comparably with LMNN and M 2 -LMNN, whereas χ2 -LMNN
obtains at times phenomenally low kNN error rates on the histograms data sets (e.g., Webcam). This
suggests that dimensionality reduction of histogram data can be highly effective, if the data properties are carefully incorporated in the process. We do not apply dimensionality reduction to Letters
as it already lies in a low-dimensional space (d = 16).
Sensitivity to parameters. One of the most compelling aspects of our methods is that each introduces only a single new hyper-parameter to the LMNN framework. During our experiments, ` was
selected by cross-validation and p was fixed to p = 6. We found very little sensitivity in GB-LMNN
to regression tree depth, while large margin size was an important but well-behaved parameter for
χ2 -LMNN. Additional graphs are included in the supplementary material.

7

Conclusion and Future Work

In this paper we introduced two non-linear extensions to LMNN, χ2 -LMNN and GB-LMNN. Although based on fundamentally different approaches, both algorithms lead to significant improvements over the original (linear) LMNN metrics and match or out-perform existing non-linear algorithms. The non-convexity of our proposed methods does not seem to impact their performance,
7

*I9:

*I6:

*I>:

*I@:

345
ABCC
B6EABCC
FGEABCC
H6EABCC
345
ABCC
B6EABCC
FGEABCC
H6EABCC
345
ABCC
B6EABCC
FGEABCC
H6EABCC
345
ABCC
B6EABCC
FGEABCC
H6EABCC

;<=32>91?67-1=9
!"#$%&
'"("
6787
9:89
>86:=:89 78:
>8;=:86
(#"
&#'%+#+
D8;
E
E
9D89
787
"#*%+#*
;8@
"#*%+#"
&#&
"#+%+#*
;8@
E
E
998:
78:
98D=:8:
;86
*#"%+#*
;86
98>=:89
"#,
E
E
?8>
789
*#)%+#*
;86
*#)%+#+
*#$
*#)%+#*
68>
E
E

)"$*
!"#$%&#'
D789=68:
D789=68:
>78<=<8>
!&#&%"#)
>789=;8<
D;8;=68@
D;8;=68@
D:8:=;8>
&*#*%&#"
>78<=;8:
D98<=:8<
D98<=:8<
D:8:=689
&&#&%"#+
;?8>=98@
D989=68>
D989=68>
D:8:=98?
&&#&%"#+

-./010232456728-39:
+%,-./
./.0#1
;68<=987
>?89=686
;@89=68D
>;87=>87
;@8>=68<
>68@=98<
;>87=687
>987=68<
*'#$%"#,
&&#,%*#*
6<8;=98<
>;8?=989
;>8:=68?
;?8?=98D
;>8;=687
>:8;=98;
;;8:=68@
;@8<=:8@
*!#&%&#*
&&#*%+#,
6?86=686
>;89=987
;78@=68:
;?8>=98:
;786=98;
;?8>=98;
;98<=98;
;?8;=98;
*+#"%"#&
&&#+%*#&
;?8>=98@
>78:=989
;78D=68@
>;8>=:8?
;D8?=68<
>;8>=98:
6<8;=;8D
>989=986
$#&%*#)
",#(%*#"

-.$&%-2
7;8@=989
(!#)%"#*
((#+%"#"
((#$%"#*
(!#'%*#,
D?8?=:8D
((#!%*#'
DD8D=98D
(&#'%*#&
DD87=:8?
D<8<=:8D
D789=98D
D789=987
(&#&%*#!
(!#+%*#!
7?8>=;8?
7:8;=:8@
D>8>=98D
D>89=98;
(*#*%"#&

Table 2: kNN classification error (in %, ± standard error where applicable) with dimensionality
reduction to output dimensionality r. Best results up to one standard error in bold.
indicating that convex algorithms (LMNN) as initialization for more expressive non-convex methods
can be a winning combination.
The strong results obtained with χ2 -LMNN show that the incorporation of data-specific constraints
can be highly beneficial—indicating that there is great potential for future research in specialized
metric learning algorithms for specific data types. Further, the ability of χ2 -LMNN to reduce the
dimensionality of data sampled from probability simplexes is highly encouraging and might lead
to interesting applications in computer vision and other fields, where histogram data is ubiquitous.
Here, it might be possible to reduce the running time of time critical algorithms drastically by shrinking the data dimensionality, while strictly maintaining its histogram properties.
The high consistency with which GB-LMNN obtains state-of-the-art results across diverse data sets
is highly encouraging. In fact, the use of ensembles of CART trees [4] not only inherits all positive
aspects of gradient boosting (robustness, speed and insensitivity to hyper-parameters) but is also a
natural match for metric learning. Each tree splits the space into different regions and in contrast to
prior work [30], this splitting is fully automated, results in new (discriminatively learned) Euclidean
representations of the data and gives rise to well-defined pseudo-metrics.

8

Acknowledgements

KQW, DK and ST would like to thank NIH for their support through grant U01 1U01NS073457-01
and NSF for grants 1149882 and 1137211. FS would like to thank DARPA for its support with grant
D11AP00278 and ONR for grant N00014-12-1-0066. DK would also like to thank the McDonnell
International Scholars Academy for their support.

References
[1] B. Babenko, S. Branson, and S. Belongie. Similarity metrics for categorization: from monolithic to
category specific. In ICCV ’09, pages 293–300. IEEE, 2009.
[2] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167–175, 2003.
[3] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent dirichlet allocation. The Journal of Machine Learning
Research, 3:993–1022, 2003.
[4] L. Breiman. Classification and regression trees. Chapman & Hall/CRC, 1984.

8

[5] R. Chatpatanasiri, T. Korsrilabutr, P. Tangchanachaianan, and B. Kijsirikul. A new kernelization framework for mahalanobis distance learning algorithms. Neurocomputing, 73(10-12):1570–1579, 2010.
[6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to
face verification. In CVPR ’05, pages 539–546. IEEE, 2005.
[7] T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE Transactions on Information Theory,
13(1):21–27, 1967.
[8] O.G. Cula and K.J. Dana. 3D texture recognition using bidirectional feature histograms. International
Journal of Computer Vision, 59(1):33–60, 2004.
[9] M. Cuturi and D. Avis. Ground metric learning. arXiv preprint, arXiv:1110.2306, 2011.
[10] J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon. Information-theoretic metric learning. In ICML ’07,
pages 209–216. ACM, 2007.
[11] J.H. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages
1189–1232, 2001.
[12] C. Galleguillos, B. McFee, S. Belongie, and G. Lanckriet. Multi-class object localization by combining
local contextual interactions. CVPR ’10, pages 113–120, 2010.
[13] A. Globerson and S. Roweis. Metric learning by collapsing classes. In NIPS ’06, pages 451–458. MIT
Press, 2006.
[14] A. Globerson and S. Roweis. Visualizing pairwise similarity via semidefinite programming. In AISTATS
’07, pages 139–146, 2007.
[15] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In
NIPS ’05, pages 513–520. MIT Press, 2005.
[16] J. Hafner, H.S. Sawhney, W. Equitz, M. Flickner, and W. Niblack. Efficient color histogram indexing
for quadratic form distance functions. Pattern Analysis and Machine Intelligence, IEEE Transactions on,
17(7):729–736, 1995.
[17] M. Hoffman, D. Blei, and P. Cook. Easy as CBA: A simple probabilistic model for tagging music. In
ISMIR ’09, pages 369–374, 2009.
[18] P. Jain, B. Kulis, J.V. Davis, and I.S. Dhillon. Metric and kernel learning using a linear transformation.
Journal of Machine Learning Research, 13:519–547, 03 2012.
[19] A.M. Mood, F.A. Graybill, and D.C. Boes. Introduction in the theory of statistics. McGraw-Hill International Book Company, 1963.
[20] O. Pele and M. Werman. The quadratic-chi histogram distance family. ECCV ’10, pages 749–762, 2010.
[21] Y. Rubner, C. Tomasi, and L.J. Guibas. The earth mover’s distance as a metric for image retrieval.
International Journal of Computer Vision, 40(2):99–121, 2000.
[22] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. Computer
Vision–ECCV 2010, pages 213–226, 2010.
[23] G. Shakhnarovich. Learning task-specific similarity. PhD thesis, MIT, 2005.
[24] N. Shental, T. Hertz, D. Weinshall, and M. Pavel. Adjustment learning and relevant component analysis.
In ECCV ’02, volume 4, pages 776–792. Springer-Verlag, 2002.
[25] M. Stricker and M. Orengo. Similarity of color images. In Storage and Retrieval for Image and Video
Databases, volume 2420, pages 381–392, 1995.
[26] L. Torresani and K. Lee. Large margin component analysis. NIPS ’07, pages 1385–1392, 2007.
R
[27] T. Tuytelaars and K. Mikolajczyk. Local invariant feature detectors: a survey. Foundations and Trends
in Computer Graphics and Vision, 3(3):177–280, 2008.
[28] S. Tyree, K.Q. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search
ranking. In WWW ’11, pages 387–396. ACM, 2011.
[29] M. Varma and A. Zisserman. A statistical approach to material classification using image patch exemplars.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(11):2032–2047, 2009.
[30] K.Q. Weinberger and L.K. Saul. Fast solvers and efficient implementations for distance metric learning.
In ICML ’08, pages 1160–1167. ACM, 2008.
[31] K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classification.
The Journal of Machine Learning Research, 10:207–244, 2009.
[32] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning, with application to clustering
with side-information. In NIPS ’02, pages 505–512. MIT Press, 2002.
[33] P.N. Yianilos. Data structures and algorithms for nearest neighbor search in general metric spaces. In
ACM-SIAM Symposium on Discrete Algorithms ’93, pages 311–321, 1993.

9

