Metric Learning for Kernel Regression

Kilian Q. Weinberger
Gerald Tesauro
Computer and Information Sciences
IBM T.J. Watson Research Center
U. of Pennsylvania, Philadelphia, PA 19104 19 Skyline Drive, Hawthorne, NY 10532
kilianw@seas.upenn.edu
gtesauro@us.ibm.com

Abstract
Kernel regression is a well-established
method for nonlinear regression in which
the target value for a test point is estimated using a weighted average of
the surrounding training samples. The
weights are typically obtained by applying a distance-based kernel function to
each of the samples, which presumes the
existence of a well-defined distance metric. In this paper, we construct a novel
algorithm for supervised metric learning,
which learns a distance function by directly minimizing the leave-one-out regression error. We show that our algorithm makes kernel regression comparable with the state of the art on several
benchmark datasets, and we provide efficient implementation details enabling
application to datasets with ∼O(10k) instances. Further, we show that our algorithm can be viewed as a supervised
variation of PCA and can be used for dimensionality reduction and high dimensional data visualization.

1

Introduction

One of the oldest and most commonly used algorithms for regression is kernel regression [1]. In
kernel regression, the target value of a test input
is computed as a weighted average of the function
values observed at training inputs. The weight
of each training input is computed using a kernel

function, which typically decays rapidly with distance between itself and the test input. This is so
that the estimated test point value has strongest
dependence on nearby training points. A common approach in kernel regression combines a
Euclidean distance metric with Gaussian kernels,
which decay exponentially with squared distance
rescaled by a kernel width factor.
We note two important drawbacks of standard
methods for kernel regression. First, they require an a priori well-defined distance metric on
the input space, which may preclude their usage
in datasets where such metrics are not meaningful. For example, the well-known Boston housing dataset contains 13 input features representing completely disparate quantities such as pollution levels, crime rates, pupil-teacher ratios, etc..
Secondly, a pre-defined distance metric may not
be particularly relevant to the regression task at
hand. For example, if certain input features are
completely irrelevant to the regression task, they
ideally should not contribute at all to the distance
metric, whereas a Euclidean distance would ascribe to them as much weight as the most significant features.
Inspired by recent innovations on metric learning
for classification [3] and feature selection [8], we
propose a novel method to learn a task-specific
(pseudo-)metric over the input space in which
small distances between two vectors imply similar target values. This metric gives rise to an
appropriate kernel function with parameters set
entirely from the data. In addition to performing
regression, we will also show that our algorithm,
which we refer to as Metric Learning for Kernel

Regression (MLKR), can be viewed as an algorithm for dimensionality reduction. This opens
the possibility to use MLKR as a pre-processing
tool for a whole variety of independent machine
learning algorithms and data visualization. Data
visualization can be particularly helpful to gain
deeper understanding of the underlying structure
and behavior of the target function with respect
to the input.
This paper is organized as follows: Section 2 establishes notation and briefly gives background on
the specifics of kernel regression. In Section 3, we
introduce our algorithm for supervised distance
metric learning. Section 4 illustrates how our algorithm can be interpreted as a variation of Principal Component Analysis [6], and how it can be
used for task-specific dimensionality reduction. In
Section 5 we evaluate the capabilities of MLKR
for data visualization on a synthetically generated
data set, and compare its regression accuracy on
several standard data sets with various common
regression algorithms. Section 6 compares our algorithm to related published work, and we conclude in Section 7.

2

Kernel regression problem setting

The standard regression task is to estimate an
unknown function f : RD → R based solely
on a training set of (possibly noisy) evaluations
(~x1 , y1 ), . . . , (~xn , yn ). Here yi = f (~xi ) + ǫ, where ǫ
represents some small noise. Specifically, for some
distribution of test points {~xt }, our task is to find
an estimator fˆ : RD → R of f that minimizes
some loss function (e.g., squared error) over the
distribution of test points.
In kernel regression, the value of ŷt ≈ f (~xt ) is approximated by a weighted average of the training
samples:
P
j6=i yj kij
(1)
ŷi = P
j6=i kij
where kij = k(~xi , ~xt ) ≥ 0 is referred to as the
kernel function. A wide variety of kernel functions have been studied in prior literature, e.g.,
Gaussian, triangular, spherical, wave, etc.. To
make the estimate ŷt truly local, the kernel function k(~xi , ~xt ) should decay rapidly with increasing
(squared) distance d(~xi , ~xt ). The optimal rate of

decay depends on the noisiness and smoothness
of the function f , the density of the training samples, and the scale of the input features. Hence
the choice of kernel function may require great
care, especially in high dimensional spaces. Several publications elaborate on how to chose the
right kernel function for a given data set [11]. Additionally, kernel functions may have several tunable parameters pertaining to the rate of decay;
these also must be set properly to obtain good
performance on the regression task.

3

Metric learning algorithm

We now present our algorithm Metric Learning
for Kernel Regression (MLKR) for learning an appropriate distance function for kernel regression.
Our approach applies to any distanced-based kernel function k(~xi , ~xj ) = kD (Dθ (~xi , ~xj )) with differentiable dependence on parameters θ specifying the distance function Dθ . Specifically, MLKR
consists of setting initial values of θ, and then
adjusting the values using a gradient descent procedure:
∂L
∆θ = −ǫ
(2)
∂θ
where ǫ is an adaptive step-size, and the loss function L is the cumulative leave-one-out quadratic
regression error of the training samples:
X
L=
(yi − ŷi )2
(3)
i

with ŷi defined as in (1). Of course, other methods for minimizing (3) such as conjugate gradient,
stochastic gradient or BFGS may also be used and
might lead to faster convergence results.
While MLKR may apply to many types of kernel functions and distance metrics, we hereafter
focus the exposition on a particular instance of
the Gaussian kernel and Mahalanobis metric, as
these are used in our empirical work. The Gaussian kernel is generally defined as follows:
d(~
xi ,~
xj )
1
kij = √ e− σ2 ,
σ 2π

(4)

where d(~xi , ~xj ) is the squared distance between
the vectors ~xi and ~xj . As the constant factor before the exponent in eq. (4) cancels out in eq.

(1), we will drop it for simplification. Also, we
will absorb σ 2 in d() and can fix σ = 1.
A Mahalanobis metric is a generalization of the
Euclidean metric, in which the squared distance
between two vectors ~xi and ~xj is defined as
d(~xi , ~xj ) = (~xi − ~xj )⊤ M(~xi − ~xj ),

(5)

where M can be any symmetric positive semidefinite1 real matrix. (Setting M to the identity
matrix recovers the standard Euclidean metric.)
Figure 1 illustrates the difference between a Mahalanobis metric and the Euclidean metric on a
synthetic regression example.
Unfortunately, learning the matrix M directly requires enforcing a positive semi-definite constraint
during optimization. This is non-linear and expensive to satisfy. One way to eliminate this expensive constraint is to decompose M as
M = A⊤ A,

(6)

√
where the ith row of A is the vector λi~vi⊤ ,
where ~vi is the ith eigenvector and λi is the corresponding eigenvalue of M. Substituting (6) into
(5) enables us to express the Mahalanobis distance as the Euclidean distance after the mapping
~x → A~x:
d(~xi , ~xj ) = kA(~xi − ~xj )k2 .

(7)

Equation (6) allows us to rewrite (3) in terms of
the unconstrained matrix A, instead of M. Let
ŷi be the target estimate of vector f (~xi ). The
gradient of (3) with respect to A can be stated as
X
X
∂L
= 4A
(ŷi − yi )
(ŷj − yj )kij ~xij ~x⊤
ij , (8)
∂A
i

j

where ~xij = (~xi −~xj ). We note that minimizing (3)
gives us a kernel function in a entirely data-driven,
non-parametric way. There are no parameters to
tune, apart from the initialization of A and the
gradient stepsize. Also, the algorithm learns its
own scaling (in the form of A) and is therefore
invariant to the scaling of the input vectors. In
our experiments, we usually initialized with the
1

A matrix M is called positive semi-definite if all its
eigenvalues are non-negative. This is the case if and only
if for any real vector ~v the following holds: ~v ⊤ M~v ≥ 0.

Figure 1: An illustration of kernel regression under varying distance metrics. The color of the
points represents the function value. The circle shows the radius that encapsulates 95% of
the weights. The function value is estimated at
a test point at the center. Left: With the use
of the Euclidean Distance metric, the kernel is
spherical and ignores the present structure that
points along the diagonal share similar function
values. Right: After training, under the Mahalanobis metric, the kernel function follows the direction of the target function. The radius has also
shrunk and has therefore adapted to the relatively
densely sampled and noiseless training samples.
identity matrix. If local minima are a concern,
one can use several runs with different random
initializations and choose the outcome with minimum training error L.
Efficient optimization
A naı̈ve computation of the gradient (8) requires
us to compute the outer product between all pair
of vectors ~xi , ~xj . This would result in a complexity of order O(D 2 n2 ). Luckily, we can approximate the gradient by making use of the fast decay
of kij . In our implementation we only considered
the c = 1000 nearest neighbors of each vector ~xi
and also disregarded weights of very small values
(kij < e−34 ). Both cutoff values turned out to be
overly conservative and seemed to have no noticeable influence on the quality of the outcome. To
avoid an expensive nearest neighbor search in each
iteration, we cached the nearest neighbors of each
point in a simple heap-tree data structure, which
we updated every 15 gradient steps. The new
complexity (O(D 2 nc + n2)) is still quadratic in n,
but only for the rare nearest neighbor search when
the heap tree is corrected. These two changes al-

lowed us to apply our algorithm on data sets of
size n = 30, 000 and higher. For even bigger data
sets, the quadratic nearest neighbor search could
be sped up with sophisticated tree data structures, such as cover trees [2] or kd-trees [7].

4

Dimensionality reduction

Many machine learning algorithms require some
sort of dimensionality reduction as pre-processing
of the input data. Recently, there have been several new methods for linear dimensionality reduction [5]. In this section, we illustrate that MLKR
can also be understood as a supervised version of
principal component analysis [6], that - in contrast to PCA - is independent of the scaling of
the input features and generates locally meaningful distances with respect to the regression value.
Principal Component Analysis
One of the most commonly used algorithms for
dimensionality reduction is Principal Component
Analysis (PCA) [6]. PCA projects the input
vectors onto the low dimensional sub-space that
minimizes the sum-squared reconstruction-error
of the input vectors. It has been shown that the
principal components are the leading eigenvectors
of the covariance matrix (of the centered inputs
~xi - up to a constant factor n1 )
C=

X

~xi ~x⊤
i .

(9)

i

One major drawback of PCA is that the outcome
heavily depends on the scaling of the input features. For example, if the individual features of ~xi
contain time measures, and we change the unit of
one feature from milliseconds to hours, the resulting embedding found with PCA would be drastically different. A second disadvantage of PCA is
that the projection is entirely unsupervised and
ignores any side information that might be known
about the input data.
We can show that learning the matrix M from
section 3 is equivalent to learning a specially constructed “covariance” matrix for PCA. In comparison to C, M is independent of the scale of
the individual input dimensions. For now let us
assume that the covariance matrix of the input

is the identity matrix (if not, the input can always be whitened through multiplication by the
1
matrix C− 2 .)2 Let M = A⊤ A be the solution
of minimizing (3). The covariance matrix (9) of
the input after the mapping ~xi → A~xi is the matrix CA = AA⊤ (because of the prior whitening).
Let ~v be the leading eigenvector of CA . One way
to combine MLKR with dimensionality reduction
would be to first map ~xi → A~xi and then perform PCA after the mapping. More explicitly, we
obtain the first dimension of our embedding by
~xi → ~v ⊤ A~xi .

(10)

Let w
~ be the leading eigenvector of M. It is easy
to verify that w
~ = A⊤~v . Hence, the mapping in
eq. (10) is equivalent to ~xi → w
~ ⊤ ~xi .
It follows that if we project onto the leading eigenvectors of M, the mapping ~xi → A~xi is in fact
PCA in the projected space. This gives us an effective method for supervised dimensionality reduction. In other words, by learning the matrix
M, we learn a covariance matrix on which we can
perform PCA.
Additionally, if the desired dimensionality is
known before the optimization, the dimensionality reduction can be directly incorporated into the
minimization of (3). M can be constraint to be
of low rank by choosing A to be rectangular (ie
A ∈ Rr×D ) which naturally leads to a mapping
A : RD → Rr , where r < D.

5

Results

We performed several tests to evaluate the capabilities of MLKR. First we demonstrate the
abilities of MLKR for dimensionality reduction
and feature selection on an artificially generated
data set. Further, we conducted extensive tests
on standard regression data sets and compared
MLKR with a variety of different state of the
art algorithms. We minimized the objective (3)
with the simple gradient descent algorithm (2).
In cases where the normalizer in (4) leads to division by zero (due to inadequate machine preci2
Please note, that whitening the input does not change
the best solution of MLKR. If1 A is the best solution for
the original vectors then AC 2 leads to the same results
for the whitened vectors.

P

n

o

r

m

a

l

i

z

e

d

e

i

g

e

n

v

a

l

u

e

C

s

A

M

L

K



R

:

n

0

0

.

.

1

0

0

.

.

0

o

r

m

a

l

i

z

e

d

e

i

g

e

n

v

a

l

u

e

s

:

2

0

.

5

0

.

4

0

.

3

0

.

0

.

5

1

2

5

1

0

0

1

2

3

4

5

6

7

8

9

1

0

1

n

o

i

s

e

s

i

g

n

a

2

3

4

5

6

7

8

9

1

0

l

s

i

g

n

a

l

n

o

i

s

e

Figure 2: The result of a 3d projection of a manifold data set under PCA and MLKR. Left: The
projection onto the leading PCA principal components projects directly onto the noise and ignores
the signal. The seven large eigenvalues would suggest to keep the noise and ignore the signal. Right:
The MLKR projection finds the three relevant features and projects out the noise perfectly. The three
eigenvalues suggest to keep the signal and cut off the noise.
sion), we assigned the target value of the closest
training point.

Supervised dimensionality reduction
We tested the abilities of MLKR for dimensionality reduction on a synthetic data set. We generated n = 8000 input vectors of dimensionality D = 10. The input data was constructed
as follows: First, we sampled data points from
a three dimensional “twin-peak” manifold with
~xi = (ri , si , sin(ri ) tanh(si ))⊤ for some randomly
sampled points (ri , si ). The target value was computed as yi = k~xi k2 . The dataset was then embedded into a ten dimensional space by adding
seven dimensions of high uniform noise (the noise
level was set to 1000% of the original input magnitude.) Finally, we multiplied the input vectors
by a randomly generated rotation matrix which
resulted in all ten input dimensions containing a
comparable mixture of noise and signal.
Figure 2 shows the low dimensional projections of
these input vectors found with PCA and MLKR.
As the noise has much higher magnitude than
the input signal, PCA returns principal components that project entirely onto the high variance
noise and therefore ignore the relevant signal. The
eigenvalue spectrum of the covariance matrix reveal that the (low-variance) signal is represented
by the bottom eigenvectors, which are generally
dropped.

MLKR on the other hand learns a transformation
of the space that suppresses the noise entirely and
restores the signal. The final result is a perfect
recovery of the clean input signal. The three nonzero eigenvalues indicated that MLKR has recovered the three dimensions that are relevant to the
target function.
In addition to the synthetically generated data,
we also applied MLKR to images from the FGNET face data set3 . The FG-NET data base consists of n = 984 frontal face images of 82 persons.
The images are accompanied by the age of each
person, at the time when the image was taken,
as additional side information. We preprocessed
the images by aligning the eyes (through rotation
and translation), cropping the edges and rescaling the images to 50 × 50 pixels. We then applied
MLKR to the set of 100 dimensional eigenfaces
(after removing some distinct outliers).
Figure 3 displays the two dimensional projection
of images obtained with MLKR and PCA. For
better visualization, we superimposed several representative images on top of their low dimensional
coordinates. The MLKR representation shows a
clear correlation between position and age (the
arrows highlight the direction of increasing age),
whereas the PCA representation seems mostly influenced by angle and illumination.
3
More information on the FG-NET face image data set
is available at http://fgnet.cycollege-pms.net/

Figure 3: The two dimensional projections of the FG-NET face data set set obtained with PCA
and MLKR. A number of representative images are superimposed on top of their low dimensional
coordinates. MLKR was trained with the persons age as target and aligns the images accordingly
(the arrows indicate age increasing directions) whereas the PCA embedding correlates images mostly
according to viewing angle and illumination.

Figure 4: The regression error on various robot arm regression data sets, obtained from the Delve
package. The applied algorithms are linear regression (lin-1), k-nearest neighbor regression with cross
validation for parameter setting (knn-cv-1), hierarchical mixture of experts trained with early stopping
(hme-ese) [12], Gaussian processes for regression [10] (gp-map-1) and MLKR.
Regression
In addition to the data sets from the previous section, we also evaluated our algorithm on several
standard regression problems from the Data for
Evaluating Learning in Valid Experiments (Delve)
collection4 . We used sixteen different data sets

generated by two synthetic robot arms5 . Half of
the sixteen data sets had 32 dimensions and the
other half were of dimension 8.
The Delve evaluation package randomly split each
data set into four disjoint training sets of size
5

4

The
Delve
package
is
http://www.cs.toronto.edu/˜delve/

available

at:

For more details on the specific data please see
http://www.cs.toronto.edu/˜delve/data/pumadyn/desc.html
and http://www.cs.toronto.edu/˜delve/data/kin/desc.html

Figure 5: The results of MLKR under varying matrix restrictions. We compared the results of MLKR
with a full matrix, a 2 × d, 3 × d, d2 × d and a diagonal matrix. The results of the low-rank constraint
variations are surprisingly close to the full rank version of MLKR.
n = 1024 and one unique test set with 1024 instances. The squared error is computed from the
results of the four individual runs. We would like
to point out that MLKR could be trained on much
bigger data sets (we successfully ran it on proprietary data of size n > 30000). We chose the
Delve evaluation package as an objective method
to compare with previously published work. All
runs finished within a matter of several minutes.
To avoid overfitting, we trained all versions of
MLKR with a validation data set for early stopping (of size 10% of the training data).
Figure 4 shows the sum-squared-test error of six
different algorithms. We compared MLKR with
four standard regression algorithms: linear regression, k-nearest neighbor regression with cross validation for parameter setting, hierarchical mixture
of experts trained with early stopping [12] and
Gaussian processes for regression [10]. All results
are relative to a baseline algorithm which is based
entirely on the mean and median of the training
target values6 .
It can be observed that MLKR outperforms most
alternative algorithms consistently and is almost
as precise as Gaussian Process Regression, an algorithm generally considered state of the art.
In addition to the full matrix version, we also evaluated four constrained variants of MLKR. First
6

For
more
information
http://www.cs.toronto.edu/˜
delve/methods/base-1/home.html

see

we constrained MLKR to learn a rectangular matrix that reduces the dimensionality of the input
from d ∈ {8, 32} to 2, 3, d2 . We also constrained the
matrix A to be diagonal, which simply rescales
the input features. Figure 5 shows that restricting
the dimensionality to 3d or even 2d has surprisingly little impact on the quality of the regression.

6

Related Work

Over the last several years there have been many
different algorithms on metric learning and kernel
regression. As far as we know, our work is the first
to combine these two fields.
In 2005 Goldberger et al. introduced a novel algorithm for metric learning, which they refer to
as neighbourhood component analysis (NCA) [3].
(Our work was inspired by this publication.) Similarly to MLKR, the authors learn a Mahalanobis
metric over the input space which appears in the
objective function in terms of a soft-max distribution. The main differences between NCA and
MLKR are that MLKR minimizes a regression error loss function, whereas NCA is designed for
classification, and that NCA’s probabilistic formulation assumes a specific soft-max probability
model.
In 2006, Takeda et al. introduced an algorithm
for kernel regression [4]. Similarly to our work,
Takeda et al. can be interpreted as learning a
Mahalanobis matrix for a Gaussian regression ker-

nel. However, instead of function estimation, the
authors apply their algorithm on image denoising. In contrast to MLKR, which learns the Mahalanobis matrix via loss function minimization,
Takeda et al. estimate the covariance matrix from
statistics of the local pixel space.

helpful discussions. Kilian Weinberger would also
like to thank Irina Rish, Rajarshi Das and Alina
Beygelzimer for stimulating discussion during his
time spent at IBM.

Navot et al. published a novel and effective
method for nearest neighbor feature selection [8]
in 2006. Their method learns weights for each input feature as an indication of its significance. In
comparison, MLKR learns a linear transformation
for dimensionality reduction and regression.

[1] J. K. Benedetti. On the nonparametric estimation of
regression functions. Journal of the Royal Statistical
Society, 39:248–253, 1977.

Finally, Keller et al. (2006) [9] applied neighborhood component regression to function approximation for reinforcement learning. Our paper focusses more on the adaptation of NCA for kernel
regression.

7

Conclusion

We introduced a novel metric learning algorithm
for kernel regression, which we refer to as Metric Learning for Kernel Regression. We demonstrated that the algorithm can give rise to stateof-the-art regression results on several standard
regression benchmark data sets. We also illustrated that MLKR can be used for dimensionality reduction and can be viewed as a supervised
alternative to PCA.
As future work, we are also interested in exploring
the possibilities of combining the learned metric
with various other machine learning algorithms.
We believe that MLKR can be combined with recent work in non-linear dimensionality reduction,
e.g., Maximum Variance Unfolding (MVU) [13].
One could view MLKR as a first step that learns a
metric with local meaningful distances and MVU
as a second step that finds a non-linear mapping
into a low dimensional representation that preserves these local distances. We hope that this
two step approach will allow function approximation in very high dimensional spaces with (relatively) few training samples.
Acknowledgements
The authors would like to thank Amir Navot,
Sam Roweis, Lawrence Saul and Lyle Ungar for

References

[2] A. Beygelzimer, S. Kakade, and J. Langford. Cover
trees for nearest neighbor. In Proc. of the 23rd Intl.
Conf. on Machine Learning, Pittsburgh, USA, 2006.
[3] J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In L. K.
Saul et al., editors, Advances in Neural Information
Processing Systems 17, pages 513–520. MIT Press,
2005.
[4] P. Milanfar H. Takeda, S. Farsiu. Robust kernel regression for restoration and reconstruction of images
from sparse, noisy data. In Intl. Conf. on Image Processing. Atlanta, GA, 2006.
[5] X. He and P. Niyogi. Locality preserving projections.
In S. Thrun et al., editors, Advances in Neural Information Processing Systems 16. MIT Press, 2004.
[6] I. T. Jolliffe. Principal Component Analysis. SpringerVerlag, New York, 1986.
[7] A. Moore.
A tutorial on kd-trees.
Extract from PhD Thesis, 1991.
Available from
http://www.cs.cmu.edu/simawm/papers.html.
[8] A. Navot, L. Shpigelman, N. Tishby, and E. Vaadia.
Nearest neighbor based feature selection for regression
and its application to neural activity. In Y. Weiss
et al., editors, Advances in Neural Information Processing Systems 18, pages 995–1002. MIT Press, 2006.
[9] D. Precup P. Keller, S. Mannor. Automatic basis
function construction for approximate dynamic programming and reinforcement learning. In Proceedings of the Twenty-third International Conference on
Machine Learning (ICML 2006), Pittsburgh, U.S.A.,
2006.
[10] C. E. Rasmussen and C. K. I Williams. Gaussian Processes for Machine Learning. Springer-Verlag, Cambridge, MA, 2006.
[11] H.G. Müller T. Gasser. Kernel Estimation of Regression Functions., pages 23–67. Springer-Verlag, Heidelberg, 1979.
[12] D. S. Touretzky et al., editors. Constructive Algorithms for Hierarchical Mixtures of Experts. MIT
Press, 1996.
[13] K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semidefinite programming.
Intl. Journal of Computer Vision, 70(1):77–90, 2006.

