Nonlinear Dimensionality Reduction by Semideﬁnite Programming and Kernel Matrix Factorization

Kilian Q. Weinberger, Benjamin D. Packer, and Lawrence K. Saul∗ Department of Computer and Information Science University of Pennsylvania, Philadelphia, PA 19104-6389 {kilianw,lsaul,bpacker}@seas.upenn.edu

Abstract
We describe an algorithm for nonlinear dimensionality reduction based on semideﬁnite programming and kernel matrix factorization. The algorithm learns a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. In earlier work, the kernel matrix was learned by maximizing the variance in feature space while preserving the distances and angles between nearest neighbors. In this paper, adapting recent ideas from semi-supervised learning on graphs, we show that the full kernel matrix can be very well approximated by a product of smaller matrices. Representing the kernel matrix in this way, we can reformulate the semideﬁnite program in terms of a much smaller submatrix of inner products between randomly chosen landmarks. The new framework leads to order-of-magnitude reductions in computation time and makes it possible to study much larger problems in manifold learning.

In recent work [21, 22], we introduced Semideﬁnite Embedding (SDE), an algorithm for manifold learning based on semideﬁnite programming [20]. SDE learns a kernel matrix by maximizing the variance in feature space while preserving the distances and angles between nearest neighbors. It has several interesting properties: the main optimization is convex and guaranteed to preserve certain aspects of the local geometry; the method always yields a semipositive deﬁnite kernel matrix; the eigenspectrum of the kernel matrix provides an estimate of the underlying manifold’s dimensionality; also, the method does not rely on estimating geodesic distances between faraway points on the manifold. This particular combination of advantages appears unique to SDE. The main disadvantage of SDE, relative to other algorithms for manifold learning, is the time required to solve large problems in semideﬁnite programming. Earlier work in SDE was limited to data sets with n ≈ 2000 examples, and problems of that size typically required several hours of computation on a mid-range desktop computer. In this paper, we describe a new framework that has allowed us to reproduce our original results in a small fraction of this time, as well as to study much larger problems in manifold learning. We start by showing that for well-sampled manifolds, the entire kernel matrix can be very accurately reconstructed from a much smaller submatrix of inner products between randomly chosen landmarks. In particular, letting K denote the full n × n kernel matrix, we can write: K ≈ QLQT , (1)

1

Introduction

A large family of graph-based algorithms has recently emerged for analyzing high dimensional data that lies or or near a low dimensional manifold [2, 5, 8, 13, 19, 21, 25]. These algorithms derive low dimensional embeddings from the top or bottom eigenvectors of specially constructed matrices. Either directly or indirectly, these matrices can be related to kernel matrices of inner products in a nonlinear feature space [9, 15, 22, 23]. These algorithms can thus be viewed as kernel methods with feature spaces that “unfold” the manifold from which the data was sampled.
∗

This work was supported by NSF Award 0238323.

where L is the m × m submatrix of inner products between landmarks (with m n) and Q is an n×m linear transformation derived from solving a sparse set of linear equations. The factorization in eq. (1) enables us to reformulate the semideﬁnite program in terms of the much smaller matrix L, yielding order-of-magnitude reductions in computation time.

The framework in this paper has several interesting connections to previous work in manifold learning and kernel methods. Landmark methods were originally developed to accelerate the multidimensional scaling procedure in Isomap [7]; they were subsequently applied to the fast embedding of sparse similarity graphs [11]. Intuitively, the methods in these papers are based on the idea of triangulation—that is, locating points in a low dimensional space based on their distances to a small set of landmarks. This idea can also be viewed as an application of the Nystr¨ om method [24, 12], which is a particular way of extrapolating a full kernel matrix from one of its sub-blocks. It is worth emphasizing that the use of landmarks in this paper is not based on this same intuition. SDE does not directly estimate geodesic distances between faraway inputs on the manifold, as in Isomap. As opposed to the Nystr¨ om method, our approach is better described as an adaptation of recent ideas for semisupervised learning on graphs [1, 16, 18, 26, 27]. Our approach is somewhat novel in that we use these ideas not for transductive inference, but for computational savings in a purely unsupervised setting. To manage the many constraints that appear in our semideﬁnite programming problems, we have also adapted certain ideas from the large-scale training of support vector machines [6]. The paper is organized as follows. In section 2, we review our earlier work on manifold learning by semideﬁnite programming. In section 3, we investigate the kernel matrix factorization in eq. (1), deriving the linear transformation that reconstructs other examples from landmarks, and showing how it simpliﬁes the semidefinite program for manifold learning. Section 4 gives experimental results on data sets of images and text. Finally, we conclude in section 5.

transformation from inputs to outputs thus looks locally like a rotation plus translation—that is, it represents an isometry. To picture such a transformation from D = 3 to d = 2 dimensions, one can imagine a ﬂag being unfurled by pulling on its four corners. The ﬁrst step of the algorithm is to compute the k nearest neighbors of each input. A neighborhoodindicator matrix is deﬁned as ηij = 1 if and only if the inputs xi and xj are k -nearest neighbors or if there exists another input of which both are k -nearest neighbors; otherwise ηij = 0. The constraints to preserve distances and angles between k -nearest neighbors can then be written as: ||yi − yj || = ||xi − xj || ,
2 2

(2)

for all (i, j ) such that ηij = 1. To eliminate a translational degree of freedom in the embedding, the outputs are also constrained to be centered on the origin: y i = 0.
i

(3)

Finally, the algorithm attempts to “unfold” the inputs by maximizing the variance var(y ) =
i

||yi ||

2

(4)

while preserving local distances and angles, as in eq. (2). Maximizing the variance of the embedding turns out to be a useful surrogate for minimizing its dimensionality (which is computationally less tractable). The above optimization can be formulated as an instance of semideﬁnite programming [20]. Let Kij = yi · yj denote the Gram (or kernel) matrix of the outputs. As shown in earlier work [21, 22], eqs. (2–4) can be written entirely in terms of the elements of this matrix. We can then learn the kernel matrix K by solving the following semideﬁnite program. Maximize trace(K ) subject to: 1) K 0. 2) Σij Kij = 0. 3) For all (i, j ) such that ηij = 1, 2 Kii − 2Kij + Kjj = ||xi − xj || . As in kernel PCA [15], the embedding is derived from the eigenvalues and eigenvectors of the kernel√ matrix; in particular, the algorithm outputs yαi = λα uαi , where λα and uα are the top d eigenvalues and eigenvectors. The dimensionality of the embedding, d, is suggested by the number of appreciably non-zero eigenvalues. In sum, the algorithm has three steps: (i) computing k -nearest neighbors; (ii) computing the kernel matrix;

2

Semideﬁnite Embedding

We brieﬂy review the algorithm for SDE; more details are given in previous work [21, 22]. As input, the algorithm takes high dimensional vectors {x1 , x2 , . . . , xn }; as output, it produces low dimensional vectors {y1 , y2 , . . . , yn }. The inputs xi ∈ RD are assumed to lie on or near a manifold that can be embedded in d dimensions, where typically d D. The goal of the algorithm is to estimate the dimensionality d and to output a faithful embedding that reveals the structure of the manifold. The main idea behind SDE has been aptly described as “maximum variance unfolding” [17]. The algorithm attempts to maximize the variance of its embedding, subject to the constraint that distances and angles between nearby inputs are preserved. The resulting

and (iii) computing its top eigenvectors. The computation time is typically dominated by the semideﬁnite program to learn the kernel matrix. In earlier work, this step limited us to problems with n ≈ 2000 examples and k ≤ 5 nearest neighbors; moreover, problems of this size typically required several hours of computation on a mid-range desktop computer.

{yi }n i=1 . In particular, denoting the unfolded landmarks by { α }m α=1 and the reconstructed outputs by {y ˆi }n , we argue that yi ≈ y ˆi , where: i=1 y ˆi =
α

Qiα

α.

(6)

3

Kernel Matrix Factorization

In practice, SDE scales poorly to large data sets because it must solve a semideﬁnite program over n × n matrices, where n is the number of examples. (Note that the computation time is prohibitive despite polynomial-time guarantees1 of convergence for semideﬁnite programming.) In this section, we show that for well-sampled manifolds, the kernel matrix K can be approximately factored as the product of smaller matrices. We then use this representation to derive much simpler semideﬁnite programs for the optimization in the previous section. 3.1 Sketch of algorithm

The connection between eqs. (5–6) will follow from the particular construction of the weighted graph that yields the linear transformation Q. This weighted graph is derived by appealing to the symmetries of linear reconstruction coeﬃcients; it is based on a similar intuition as the algorithm for manifold learning by locally linear embedding (LLE) [13, 14]. Finally, the kernel matrix factorization in eq. (1) follows if we make the approximation Kij = yi · yj ≈ y ˆi · y ˆj . (7)

In particular, substituting eq. (6) into eq. (7) gives the approximate factorization K ≈ QLQT , where Lαβ = α · β is the submatrix of inner products between (unfolded) landmark positions. 3.2 Reconstructing from landmarks

We begin by sketching the basic argument behind the factorization in eq. (1). The argument has three steps. First, we derive a linear transformation for approximately reconstructing the entire data set of high dimensional inputs {xi }n i=1 from m randomly chosen inputs designated as landmarks. In particular, denoting these landmarks by {µα }m α=1 , the reconstructed inputs {x ˆ i }n are given by the linear transformation: i=1 x ˆi =
α

Qiα µα .

(5)

The linear transformation Q is derived from a sparse weighted graph in which each node represents an input and the weights are used to propagate the positions of the m landmarks to the remaining n − m nodes. The situation is analogous to semi-supervised learning on large graphs [1, 16, 18, 26, 27], where nodes represent labeled or unlabeled examples and transductive inferences are made by diﬀusion through the graph. In our setting, the landmarks correspond to labeled examples, the reconstructed inputs to unlabeled examples, and the vectors µα to the actual labels. Next, we show that the same linear transformation can be used to reconstruct the unfolded data set—that is, after the mapping from inputs {xi }n i=1 to outputs
For the examples in this paper, we used the SDP solver CSDP v4.9 [4] with time complexity of O(n3 + c3 ) per iteration for sparse problems with n × n target matrices and c constraints. It seems, however, that large constant factors can also be associated with these complexity estimates.
1

To derive the linear transformation Q in eqs. (5–6), we assume the high dimensional inputs {xi }n i=1 are well sampled from a low dimensional manifold. In the neighborhood of any point, this manifold can be locally approximated by a linear subspace. Thus, to a good approximation, we can hope to reconstruct each input by a weighted sum of its r-nearest neighbors for some small r. (The value of r is analogous but not necessarily equal to the value of k used to deﬁne neighborhoods in the previous section.) Reconstruction weights can be found by minimizing the error function:
2

E (W ) =
i

xi −

j

Wij xj

,

(8)

subject to the constraint that j Wij = 1 for all j , and where Wij = 0 if xj is not an r-nearest neighbor of xi . The sum constraint on the rows of W ensures that the reconstruction weights are invariant to the choice of the origin in the input space. A small regularizer for weight decay can also be added to this error function if it does not already have a unique global minimum. Without loss of generality, we now identify the ﬁrst m inputs {x1 , x2 , . . . , xm } as landmarks {µ1 , µ2 , . . . , µm } and ask the following question: is it possible to reconstruct (at least approximately) the remaining inputs given just the landmarks µα and the weights Wij ? For suﬃciently large m, a unique reconstruction can be obtained by minimizing eq. (8) with respect to {xi }i>m .

To this end, we rewrite the reconstruction error as a function of the inputs, in the form: E (X ) =
ij

Φij xi · xj ,

(9)

where Φ = (In − W )T (In − W ) and In is the n × n identity matrix. It is useful to partition the matrix Φ into blocks distinguishing the m landmarks from the other (unknown) inputs:
m n−m

Maximize trace(QLQT ) subject to: 1) L 0. 2) Σij (QLQT )ij = 0. 3) For all (i, j ) such that ηij = 1, 2 (QLQT )ii − 2(QLQT )ij +(QLQT )jj ≤ ||xi − xj || . This optimization is nearly but not quite identical to the previous SDP up to the substitution K ≈ QLQT . The only diﬀerence is that we have changed the equality constraints in eq. (2) to inequalities. The SDP in section 2 is guaranteed to be feasible since all the constraints are satisﬁed by taking Kij = xi · xj (assuming the inputs are centered on the origin). Because the matrix factorization in eq. (1) is only approximate, however, here we must relax the distance constraints to preserve feasibility. Changing the equalities to inequalities is the simplest possible relaxation; the trivial solution Lαβ = 0 then provides a guarantee of feasibility. In practice, this relaxation does not appear to change the solutions of the SDP in a signiﬁcant way; the variance maximization inherent to the objective function tends to saturate the pairwise distance constraints, even if they are not enforced as strict equalities. To summarize, the overall procedure for unfolding the inputs xi based on the kernel matrix factorization in eq. (1) is as follows: (i) compute reconstruction weights Wij that minimize the error function in eq. (8); (ii) choose landmarks and compute the linear transformation Q in eq. (11); (iii) solve the SDP for the landmark kernel matrix L; (iv) derive a low dimensional embedding for the landmarks α from the eigenvectors and eigenvalues of L; and (v) reconstruct the outputs yi from eq. (6). The free parameters of the algorithm are the number of nearest neighbors r used to derive locally linear reconstructions, the number of nearest neighbors k used to generate distance constraints in the SDP, and the number of landmarks m (which also constrains the rank of the kernel matrix). In what follows, we will refer to this algorithm as landmark SDE, or simply SDE. SDE can be much faster than SDE because its main optimization is performed over m × m matrices, where m n. The computation time in semideﬁnite programming, however, depends not only on the matrix size, but also on the number of constraints. An apparent diﬃculty is that SDE and SDE have the same number of constraints; moreover, the constraints in the latter are not sparse, so that a naive implementation of SDE can actually be much slower than SDE. This diﬃculty is surmounted in practice by solving the semideﬁnite program for SDE while only explicitly monitoring a small fraction of the original constraints. To start, we feed an initial subset of constraints to

Φ

=

Φ Φu

Φu Φuu

(10)

In terms of this matrix, the solution with minimum reconstruction error is given by the linear transformation in eq. (5), where: Q= Im (Φuu )−1 Φul . (11)

An example of this minimum error reconstruction is shown in Fig. 1. The ﬁrst two panels show n = 10000 inputs sampled from a Swiss roll and their approximate reconstructions from eq. (5) and eq. (11) using r = 12 nearest neighbors and m = 40 landmarks. Intuitively, we can imagine the matrix Φij in eq. (9) as deﬁning a sparse weighted graph connecting nearby inputs. The linear transformation reconstructing inputs from landmarks is then analogous to the manner in which many semi-supervised algorithms on graphs propagate information from labeled to unlabeled examples. To justify eq. (6), we now imagine that the data set has been unfolded in a way that preserves distances and angles between nearby inputs. As noted in previous work [13, 14], the weights Wij that minimize the reconstruction error in eq. (8) are invariant to translations and rotations of each input and its r-nearest neighbors. Thus, roughly speaking, if the unfolding looks locally like a rotation plus translation, then the same weights Wij that reconstruct the inputs xi from their neighbors should also reconstruct the outputs yi from theirs. This line of reasoning yields eq. (6). It also suggests that if we could somehow learn to faithfully embed just the landmarks in a lower dimensional space, the remainder of the inputs could be unfolded by a simple matrix multiplication. 3.3 Embedding the landmarks

It is straightforward to reformulate the semideﬁnite program (SDP) for the kernel matrix Kij = yi · yj in section 2 in terms of the smaller matrix Lαβ = α · β . In particular, appealing to the factorization K ≈ QLQT , we consider the following SDP:

word one may men iraq drugs january germany recession california republican government

four nearest neighbors two, three, four, six won’t, cannot, would, will passengers, soldiers, oﬃcers, lawmakers states, israel, china, noriega computers, missiles, equipment, programs july, october, august, march canada, africa, arabia, marks environment, yen, season, afternoon minnesota, arizona, ﬂorida, georgia democratic, strong, conservative, phone pentagon, airline, army, bush

Figure 1: (1) n = 10000 inputs sampled from a Swiss roll; (2) linear reconstruction from r = 12 nearest neighbors and m = 40 landmarks (denoted by black x’s); (3) embedding from SDE, with distance and angle constraints to k = 4 nearest neighbors, computed in 16 minutes. the SDP solver, consisting only of the semideﬁniteness constraint, the centering constraint, and the distance constraints between landmarks and their nearest neighbors. If a solution is then found that violates some of the unmonitored constraints, these are added to the problem, which is solved again. The process is repeated until all the constraints are satisﬁed. Note that this incremental scheme is made possible by the relaxation of the distance constraints from equalities to inequalities. As in the large-scale training of support vector machines [6], it seems that many of the constraints in SDE are redundant, and simple heuristics to prune these constraints can yield orderof-magnitude speedups. (Note, however, that the centering and semideﬁniteness constraints in SDE are always enforced.)

Table 1: Selected words and their four nearest neighbors (in order of increasing distance) after nonlinear dimensionality reduction by SDE. The d = 5 dimensional embedding of D = 60000 dimensional bigram distributions was computed by SDE in 35 minutes (with n = 2000, k = 4, r = 12, and m = 30).

had to be explicitly enforced by the SDP solver to ﬁnd a feasible solution. Interestingly, similarly faithful embeddings were obtained in shorter times using as few as m = 10 landmarks, though the input reconstructions in these cases were of considerably worse quality. Also worth mentioning is that adding low variance Gaussian noise to the inputs had no signiﬁcant impact on the algorithm’s performance. The second data set was created from the n = 2000 most common words in the ARPA North American Business News corpus. Each of these words was represented by its discrete probability distribution over the D = 60000 words that could possibly follow it. The distributions were estimated from a maximum likelihood bigram model. The embedding of these high dimensional distributions was performed by SDE (with k = 4, r = 12, and m = 30) in about 35 minutes; the variance of the embedding, as revealed by the eigenvalue spectrum of the landmark kernel matrix, was essentially conﬁned to d = 5 dimensions. Table 1 shows a selection of words and their four nearest neighbors in the low dimensional embedding. Despite the massive dimensionality reduction from D = 60000 to d = 5, many semantically meaningful neighborhoods are seen to be preserved. The third experiment was performed on n = 400 color images of a teapot viewed from diﬀerent angles in the plane. Each vectorized image had a dimensionality of D = 23028, resulting from 3 bytes of color information for each of 76 × 101 pixels. In previous work [22] it was shown that SDE represents the angular mode of variability in this data set by an almost perfect circle. Fig. 2 compares embeddings from SDE (k = 4, r = 12, m = 20) with normal SDE (k = 4) and LLE (r = 12). The eigenvalue spectrum of SDE is very

4

Experimental Results

Experiments were performed in MATLAB to evaluate the performance of SDE on various data sets. The SDPs were solved with the CSDP (v4.9) optimization toolbox [4]. Of particular concern was the speed and accuracy of SDE relative to earlier implementations of SDE. The ﬁrst data set, shown in the top left panel of Fig. 1, consisted of n = 10000 inputs sampled from a three dimensional “Swiss roll”. The other panels of Fig. 1 show the input reconstruction from m = 40 landmarks and r = 12 nearest neighbors, as well as the embeddingobtained in SDE by constraining distances and angles to k = 4 nearest neighbors. The computation took 16 minutes on a mid-range desktop computer. Table 2 shows that only 1205 out of 43182 constraints

Figure 3: Top: Error rate of ﬁve-nearest-neighbors classiﬁcation on the test set of USPS handwritten digits. The error rate is plotted against the dimensionality of embeddings from PCA and SDE (with k = 4, r = 12, m = 10). It can be seen that SDE preserves the neighborhood structure of the digits fairly well with only a few dimensions. Bottom: Normalized eigenvalue spectra from SDE and PCA. The latter reveals many more dimensions with appreciable variance.

Figure 4: Relative speedup of SDE versus SDE on data sets with diﬀerent numbers of examples (n) and landmarks (m). Speedups of two orders of magnitude are observed on larger data sets. On small data sets, however, SDE can be faster than SDE. of this size. To evaluate the embeddings from SDE, we compared their nearest neighbor classiﬁcation error rates to those of PCA. The top plot in Fig. 3 shows the classiﬁcation error rate (using ﬁve nearest neighbors in the training images to classify test images) versus the dimensionality of the embeddings from SDE and PCA. The error rate from SDE drops very rapidly with dimensionality, nearly matching the error rate on the actual images with only d = 3 dimensions. By contrast, PCA requires d = 12 dimensions to overtake the performace of SDE. The bar plot at the bottom of Fig. 3 shows the normalized eigenvalue spectra from both SDE and PCA. From this plot, it is clear that SDE concentrates the variance of its embedding in many fewer dimensions than PCA. When does SDE outperform SDE? Figure 4 shows the speedup of SDE versus SDE on several data sets. Not surprisingly, the relative speedup grows in proportion with the size of the data set. Small data sets (with n < 500) can generally be unfolded faster by SDE, while larger data sets (with 500 < n < 2000) can be unfolded up to 400 times faster by SDE. For even larger data sets, only SDE remains a viable option.

similar to that of SDE, revealing that the variance of the embedding is concentrated in two dimensions. The results from SDE do not exactly reproduce the results from SDE on this data set, but the diﬀerence becomes smaller with increasing number of landmarks (at the expense of more computation time). Actually, as shown in Fig. 4, SDE (which took 79 seconds) is slower than SDE on this particular data set. The increase in computation time has two simple explanations that seem peculiar to this data set. First, this data set is rather small, and SDE incurs some overhead in its setup that is only negligible for large data sets. Second, this data set of images has a particular cyclic structure that is easily “broken” if the monitored constraints are not sampled evenly. Thus, this particular data set is not well-suited to the incremental scheme for adding unenforced constraints in SDE; a large number of SDP reruns are required, resulting in a longer overall computation time than SDE. (See Table 2.) The ﬁnal experiment was performed on the entire data set of n = 9298 USPS handwritten digits [10]. The inputs were 16 × 16 pixel grayscale images of the scanned digits. Table 2 shows that only 690 out of 61735 inequality constraints needed to be explicitly monitored by the SDP solver for SDE to ﬁnd a feasible solution. This made it possible to obtain an embedding in 40 minutes (with k = 4, r = 12, m = 10), whereas earlier implementations of SDE could not handle problems

5

Conclusion

In this paper, we have developed a much faster algorithm for manifold learning by semideﬁnite programming. There are many aspects of the algorithm that we are still investigating, including the interplay between the number and placement of landmarks, the deﬁnition of local neighborhoods, and the quality of

Figure 2: Comparison of embeddings from SDE, LLE and SDE for n = 400 color images of a rotating teapot. The vectorized images had dimension D = 23028. LLE (with r = 12) and SDE (with k = 4, r = 12, m = 20) yield similar but slightly more irregular results than SDE (with k = 4). The normalized eigenspectra in SDE and SDE (i.e., the eigenspectra divided by the trace of their kernel matrices) reveal that the variances of their embeddings are concentrated in two dimensions; the eigenspectrum from LLE does not reveal this sort of information. data set teapots bigrams USPS digits Swiss roll n 400 2000 9298 10000 m 20 30 10 20 constraints 1599 11170 61735 43182 monitored 565 1396 690 1205 time (secs) 79 2103 2420 968

Table 2: Total number of constraints versus number of constraints explicitly monitored by the SDP solver for SDE on several data sets. The numbers of inputs (n) and landmarks (m) are also shown, along with computation times. The speedup of SDE is largely derived from omitting redundant constraints. the resulting reconstructions and embeddings. Nevertheless, our initial results are promising and show that manifold learning by semideﬁnite programming can scale to much larger data sets than we originally imagined in earlier work [21, 22]. Beyond the practical applications of SDE, the framework in this paper is interesting in the way it combines ideas from several diﬀerent lines of recent work. SDE is based on the same appeals to symmetry at the heart of LLE [13, 14] and SDE [21, 22]. The linear reconstructions that yield the factorization of the kernel matrix in eq. (1) are also reminiscent of semi-supervised algorithms for propagating labeled information through large graphs of unlabeled examples [1, 16, 18, 26, 27]. Finally, though based on a somewhat diﬀerent intuition, the computational gains of SDE are similar to those obtained by landmark methods for Isomap [7]. While we have applied SDE (in minutes) to data sets with as many as n = 10000 examples, there exist many larger data sets for which the algorithm remains impractical. Further insights are therefore required. In related work, we have developed a simple out-of-sample extension for SDE, analogous to similar extensions for other spectral methods [3]. Algorithmic advances may also emerge from the dual formulation of “maximum variance unfolding” [17], which is related to the problem of computing fastest mixing Markov chains on graphs. We are hopeful that a combination of complementary approaches will lead to even faster and more powerful algorithms for manifold learning by semideﬁnite programming. Acknowledgments We are grateful to Ali Jadbabaie (University of Pennsylvania) for several discussions about semideﬁnite programming and to the anonymous reviewers for many useful comments.

References
[1] M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised learning on large graphs. In Proceedings of the Seventeenth Annual Conference on Computational Learning Theory (COLT 2004), pages 624–638, Banﬀ, Canada, 2004. [2] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003.

[3] Y. Bengio, J-F. Paiement, and P. Vincent. Out-ofsample extensions for LLE, Isomap, MDS, eigenmaps, and spectral clustering. In S. Thrun, L. K. Saul, and B. Sch¨ olkopf, editors, Advances in Neural Information Processing Systems 16, Cambridge, MA, 2004. MIT Press. [4] B. Borchers. CSDP, a C library for semideﬁnite programming. Optimization Methods and Software 11(1):613-623, 1999. [5] M. Brand. Charting a manifold. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 985– 992, Cambridge, MA, 2003. MIT Press. [6] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, Cambridge, UK, 2000. [7] V. de Silva and J. B. Tenenbaum. Global versus local methods in nonlinear dimensionality reduction. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 721–728, Cambridge, MA, 2003. MIT Press. [8] D. L. Donoho and C. E. Grimes. Hessian eigenmaps: locally linear embedding techniques for highdimensional data. Proceedings of the National Academy of Arts and Sciences, 100:5591–5596, 2003. [9] J. Ham, D. D. Lee, S. Mika, and B. Sch¨ olkopf. A kernel view of the dimensionality reduction of manifolds. In Proceedings of the Twenty First International Conference on Machine Learning (ICML-04), pages 369–376, Banﬀ, Canada, 2004. [10] J. J. Hull. A database for handwritten text recognition research. IEEE Transaction on Pattern Analysis and Machine Intelligence, 16(5):550–554, May 1994. [11] J. C. Platt. Fast embedding of sparse similarity graphs. In S. Thrun, L. K. Saul, and B. Sch¨ olkopf, editors, Advances in Neural Information Processing Systems 16, Cambridge, MA, 2004. MIT Press. [12] J. C. Platt. FastMap, MetricMap, and landmark MDS are all nystr¨ om algorithms. In Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, Barbados, WI, January 2005. [13] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290:2323–2326, 2000. [14] L. K. Saul and S. T. Roweis. Think globally, ﬁt locally: unsupervised learning of low dimensional manifolds. Journal of Machine Learning Research, 4:119– 155, 2003. [15] B. Sch¨ olkopf, A. J. Smola, and K.-R. M¨ uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998. [16] A. J. Smola and R. Kondor. Kernels and regularization on graphs. In Proceedings of the Sixteenth Annual Conference on Computational Learning Theory and Kernel Workshop, Washington D.C., 2003.

[17] J. Sun, S. Boyd, L. Xiao, and P. Diaconis. The fastest mixing Markov process on a graph and a connection to a maximum variance unfolding problem. SIAM Review, submitted. [18] M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with Markov random walks. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press. [19] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000. [20] L. Vandenberghe and S. P. Boyd. Semideﬁnite programming. SIAM Review, 38(1):49–95, March 1996. [21] K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semideﬁnite programming. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR04), volume 2, pages 988–995, Washington D.C., 2004. [22] K. Q. Weinberger, F. Sha, and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In Proceedings of the Twenty First International Conference on Machine Learning (ICML-04), pages 839–846, Banﬀ, Canada, 2004. [23] C. K. I. Williams. On a connection between kernel PCA and metric multidimensional scaling. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 675–681, Cambridge, MA, 2001. MIT Press. [24] Christopher K. I. Williams and Matthias Seeger. Using the Nystr¨ om method to speed up kernel machines. In T. Leen, T. Dietterich, and V. Tresp, editors, Neural Information Processing Systems 13, pages 682– 688, Cambridge, MA, 2001. MIT Press. [25] Z. Zhang and H. Zha. Principal manifolds and nonlinear dimensionality reduction by local tangent space alignment. SIAM Journal of Scientiﬁc Computing, in press. [26] D. Zhou, O. Bousquet, T. N. Lai, J. Weston, and B. Sch¨ olkopf. Learning with local and global consistency. In S. Thrun, L. K. Saul, and B. Sch¨ olkopf, editors, Advances in Neural Information Processing Systems 16, pages 321–328, Cambridge, MA, 2004. MIT Press. [27] X. Zhu, Z. Ghahramani, and J. Laﬀerty. Semisupervised learning using Gaussian ﬁelds and harmonic functions. In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2003), pages 912–919, Washington D.C., 2003.

