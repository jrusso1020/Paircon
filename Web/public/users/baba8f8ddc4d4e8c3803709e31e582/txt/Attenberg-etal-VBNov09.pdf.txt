VIRUS BULLETIN www.virusbtn.com

FEATURE
COLLABORATIVE SPAM FILTERING WITH THE HASHING TRICK
Josh Attenberg Polytechnic Institute of NYU, USA Kilian Weinberger, Alex Smola, Anirban Dasgupta, Martin Zinkevich Yahoo! Research, USA
User feedback is vital to the quality of the collaborative spam ﬁlters frequently used in open membership email systems such as Yahoo Mail or Gmail. Users occasionally designate emails as spam or non-spam (often termed as ham), and these labels are subsequently used to train the spam ﬁlter. Although the majority of users provide very little data, as a collective the amount of training data is very large (many millions of emails per day). Unfortunately, there is substantial deviation in users’ notions of what constitutes spam and ham. Additionally, the open membership policy of these systems makes it vulnerable to users with malicious intent – spammers who wish to see their emails accepted by any spam ﬁltration system can create accounts and use these to give malicious feedback to ‘train’ the spam ﬁlter in giving their emails a free pass. When combined, these realities make it extremely difﬁcult to assemble a single, global spam classiﬁer. The aforementioned problems could be avoided entirely if we could create a completely separate classiﬁer for each user based solely on that user’s feedback. Unfortunately, few users provide the magnitude of feedback required for this approach (many not providing any feedback at all). The number of emails labelled by an individual user approximates a power law distribution. Purely individualized classiﬁers offer the possibility of excellent performance to a few users with many labelled emails, at the expense of the great many users whose classiﬁers will become unreliable due to a lack of training data. This article illustrates a simple and effective technique that is able to balance the wide coverage provided by a global spam ﬁlter with the ﬂexibility provided by personalized ﬁlters. By using ideas from multi-task learning, we build a hybrid method that combines both global and personalized ﬁlters. By training both the collection of personal and global classiﬁers simultaneously we are able to accommodate the idiosyncrasies of each user, as well as provide a global classiﬁer for users that label few emails. In fact, as is well known in multi-task learning [2], in addition to improving the experience of users who label many examples, this multi-task learning approach actually mitigates the impact of malicious users on the global ﬁlter. By offering speciﬁc consideration to the intents of the most active and unusual users, a global classiﬁer is created that focuses on the truly common aspects of the classiﬁcation problem. The end result is improved classiﬁer performance for everyone, including users who label relatively few emails. With large-scale open membership email systems such as Yahoo Mail, one of the main hurdles to a hybrid personal/ global spam ﬁlter is the enormous amount of memory required to store individual classiﬁers for every user. We circumvent this obstacle with the use of the hashing trick [1, 5]. The hashing trick allows a ﬁxed amount of memory to store all of the parameters for all the personal classiﬁers and a global classiﬁer by mapping all personal and global features into a single low-dimensional feature space, in a way which bounds the required memory independently of the input. In this space, a single parameter vector, w, is trained which captures both global spam activity and the individual aspects of all active users. Feature hashing provides an extremely simple means of dimensionality reduction, eliminating the large word-to-dimension dictionary data structure typically needed for text-based classiﬁcation, providing substantial savings in both complexity and available system memory.

1. HASHING-TRICK
The standard way to represent instances (i.e. emails) in text classiﬁcation is the so-called bag-of-words approach. This method assumes the existence of a dictionary that contains all possible words and represents an email as a very large vector, , with as many entries as there are words in the dictionary. For a speciﬁc email, the ith entry in the vector contains the number of occurrences of word i in the email. Naturally, this method lends itself to very sparse representations of instances and examples – as the great majority of words do not appear in any speciﬁc text, almost all entries in the data vectors are zero. However, when building a classiﬁer one often has to maintain information on all words in the entire corpus (e.g. in a weight vector), and this can become unmanageable in large corpora. The hashing trick is a dimensionality reduction technique used to give traditional learning algorithms a foothold in high dimensional input spaces (i.e. in settings with large dictionaries), by reducing the memory footprint of learning, and reducing the inﬂuence of noisy features. The main idea behind the hashing trick is simple and intuitive: instead of generating bag-of-word feature vectors through a dictionary that maps tokens to word indices, one uses a hash function that hashes words directly into a feature vector of size b. The hash function

18

NOVEMBER 2009

VIRUS BULLETIN www.virusbtn.com

h : {Strings} → [1..b] operates directly on strings and should be approximately uniform1. In [5] we propose using a second independent hash function ξ : {Strings} → {-1, 1}, that determines whether the particular hashed dimension of a token should be incremented or decremented. This causes the hashed feature vectors to be unbiased, since the expectation of the noise for any entry is zero. The algorithm below shows a pseudo-code implementation of the hashing trick that generates a hashed bag-of-words feature vector for an email: hashingtrick([string] email) = for word in email do i = h(word) = + ξ(word) end for return The key point behind this hashing is that every hashed feature effectively represents an inﬁnite number of unhashed features. It is the mathematical equivalent of a group of homographs (e.g. lie and lie) or homophones (e.g. you’re and your) – words with different meanings that look or sound alike. It is important to realize that having two meanings of the same feature is no more and no less of an issue than a homograph or homophone: if a computer can guess the meaning of the feature, or more importantly, the impact of the feature on the label of the message, it will change its decision based upon the feature. If not, then it will try to make its decision based on the rest of the email. The wonderful thing about hashing is that instead of trying to cram a lot of different conﬂicting meanings into short words as humans do, we are trying to randomly spread the meanings evenly into over a million different features in our hashed language. So, although a word like ‘antidisestablishmentarianism’ might accidentally run into ‘the’, our hashing function is a lot less likely to make two meaningful words homographs in our hashed language than those already put there by human beings. Of course there are so many features in our hashed language, that in the context of spam detection most features won’t mean anything at all. In the context of email spam ﬁltering, the hashing trick by itself has several great advantages over the traditional dictionary-based bag-of-words method: 1. It considers even low-frequency tokens that might traditionally be ignored to keep the dictionary manageable – this is especially useful in view of attacks by spammers using rare variants of words
1 For the experiments in this paper we used a public domain implementation of a hash function from http://burtleburtle.net/bob/ hash/doobs.html.

(e.g. misspellings like ‘viogra’). 2. Hashing the terms makes a classiﬁer agnostic to changes in the set of terms used, and if the spam classiﬁer is used in an online setting, the hashing trick equips the classiﬁer with a dictionary of effectively inﬁnite size, which helps it adapt naturally to changes in the language of spam and ham. 3. By associating many raw words in its ‘inﬁnite’ dictionary (most of which never occur) with a single parameter, the meaning of this parameter changes depending upon which words are common, rare, or absent from the corpus. So, how large a language can this hashing trick handle? As we show in the next section, if it allows us to ‘square’ the number of unhashed features we may possibly see, it could help us handle personalization.

2. PERSONALIZATION
As the hashing trick frees up a lot of memory, the number of parameters a spam classiﬁer can manage increases. In fact, we can train multiple classiﬁers which ‘share’ the same parameter space [5]. For a set of users, U, and a dictionary , that is size d, our goal is to train one global classiﬁer, shared amongst all users and one local classiﬁer, , for each user u ∈U. In a system with | U | users, we need | U | + 1 classiﬁers. When an email arrives, it is classiﬁed by the combination of the recipient’s local classiﬁer and the global – we call this the hybrid classiﬁer. classiﬁer + Traditionally, this goal would be very hard to achieve, as has d parameters, and hence the total each classiﬁer number of parameters we need to store becomes (| U | + 1)d. Systems like Yahoo Mail handle billions of emails for hundreds of millions of users per day. With millions of users and millions of words, storing all vectors would require hundreds of terabytes of parameters. Further, to load the appropriate classiﬁer for any given user in time when an email arrives would be prohibitively expensive. The hashing trick provides a convenient solution to the aforementioned complexity, allowing us to perform personalized and global spam ﬁltration in a single hashed bag-of-words representation. Instead of training | U | + 1 classiﬁers, we train a single classiﬁer with a very large feature space. For each email, we create a personalized bag of words by concatenating the recipient’s user id to each word of the email2, and add to this the traditional global bag of words. All the elements in these bags are hashed into one of b buckets to form a b-dimensional representation of the email, which is then fed into the classiﬁer. Effectively, this process allows | U | +1 classiﬁers to share a b-dimensional parameter space nicely [5]. It is important to point out that
2

We use the º symbol to indicate string concatenation.

NOVEMBER 2009

19

VIRUS BULLETIN www.virusbtn.com

the one classiﬁer – over b hashed features – is trained after hashing. Because b will be much smaller than d x | U |, there will be many hash collisions. However, because of the sparsity and high redundancy of each email, we can show that the theoretical number of possible collisions does not really matter for most of the emails. Moreover, because the classiﬁer is aware of any collisions before the weights are learned, the classiﬁer is not likely to put weights of high magnitude on features with an ambiguous meaning. Figure 2: The results of the global and hybrid classiﬁers applied to a large-scale real-world data set of 3.2 million emails. trec07p benchmark data set, and on a large-scale proprietary data set representing the realities of an open-membership email system. The trec data set contains 75,419 labelled and chronologically ordered emails taken from a single email server over four months in 2007 and compiled for trec spam ﬁltering competitions [3]. Our proprietary data was collected over 14 days and contains n = 3.2 million anonymized emails from | U | = 400,000 anonymized users. Here the ﬁrst ten days are used for training, and the last four days are used for experimental validation. Emails are either spam (positive) or ham (non-spam, negative). All spam ﬁlter experiments utilize the Vowpal Wabbit (VW) [4] linear classiﬁer trained with stochastic gradient descent on a squared loss. Note that the hashing trick is independent of the classiﬁcation scheme used; the hashing trick could apply equally well with many learning-based spam ﬁltration solutions. To analyse the performance of our classiﬁcation scheme we evaluate the spam catch rate (SCR, the percentage of spam emails detected) of our classiﬁer at a ﬁxed 1% ham misclassiﬁcation rate (HMR, the percentage of good emails erroneously labelled as spam). We note that the proprietary nature of the latter data set precludes publishing of exact performance numbers. Instead we compare the performance to a baseline classiﬁer, a global classiﬁer hashed onto b = 226 dimensions. Since 226 is far larger than the actual number of terms used, d = 40M, we believe this is representative of full-text classiﬁcation without feature hashing.

Figure 1: Global/personal hybrid spam ﬁltering with feature hashing. Intuitively, the weights on the individualized tokens (i.e. those that are concatenated with the recipient’s id) indicate the personal eccentricities of the particular users. Imagine for example that user ‘barney’ likes emails containing the word ‘viagra’, whereas the majority of users do not. The personalized hashing trick will learn that ‘viagra’ itself is a spam indicative word, whereas ‘viagra_barney’ is not. The entire process is illustrated in Figure 1. See the algorithm below for details on a pseudo-code implementation of the personalized hashing trick. Note that with the personalized hashing trick, using a hash function h : {Strings} → [1..b], we only require b parameters independent of how many users or words appear in our system. personalized_hashingtrick(string userid, [string] email) = for word in email do i = h(word) = + ξ(word) j = h(word userid) + ξ(word = end for return userid)

4. THE VALIDITY OF HASHING IN EMAIL SPAM FILTERING
To measure the performance of the hashing trick and the inﬂuence of aggressive dimensionality reduction on classiﬁer quality, we compare global classiﬁer performance to that of our baseline classiﬁer when hashing onto spaces of dimension b = {218, 220, 222, 224, 226} on our proprietary data

3. EXPERIMENTAL SET-UP AND RESULTS
To assess the validity of our proposed techniques, we conducted a series of experiments on the freely distributed

20

NOVEMBER 2009

VIRUS BULLETIN www.virusbtn.com

In Section 2, we hypothesized that using a hybrid spam classiﬁer could mitigate the idiosyncrasies of the most active spam labellers, thereby creating a more general classiﬁer for the remaining users, beneﬁting everyone. To validate this claim, we segregate users according to the number of training labels provided in our proprietary data. As before, a hybrid classiﬁer is trained with b = {218, 220, 222, 224, 226} bins. The results of this experiment are seen in Figure 3. Note that for small b, it does indeed appear that the most active users beneﬁt at the expense of those with few labelled examples. However, as b increases, therefore reducing the noise due to hash collisions, users with no or very few examples in the training set also beneﬁt from the added personalization. This improvement can be explained if we recall the subjective nature of spam and ham – users do not always agree, especially in the case of business emails or newsletters. Additionally, spammers may have inﬁltrated the data set with malicious labels. The hybrid classiﬁer absorbs these peculiarities with the personal component, freeing the global component to truly reﬂect a common deﬁnition of spam and ham and leading to better overall generalization, which beneﬁts all users.

Figure 3: The amount of spam left in users’ inboxes, relative to the baseline. The users are binned by the amount of training data they provide. set. The results of this experiment are displayed as the blue line in Figure 2. Note that using 218 bins results in only an 8% reduction in classiﬁer performance, despite large numbers of hash collisions. Increasing b to 220 improves the performance to within 3% of the baseline. Given that our data set has 40M unique tokens, this means that using a weight vector of 0.6% of the size of the full data results in approximately the same performance as a classiﬁer using all dimensions. Previously, we have proposed using hybrid global/personal spam ﬁltering via feature hashing as a means for effectively mitigating the effects of differing opinions of spam and ham amongst a population of email users. We now seek to verify the efﬁcacy of these techniques in a realistic setting. On our proprietary data set, we examine the techniques illustrated in Section 2 and display the results as the red line in Figure 2. Considering that our hybrid technique results from the cross product of | U | = 400K users and d = 40M tokens, a total of 16 trillion possible features, it is understandable that noise induced by collisions in the hash table adversely affects classiﬁer performance when b is small. As the number of hash bins grows to 222, personalization already offers a 30% spam reduction over the baseline, despite aggressive hashing. In any open email system, the number of emails labelled as either spam or non-spam varies greatly among users. Overall, the labelling distribution approximates a power law distribution. With this in mind, one possible explanation for the improved performance of the hybrid classiﬁer in Figure 2 could be that we are heavily beneﬁting those few users with a rich set of personally labelled examples, while the masses of email users – those with few labelled examples – actually suffer. In fact, many users do not appear at all during training time and are only present in our test set. For these users, personalized features are mapped into hash buckets with weights set exclusively by other examples, resulting in some interference being added to the global spam prediction.

5. MITIGATING THE ACTIONS OF MALICIOUS USERS WITH HYBRID HASHING
In order to simulate the inﬂuence of deliberate noise in a controlled setting we performed additional experiments on the trec data set. We chose some percentage, mal, of ‘malicious’ users uniformly at random from the pool of email receivers, and set their email labels at random. Note that having malicious users label randomly is actually a harder case than having them label them adversarially in a consistent fashion – as then the personalized spam ﬁlter could potentially learn and invert their preferences. Figure 4 presents a comparison of global and hybrid spam ﬁlters under varying loads of malicious activity and different sized hash tables. Here we set mal ∈ {0%, 20%, 40%}. Note that malicious activity does indeed harm the overall spam ﬁlter performance for a ﬁxed classiﬁer conﬁguration. The random nature of our induced malicious activity leads to a ‘background noise’ occurring in many bins of our hash table, increasing the harmful nature of collisions. Both global and hybrid classiﬁers can mitigate this impact somewhat if the number of hash bins b is increased. In short, with malicious users, both global (dashed line) and hybrid (solid line) classiﬁers require more hash bins to achieve near-optimum performance. Since the hybrid classiﬁer has more tokens, the number of hash collisions is also correspondingly larger. Given a large enough number of hash bins, the hybrid classiﬁer clearly outperforms the single global classiﬁer under the malicious

NOVEMBER 2009

21

VIRUS BULLETIN www.virusbtn.com

Figure 4: The inﬂuence of the number of hash bins on global and hybrid classiﬁer performance with varying percentages of malicious users. settings. We do not include the results of a pure local approach, as the performance is abysmal for many users due to a lack of training data.

6. CONCLUSION
This work demonstrates the hashing trick as an effective method for collaborative spam ﬁltering. It allows spam ﬁltering without the necessity of a memory-consuming dictionary and strictly bounds the overall memory required by the classiﬁer. Further, the hashing trick allows the compression of many (thousands of) classiﬁers into a single, ﬁnite-sized weight vector. This allows us to run personalized and global classiﬁcation together with very little additional computational overhead. We provide strong empirical evidence that the resulting classiﬁer is more robust against noise and absorbs individual preferences that are common in the context of open-membership spam classiﬁcation.

REFERENCES
[1] Attenberg, J.; Weinberger, K.; Dasgupta, A.; Smola. A.; Zinkevich, M. Collaborative email-spam ﬁltering with the hashing trick. Proceedings of the Sixth Conference on Email and Spam, CEAS 2009, 2009. Caruana, R. Algorithms and applications for multitask learning. Proc. Intl. Conf. Machine Learning, pp.87–95. Morgan Kaufmann, 1996. Cormack, G. TREC 2007 spam track overview. The Sixteenth Text REtrieval Conference (TREC 2007) Proceedings, 2007. Langford, J.; Li, L.; Strehl, A. Vowpal Wabbit online learning project. http://hunch.net/?p=309, 2007. Weinberger, K.; Dasgupta, A.; Attenberg, J.; Langford, J.; Smola, A. Feature hashing for large scale multitask learning. ICML, 2009.

[2]

[3]

[4] [5]

22

NOVEMBER 2009

