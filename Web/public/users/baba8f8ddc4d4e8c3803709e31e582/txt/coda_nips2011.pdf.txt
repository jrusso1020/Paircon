Co-Training for Domain Adaptation

Minmin Chen, Kilian Q. Weinberger Department of Computer Science and Engineering Washington University in St. Louis St. Louis, MO 63130 mc15,kilian@wustl.edu

John C. Blitzer Google Research 1600 Amphitheatre Parkway Mountain View, CA 94043 blitzer@google.com

Abstract
Domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain. In many practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain. In this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding to the training set both the target features and instances in which the current algorithm is the most conﬁdent. Our algorithm is a variant of co-training [7], and we name it CODA (Co-training for domain adaptation). Unlike the original co-training work, we do not assume a particular feature split. Instead, for each iteration of cotraining, we formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a subset of source and target features to include in the predictor. CODA signiﬁcantly out-performs the state-of-the-art on the 12-domain benchmark data set of Blitzer et al. [4]. Indeed, over a wide range (65 of 84 comparisons) of target supervision CODA achieves the best performance.

1

Introduction

Domain adaptation addresses the problem of generalizing from a source distribution for which we have ample labeled training data to a target distribution for which we have little or no training data [3, 15, 29]. Domain adaptation is of practical importance in many areas of applied machine learning, ranging from computational biology [18] to natural language processing [11, 20] to computer vision [24]. In this work, we focus primarily on domain adaptation problems that are characterized by missing features. This is often the case in natural language processing, where different genres often use very different vocabulary to describe similar concepts. For example, in our experiments we use the sentiment data of Blitzer et al. [4], where a breeze to use is a way to express positive sentiment about kitchen appliances, but not about books. In this situation, most domain adaptation algorithms seek to eliminate the difference between source and target distributions, either by re-weighting source instances [15, 19] or learning a new feature representation [6, 29]. We present an algorithm which differs from both of these approaches. Our method seeks to slowly adapt its training set from the source to the target domain, using ideas from co-training. We accomplish this in two ways: First, we train on our own output in rounds, where at each round, we include in our training data the target instances we are most conﬁdent of. Second, we select a subset of shared source and target features based on their compatibility. Different from most previous work on selecting features for domain adaptation, the compatibility is measured across the training set and the unlabeled set, instead of across the two domains. As more target instances are added to the training set, target speciﬁc features become compatible across the two sets, therefore are included in the predictor. Finally, we exploit the pseudo multiview co-training algorithm of Chen et al. [10] 1

to exploit the unlabeled data efﬁciently. These three intuitive ideas can be combined in a single optimization problem. We name our algorithm CODA (Co-Training for Domain Adaptation). By allowing us to slowly change our training data from source to target, CODA has an advantage over representation-learning algorithms [6, 29], since they must decide a priori what the best representation is. In contrast, each iteration of CODA can choose exactly those few target features which can be related to the current (source and pseudo-labeled target) training set. We ﬁnd that in the sentiment prediction data set of Blitzer et al. [4] CODA improves the state-of-the-art cross widely varying amounts of target labeled data in 65 out of 84 settings.

2

Notation and Setting

We assume our data originates from two domains, Source (S) and Target (T). The source data is fully labeled DS = {(x1 , y1 ), . . . , (xns , yns )} ⊂ Rd × Y and sampled from some distribul tion PS (X, Y ). The target data is sampled from PT (X, Y ) and is divided into labeled DT = d u d {(x1 , y1 ), . . . , (xnt , ynt )} ⊂ R × Y and unlabeled DT = {(x1 , ?), . . . (xmt , ?)} ⊂ R × Y parts, where in the latter the labels are unknown during training time. Both domains are of equal dimensionality d. Our goal is to learn a classiﬁer h ∈ H to accurately predict the labels on the unlabeled portion of DT , but also to extend to out-of-sample test points, such that for any (x, y ) sampled from PT , we have h(x) = y with high probability. For simplicity we assume that Y = {+1, −1}, although our method can easily be adapted to multi-class or regression settings. We assume the existence of a base classiﬁer, which determines the set H. Throughout this paper we simply use logistic regression, i.e. our classiﬁer is parameterized by a weight-vector w ∈ Rd and deﬁned as hw (x) = (1 + e−w x )−1 . The weights w are set to minimize the loss function (w ; D ) = − 1 |D| log(1 + exp(−y w x)).
(x,y )∈D

(1)

If trained on data sampled from PS (X, Y ), logistic regression models the distribution PS (Y |X ) [14] through Ph (Y = y |X = x; w) = (1 + e−w xy )−1 . In this paper, our goal is to adapt this classiﬁer to the target distribution PT (Y |X ).

3

Method

In this section, we begin with a semi-supervised approach and describe the rote-learning procedure to automatically annotate target domain inputs. The algorithm maintains and grows a training set that is iteratively adapted to the target domain. We then incorporate feature selection into the optimization, a crucial element of our domain-adaptation algorithm. The feature selection addresses the change in distribution and support from PS to PT . Further, we introduce pseudo multi-view co-training [7, 10], which improves the rote-learning procedure by adding inputs with features that are still not used effectively by the current classiﬁer. We use automated feature decomposition to artiﬁcially split our data into multiple views, explicitly to enable successful co-training. 3.1 Self-training for Domain Adaptation

First, we assume we are given a loss function – in our case the log-loss from eq. (1) – which provides some estimate of conﬁdence in its predictions. In logistic regression, if y ˆ = sign(h(x)) is the prediction for an input x, the probability Ph (Y = y ˆ|X = x; w) is a natural metric of certainty (as h(x) can be interpreted as a probability for x to be of label +1), but other methods [23] can be used. Self-training [20] is a simple and intuitive iterative algorithm to leverage unlabeled data. During training one maintains a labeled training set L and an unlabeled test set U , initialized as l u L = DS ∪ DT and U = DT . Each iteration, a classiﬁer hw is trained to minimize the loss function over L and is evaluated on all elements of U . The c most conﬁdent predictions on U are moved to L for the next iteration, labeled by the prediction of sign(hw ). The algorithm terminates when U is empty or all predictions are below a pre-deﬁned conﬁdence threshold (and considered unreliable). Algorithm 1 summarizes self-training in pseudo-code with the use of feature selection, described in the following section. 2

Algorithm 1 SEDA pseudo-code.
1: 2: 3: 4: 5: 6: 7: Inputs: L and U . repeat w∗ = argminw (w; L) + γs(L, U, w) Apply hw∗ on all elements of U . Move up-to c conﬁdent inputs xi from U to L, labeled as sign(h(xi )). until No more predictions are conﬁdent Return hw∗

3.2

Feature Selection

So far, we have not addressed that the two data sets U and L are not sampled from the same distribution. In domain adaptation, the training data is no longer representative of the test data. More explicitly, PS (Y |X = x) is different from PT (Y |X = x). For illustration, consider the sentiment analysis problem in section 4, where data consists of unigram and bigram bag-of-words features and the task is to classify if a book-review (source domain) or dvd-review (target domain) is positive or negative. Here, the bigram feature “must read” is indicative of a positive opinion within the source (“books”) domain, but rarely appears in the target (“dvd”) domain. A classiﬁer, trained on the source-dominated set L, that relies too heavily on such features will not make enough highu . conﬁdence predictions on the set U = DT To address this issue, we extend the classiﬁer with a weighted 1 regularization for feature selection. The weights are assigned to encourage the classiﬁer to only use features that behave similarly in both L and U . Different from previous work on feature selection for domain adaptation [26], where the goal is to ﬁnd a new representation to minimize the difference between the distributions of the source and target domain, what we are proposing is to minimize the difference between the distributions of the labeled training set L and the unlabeled set U (which coincides with the testing set in our setting). This difference is crucial, as it makes the empirical distributions of L and U align gradually. For example, after some iterations, the classiﬁer can pick features that are never present in the source domain, but which have entered L through the rote-learning procedure. We perform the feature selection implicitly through w. For a feature α, let us denote the Pearson correlation coefﬁcient (PCC)1 between feature value xα and the label y for all pairs (x, y ) ∈ L as ρL (xα , y ). It can be shown that ρL (xα , y ) ∈ [−1, 1] with a value of +1 if a feature is perfectly aligned with the label (i.e. the feature is the label), 0 if it has no correlation, and −1 if it is of opposite polarity (i.e. the inverted label). Similarly, let us deﬁne the PCC for all pairs in U as ρU ;w (xα , Y ), where the unknown label Y is a random variable drawn from the conditional probability Ph (Y |X ; w). The two PCC values indicate how predictive a feature is of the (estimated) class label in the two respective data sets. Ideally, we would like to choose features that are similarly predictive across the two sets. We measure how similarly a feature behaves across L and U with the product ρL (xα , y )ρU ;w (xα , Y ). With this notation, we deﬁne the feature weight that reﬂects the cross-domain incompatibility of a feature as ∆L,U,w (α) = (1 − ρL (xα , y )ρU ;w (xα , Y )). (2) It is straight-forward to show that ∆L,U,w ∈ [0, 2]. Intuitively, ∆L,U,w expresses to what degree we would like to remove a feature. A perfect feature, that is the label itself (and the prediction in U ), results in a score of 0. A feature that is not correlated with the class label in at least one of the two domains (and therefore is too domain-speciﬁc) obtains a score of 1. A feature that switches polarization across domains (and therefore is “malicious”) has a score ∆L,U,w (α) > 1 (in the extreme case if it is the label in L and the inverted label in U , its score would be 2). We incorporate (2) into a weighted
1

regularization
d

s(L, U, w) =
α=1

∆L,U,w (α)|wα |.

(3)

Intuitively (3) encourages feature sparsity with a strong emphasis on features with little or opposite correlation across the domains, whereas good features that are consistently predictive in both
The PCC for two random variables X, Y is deﬁned as ρ = mean and σX the standard deviation of X .
1 E [(X −µX )(Y −µY )] , σX σY

where µX denotes the

3

domains become cheap. We refer to this version of the algorithm as Self-training for Domain Adaptation (SEDA). The optimization with feature selection, used in Algorithm 1, becomes w = argminw (L) + γs(L, U, w). (4) Here, γ ≥ 0 denotes the loss-regularization trade-off parameter. As we have very few labeled inputs from the target domain in the early iterations, stronger regularization is imposed so that only features shared across the two domains are used. When more and more inputs from the target domain are included in the training set, we gradually decrease the regularization to accommodate target speciﬁc features. The algorithm is very insensitive to the exact initial choice of γ . The guideline is to start with a relatively large number, and decrease it until the selected feature set is not empty. In our implementation, we set it to γ0 = 0.1, and we divide it by a factor of 1.1 during each iteration. 3.3 Co-training for Domain Adaptation

For rote-learning to be effective, we need to move test inputs from U to L that 1) are correctly classiﬁed (with high probability) and 2) have potential to improve the classiﬁer in future iterations. The former is addressed by the feature selecting regularization from the previous section – restricting the classiﬁer to a sub-set of features that are known to be cross-data set compatible reduces the generalization error on U . In this section we address the second requirement. We want to add inputs xi that contain additional features, which were not used to obtain the prediction hw (xi ) and would enrich the training set L. If the exact labels of the inputs in U were known, a good active learning [27] strategy would be to move inputs to L on which the current classiﬁer hw is most uncertain. In our setting, this would be clearly ill advised as the uncertain prediction is also used as the label. A natural solution to this dilemma is co-training [7]. Co-training assumes the data set is presented in two separate views and two classiﬁers are trained, one in each view. Each iteration, only inputs that are conﬁdent according to exactly one of the two classiﬁers are moved to the training set. This way, one classiﬁer provides the (estimated) labels to the inputs on which the other classiﬁer is uncertain. In our setting we do not have multiple views and which features are selected varies in each iteration. Hence, co-training does not apply out-of-the-box. We can, however, split our features into two mutually exclusive views such that co-training is effective. To this end we follow the pseudo-multiview regularization introduced by Chen et al. [10]. The main intuition is to train two classiﬁers on a single view X such that: (1) both perform well on the labeled data; (2) both are trained on strictly different features; (3) together they are likely to satisfy Balcan’s condition of -expandability [2], a necessary and sufﬁcient pre-condition for co-training to work2 . These three aspects can be formulated explicitly as three modiﬁcations of our optimization problem (4). We discuss each of them in detail in the following. Loss. Two classiﬁers are required for co-training, whose weight vectors we denote by u and v. The performance of each classiﬁer is measured by the log-loss (·; L) in eq. (1). To ensure that both classiﬁers perform well on the training set L, i.e. both have a small training loss, we train them jointly while minimizing the soft-maximum3 of the two losses, log e
(u;L)

+e

(v;L)

.

(5)

Feature Decomposition. Co-training requires the two classiﬁers to be trained on different feature spaces. We create those by splitting the feature-space into two mutually exclusive sub-sets. More precisely, for each feature α, at least one of the two classiﬁers must have a zero weight in the αth dimension. We can enforce this across all features with the equality constraint
d 2 u2 α vα = 0. α=1

(6)

-Expandability. In the original co-training formulation [7], it is assumed that the two views of the data are class conditionally independent. This assumption is very strong and can easily be
2 3

Provided that the classiﬁers are never conﬁdent and wrong — which can be violated in practice. The soft-max of a set of elements S is a differentiable approximation of max(S ) ≈ log( s∈S es ).

4

violated in practice [21]. Recent work [2] weakens this requirement signiﬁcantly to a condition of -expandability. Loosely phrased, for the two classiﬁers to be able to teach each other, they must make conﬁdent predictions on different subsets of the unlabeled set U . For the classiﬁer hu , let y ˆ = sign(u x) ∈ {±1} denote the class prediction and Ph (ˆ y |x; u) its conﬁdence. Deﬁne cu (x) as a conﬁdence indicator function (for some conﬁdence threshold τ > 0)4 cu (x) = 1 0 if p(ˆ y |x; u) > τ otherwise, (7)

and cv respectively. Then the -expanding condition translates to [cu (x)¯ cv (x) + c ¯u (x)cv (x)] ≥ min
x∈U x∈U

cu (x)cv (x),
x∈U

c ¯u (x)¯ cv (x) ,

(8)

for some > 0. Here, cu (x) = 1 − cu (x) indicates that classiﬁer hu is not conﬁdent about input x. Intuitively, the constraint in eq. (8) ensures that the total number of inputs in U that can be used for rote-learning because exactly one classiﬁer is conﬁdent (LHS), is larger than the set of inputs which cannot be used because both classiﬁers are already conﬁdent or both are not conﬁdent (RHS). In summary, the framework splits the feature space into two mutually exclusive sub-sets. This representation enables us to train two logistic regression classiﬁers, both with small loss on the labeled data set, while satisfying two constraints to ensure feature decomposition and -expandability. Our ﬁnal classiﬁer has the weight vector w = u + v. We refer to the resulting algorithm as CODA (Cotraining for Domain Adaptation), which can be stated concisely with the following optimization problem:
w,u,v

min

log e
d

(u;L)

+e

(v;L)

+ γs(L, U, w)

subject to:
2 (1) i=1 u2 i vi = 0 (2) x∈U [cu (x)¯ cv (x) + c ¯u (x)cv (x)] ≥ min (3) w = u + v

x∈U

cu (x)cv (x),

x∈U

c ¯u (x)¯ cv (x)

The optimization is non-convex. However, as it is not particularly sensitive to initialization, we set u, v randomly and optimize with standard conjugate gradient descent5 . Due to space constraints we do not include a pseudo-code implementation of CODA. The implementation is essentially identical to that of SEDA (Algorithm 1) where the above optimization problem is solved instead of eq. (4) in line 3. In line 5, we move inputs that one classiﬁer is conﬁdent about while the other one is uncertain to the training set L to improve the classiﬁer in future iterations.

4

Results

We evaluate our algorithm together with several other domain adaptation algorithms on the “Amazon reviews” benchmark data sets [6]. The data set contains reviews of four different types of products: books, DVDs, electronics, and kitchen appliances from Amazon.com. In the original dataset, each review is associated with a rating of 1-5 stars. For simplicity, we are only concerned about whether or not a review is positive (higher than 3 stars) or negative (3 stars or lower). That is, yi = {+1, −1}, where yi = 1 indicates that it is a positive review, and −1 otherwise. The data from four domains results in 12 directed adaptation tasks (e.g. books → dvds). Each domain adaptation task consists of 2, 000 labeled source inputs and around 4, 000 unlabeled target test inputs (varying slightly between tasks). We let the amount of labeled target data vary from 0 to 1600. For each setting with target labels we ran 10 experiments with different, randomly chosen, labeled instances. The original feature space of unigrams and bigrams is on average approximately 100, 000 dimensions across
4 In our implementation, the 0-1 indicator was replaced by a very steep differentiable sigmoid function, and τ was set to 0.8 across different experiments. 5 We use minimize.m (http://tinyurl.com/minimize-m).

5

different domains. To reduce the dimensionality, we only use features that appear at least 10 times in a particular domain adaptation task (with approximately 40, 000 features remaining). Further, we pre-process the data set with standard tf-idf [25] feature re-weighting.
1.05 1 0.95 Relative Test Error Relative Test Error 0.9 0.85 0.8 0.75 0.7 Logistic Regression Self−training SEDA CODA 0 50 100 200 400 800 Number of target labeled data 1600 1.15 1.1 1.05 1 0.95 0.9 0.85 0.8 0.75 Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0

50

100 200 400 800 Number of target labeled data

1600

Figure 1: Relative test-error reduction over logistic regression, averaged across all 12 domain adaptation tasks, as a function of the target training set size. Left: A comparison of the three algorithms from section 3. The graph shows clearly that self-training (Self-training vs. Logistic Regression), feature-selection (SEDA vs. Self-training) and co-training (CODA vs. SEDA), each improve the accuracy substantially. Right: A comparison of CODA with four state-of-the-art domain adaptation algorithms. CODA leads to particularly strong improvements under little target supervision. As a ﬁrst experiment, we compare the three algorithms from Section 3 and logistic regression as a baseline. The results are in the left plot of ﬁgure 1. For logistic regression, we ignore the difference between source and target distribution, and train a classiﬁer on the union of both labeled data sets. We use 2 regularization, and set the regularization constant with 5-fold cross-validation. In ﬁgure 1, all classiﬁcation errors are shown relative to this baseline. Our second baseline is self-training, which adds self-training to logistic regression – as described in section 3.1. We start with the set of labeled instances from source and target domain, and gradually add conﬁdent predictions to the training set from the unlabeled target domain (without regularization). SEDA adds feature selection to the self-training procedure, as described in section 3.2. We optimize over 100 iterations of selftraining, at which stage the regularization was effectively zero and the classiﬁer converged. For CODA we replace self-training with pseudo-multi-view co-training, as described in section 3.3. The left plot in ﬁgure 1 shows the relative classiﬁcation errors of these four algorithms averaged over all 12 domain adaptation tasks, under varying amounts of target labels. We observe two trends: First, there are clear gaps between logistic regression, self-training, SEDA, and CODA. From these three gaps one can conclude that self-training, feature-selection and co-training each lead to substantial improvements in classiﬁcation error. A second trend is that the relative improvement over logistic regression reduces as more labeled target data becomes available. This is not surprising, as with sufﬁcient target labels the task turns into a classical supervised learning problem and the source data becomes irrelevant. As a second experiment, we compare CODA against three state-of-the-art domain adaptation algorithms. We refer to these as Coupled, the coupled-subspaces approach [6], EasyAdapt [11], and EasyAdapt++. [16]. Details about the respective algorithms are provided in section 5. Coupled subspaces, as described in [6], does not utilize labeled target data and its result is depicted as a single point. The right plot in ﬁgure 1 compares these algorithms, relative to logistic regression. Figure 3 shows the individual results on all the 12 adaptation tasks with absolute classiﬁcation error rates. The error bars show the standard deviation across the 10 runs with different labeled instances. EasyAdapt and EasyAdapt++, both consistently improve over logistic regression once sufﬁcient target data is available. It is noteworthy that, on average, CODA outperforms the other algorithms in almost all settings when 800 labeled target points or less are present. With 1600 labeled target points all algorithms perform similar to the baseline and additional source data is irrelevant. All hyper-parameters of competing algorithms were carefully set by 5-fold cross validation. Concerning computational requirements, it is fair to say that CODA is signiﬁcantly slower than the other algorithms, as each iteration is of comparable complexity as logistic regression or EasyAdapt. 6

Ratio of used features (source/target)
Ratio of usedof features (source/target) Ratio used features

1.4 1.3

0 target labels
Ratio of used features

1.4 1.3 1.2 1.1 1 0.9

400 target labels
Ratio of used features
Source heavy

1.4 1.3 1.2

1600 target labels

Source heavy

r (w )

1.2 1.1 1 0.9
Target heavy

Source heavy

1.1 1 0.9
Target heavy

Target heavy

20

40 60 Iterations

80

100

20

40 60 Iterations

80

100

20

40 60 Iterations

80

100

Figure 2: The ratio of the average number of used features between source and target inputs (9), tracked throughout the CODA optimization. The three plots show the same statistic at different amounts of target labels. Initially, an input from the source domain has on average 10-35% more features that are used by the classiﬁer than a target input. At around iteration 40, this relation changes and the classiﬁer uses more target-typical features. The graph shows the geometric mean across all adaptation tasks. With no target data available (left plot), the early spike in source dominance is more pronounced and decreases when more target labels are available (middle and right plot). In typical domain adaptation settings this is generally not a problem, as training sets tend to be small. In our experiments, the average training time for CODA6 was about 20 minutes. Finally, we investigate the feature-selection process during CODA training. Let us deﬁne the indicator function δ (a) ∈ {0, 1} to be δ (a) = 0 if and only if a = 0, which operates element-wise on vectors. The vector δ (w) ∈ {0, 1}d indicates which features are used in the classiﬁer and δ (xi ) indicates which features are present in input xi . We can denote the ratio between the average number of used features in labeled training inputs over those in unlabeled target inputs as r(w) =
1 l | |DS 1 l | |DT
l xs ∈DS l xt ∈DT

δ (w) δ (xs ) δ (w) δ (xt ) . (9)

Figure 2 shows the plot of r(w) for all weight vectors during the 100 iterations of CODA, averaged across all 12 data sets. The three plots show the same statistic under varying amounts of target labels. Two trends can be observed: First, during CODA training, the classiﬁer initially selects more source-speciﬁc features. For example in the case with zero labeled target data, during early iterations the average source input contains 20 − 35% more used features relative to target inputs. This source-heavy feature distribution changes and eventually turns into target-heavy distribution as the classiﬁer adapts to the target domain. As a second trend, we observe that with more target labels (right plot), this spike in source features is much less pronounced whereas the ﬁnal target-heavy ratio is unchanged but starts earlier. This indicates that as the target labels increase, the classiﬁer makes less use of the source data and relies sooner and more directly on the target signal.

5

Related Work and Discussion

Domain adaptation algorithms that do not use labeled target domain data are sometimes called unsupervised adaptation algorithms. There are roughly three types of algorithms in this group. The ﬁrst type, which includes the coupled subspaces algorithm of Blitzer et al. [5] and the deep learning approach of Glorot et al. [13], learns a shared representation under which the source and target distributions are closer than under the ambient feature space [29]. The largest disadvantage of these algorithms is that they do not jointly optimize the predictor and the representation, which prevents them from focusing on those features which are both different and predictive. By jointly optimizing the feature selection, the multi-view split and the prediction, CODA allows us to do both. The second type of algorithm attempts to directly minimize the divergence between domains, typically by weighting individual instances [15, 17, 19]. These algorithms do not assume highly divergent domains (e.g. those with unique target features), but they have the advantage over both CODA and representation-learning of learning asymptotically optimal target predictors from only
6

We used a straight-forward MatlabT M implementation.

7

Dvd −> Books 0.35 0.3 Test Error 0.25 0.2 0.15 0.1 Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA 0.35 0.3 Test Error 0.25 0.2 0.15 0.1

Electronics −> Books Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA 0.35 0.3 Test Error 0.25 0.2 0.15 0.1

Kitchen −> Books Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0

50 100 200 400 800 1600 Number of target labeled data Books −> Dvd Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0

50 100 200 400 800 1600 Number of target labeled data Electronics −> Dvd Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0

50 100 200 400 800 1600 Number of target labeled data Kitchen −> Dvd Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0.35 0.3 Test Error 0.25 0.2 0.15 0.1

0.35

0.35 0.3 Test Error 0.25 0.2 0.15 0.1

0.3 Test Error

0.25

0.2

0

50 100 200 400 800 1600 Number of target labeled data Books −> Electronics Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA 0.35 0.3 Test Error 0.25 0.2 0.15 0.1

0

50 100 200 400 800 1600 Number of target labeled data Dvd −> Electronics Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0

50 100 200 400 800 1600 Number of target labeled data Kitchen −> Electronics Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0.35 0.3 Test Error 0.25 0.2 0.15 0.1

0.3 0.25 Test Error 0.2 0.15 0.1 0.05

0

50 100 200 400 800 1600 Number of target labeled data Books −> Kitchen Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0

50 100 200 400 800 1600 Number of target labeled data Dvd −> Kitchen Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0

50 100 200 400 800 1600 Number of target labeled data Electronics −> Kitchen Logistic Regression Coupled EasyAdapt EasyAdapt++ CODA

0.35 0.3 Test Error 0.25 0.2 0.15 0.1

0.25

0.18 0.16 Test Error 0.14 0.12 0.1

Test Error

0.2

0.15

0

50 100 200 400 800 1600 Number of target labeled data

0.1

0

50 100 200 400 800 1600 Number of target labeled data

0.08

0

50 100 200 400 800 1600 Number of target labeled data

Figure 3: The individual results on all domain adaptation tasks under varying amounts of labeled target data. The graphs show the absolute classiﬁcation error rates. All settings with existing labeled target data were averaged over 10 runs (with randomly selected labeled instances). The vertical bars indicate the standard deviation in these cases. source training data (when their assumptions hold). We did not explore them here because their assumptions are clearly violated for this data set. In natural language processing, a ﬁnal type of very successful algorithm self-trains on its own target predictions to automatically annotate new target domain features [20]. These methods are most closely related, in spirit, to our own CODA algorithm. Indeed, our self-training baseline is intended to mimic this style of algorithm. The ﬁnal set of domain adaptation algorithms, which we compared against but did not describe, are those which actively seek to minimize the labeling divergence between domains using multi-task techniques [1, 8, 9, 12, 22, 28]. Most prominently, Daum´ e [11] trains separate source and target models, but regularizes these models to be close to one another. The EasyAdapt++ variant of this algorithm, which we compared against, generalizes this to the semi-supervised setting by making the assumption that for unlabeled target instances, the tasks should be similar. Although these methods did not signiﬁcantly out-perform our baselines in the sentiment data set, we note that there do exist data sets on which such multi-task techniques are especially important [11], and we hope soon to explore combinations of CODA with multi-task learning on those data sets. 8

References
[1] R.K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6, 2005. [2] M.F. Balcan, A. Blum, and K. Yang. Co-training and expansion: Towards bridging theory and practice. NIPS, 17, 2004. [3] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and Jenn Wortman. A theory of learning from different domains. Machine Learning, 2009. [4] J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classiﬁcation. In Association for Computational Linguistics, 2007. [5] J. Blitzer, D. Foster, and S. Kakade. Domain adaptation with coupled subspaces. In Conference on Artiﬁcial Intelligence and Statistics, 2011. [6] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2006. [7] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory. ACM, 1998. [8] R. Caruana. Multitask learning. Machine Learning, 28, 1997. [9] O. Chapelle, P. Shivaswamy, S. Vadrevu, K.Q. Weinberger, Y. Zhang, and B. Tseng. Multi-task learning for boosting with application to web search ranking. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD, 2010. [10] M. Chen, K.Q. Weinberger, and Y. Chen. Automatic Feature Decomposition for Single View Co-training. In International Conference on Machine Learning, 2011. [11] H. Daume III. Frustratingly easy domain adaptation. In Association for Computational Linguistics, 2007. [12] T. Evgeniou, C.A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. Journal of Machine Learning Research, 6(1), 2006. [13] X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classiﬁcation: A deep learning approach. In Proceedings of the Twenty-eight International Conference on Machine Learning, ICML, 2011. [14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Verlag, New York, 2009. [15] J. Huang, A.J. Smola, A. Gretton, K. M. Borgwardt, and B. Scholkopf. Correcting sample selection bias by unlabeled data. In NIPS. MIT Press, Cambridge, MA, 2007. [16] H. Daume III, A. Kumar, and A. Saha. Co-regularization based semi-supervised domain adaptation. In NIPS. MIT Press, 2010. [17] J. Jiang and C.X. Zhai. Instance weighting for domain adaptation in nlp. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, June 2007. [18] Qian Liu, Aaron Mackey, David Roos, and Fernando Pereira. Evigan: a hidden variable model for integrating gene evidence for eukaryotic gene prediction. Bioinformatics, 2008. [19] T. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In NIPS. MIT Press, 2009. [20] D. McClosky, E. Charniak, and M. Johnson. Reranking and self-training for parser adaptation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2006. [21] K. Nigam and R. Ghani. Analyzing the effectiveness and applicability of co-training. In Proceedings of the ninth international conference on Information and knowledge management, 2000. [22] S. Parameswaran and K.Q. Weinberger. Large margin multi-task metric learning. In NIPS. 2010. [23] J.C. Platt et al. Probabilities for sv machines. NIPS, 1999. [24] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. Computer Vision–ECCV 2010, 2010. [25] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information processing & management, 24(5), 1988. [26] S. Satpal and S. Sarawagi. Domain adaptation of conditional probability models via feature subsetting. Knowledge Discovery in Databases: PKDD 2007, 2007. [27] B. Settles. Active learning literature survey. Machine Learning, 15(2), 1994. [28] K.Q. Weinberger, A. Dasgupta, J. Langford, A. Smola, and J. Attenberg. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009. [29] G. Xue, W. Dai, Q. Yang, and Y. Yu. Topic-bridged plsa for cross-domain text classication. In SIGIR, 2008.

9

