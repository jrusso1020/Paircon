Gradient Boosted Feature Selection
Zhixiang (Eddie) Xu ∗
Washington University in St. Louis One Brookings Dr. St. Louis, USA

Gao Huang
Tsinghua University 30 Shuangqing Rd. Beijing, China

Kilian Q. Weinberger
Washington University in St. Louis One Brookings Dr. St. Louis, USA

xuzx@cse.wustl.edu

huangg09@mails.tsinghua.edu.cn Alice X. Zheng
∗

kilian@wustl.edu

alicez@graphlab.com ABSTRACT
A feature selection algorithm should ideally satisfy four conditions: reliably extract relevant features; be able to identify non-linear feature interactions; scale linearly with the number of features and dimensions; allow the incorporation of known sparsity structure. In this work we propose a novel feature selection algorithm, Gradient Boosted Feature Selection (GBFS), which satisﬁes all four of these requirements. The algorithm is ﬂexible, scalable, and surprisingly straight-forward to implement as it is based on a modiﬁcation of Gradient Boosted Trees. We evaluate GBFS on several real world data sets and show that it matches or outperforms other state of the art feature selection algorithms. Yet it scales to larger data set sizes and naturally allows for domain-speciﬁc side information.

GraphLab 936 N. 34th St. Ste 208 Seattle, USA

1.

INTRODUCTION

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Miscellaneous; I.5.2 [Pattern Recognition]: Design Methodology—Feature evaluation and selection

General Terms
Learning

Keywords
Feature selection; Large-scale; Gradient boosting

∗Work done while at Microsoft Research
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. KDD’14, August 24–27, 2014, New York, NY, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2956-9/14/08 ...$15.00. http://dx.doi.org/10.1145/2623330.2623635 .

Feature selection (FS) [8] is an important problems in machine learning. In many applications, e.g., bio-informatics [21] or neuroscience [12], researchers hope to gain insight by analyzing how a classiﬁer can predict a label and what features it uses. Moreover, eﬀective feature selection leads to parsimonious classiﬁers that require less memory [25] and are faster to train and test [5]. It can also reduce feature extraction costs [29, 30] and lead to better generalization [9]. Linear feature selection algorithms such as LARS [7] are highly eﬀective at discovering linear dependencies between features and labels. However, they fail when features interact in nonlinear ways. Nonlinear feature selection algorithms, such as Random Forest [9] or recently introduced kernel methods [32, 23], can cope with nonlinear interactions. But their computational and memory complexity typically grow super-linearly with the training set size. As data sets grow in size, this is increasingly problematic. Balancing the twin goals of scalability and nonlinear feature selection is still an open problem. In this paper, we focus on the scenario where data sets contain a large number of samples. Speciﬁcally, we aim to perform eﬃcient feature selection when the number of data points is much larger than the number of features (n d). We start with the (NP-Hard) feature selection problem that also motivated LARS [7] and LASSO [26]. But instead of using a linear classiﬁer and approximating the feature selection cost with an l1 -norm, we follow [31] and use gradient boosted regression trees [7] for which greedy approximations exist [2]. The resulting algorithm is surprisingly simple yet very effective. We refer to it as Gradient Boosted Feature Selection (GBFS). Following the gradient boosting framework, trees are built with the greedy CART algorithm [2]. Features are selected sparsely following an important change in the impurity function: splitting on new features is penalized by a cost λ > 0, whereas re-use of previously selected features incurs no additional penalty. GBFS has several compelling properties. 1. As it learns an ensemble of regression trees, it can naturally discover nonlinear interactions between features. 2. In contrast to, e.g., FS with Random Forests, it uniﬁes feature selection and classiﬁcation into a single optimization. 3. In contrast

to existing nonlinear FS algorithms, its time and memory complexity scales as O(dn), where d denotes the number of features dimensionality and n the number of data points1 , and is very fast in practice. 4. GBFS can naturally incorporate pre-speciﬁed feature cost structures or side-information, e.g., select bags of features or focus on regions of interest, similar to generalized lasso in linear FS [19]. We evaluate this algorithm on several real-world data sets of varying diﬃculty and size, and we demonstrate that GBFS tends to match or outperform the accuracy and feature selection trade-oﬀ of Random Forest Feature Selection, the current state-of-the-art in nonlinear feature selection. We showcase the ability of GBFS to naturally incorporate side-information about inter-feature dependencies on a real world biological classiﬁcation task [1]. Here, features are grouped into nine pre-speciﬁed bags with biological meaning. GBFS can easily adapt to this setting and select entire feature bags. The resulting classiﬁer matches the best accuracy of competing methods (trained on many features) with only a single bag of features.

trices in capital bold (F) font. Speciﬁc entries in vectors or matrices are scalars and follow the corresponding convention. The data set consists of input vectors {x1 , . . . , xn } ∈ Rd with corresponding labels {y1 , . . . , yn } ∈ Y drawn from an unknown distribution. The labels can be binary, categorical (multi-class) or real-valued (regression). For the sake of clarity, we focus on binary classiﬁcation Y ∈ {−1, +1}, although the algorithm can be extended to multi-class and regression as well.

3.1

Feature selection with the l1 norm
min
w (xi ,yi )

Lasso [26] combines linear classiﬁcation and l1 regularization ( x i , y i , w ) + λ |w |1 . (1)

2.

RELATED WORK

In its original formulation, (·) is deﬁned to be the squared loss, (xi , yi , w) = (w xi − yi )2 . However, for the sake of feature selection, other loss functions are possible. In the binary classiﬁcation setting, where yi ∈ {−1, +1}, we use the better suited log-loss, (xi , yi , w) = log(1+exp(yi w xi )) [11].

One of the most widely used feature selection algorithms is Lasso [26]. It minimizes the squared loss with l1 regularization on the coeﬃcient vector, which encourages sparse solutions. Although scalable to very large data sets, Lasso models only linear correlations between features and labels and cannot discover non-linear feature dependencies. [17] propose the Minimum Redundancy Maximum Relevance (mRMR) algorithm, which selects a subset of the most responsive features that have high mutual information with labels. Their objective function also penalizes selecting redundant features. Though elegant, computing mutual information when the number of instance is large is intractable, and thus the algorithm does not scale. HSIC Lasso [32], on the other hand, introduces non-linearity by combining multiple kernel functions that each uses a single feature. The resulting convex optimization problem aligns this kernel with a “perfect” label kernel. The algorithm requires constructing kernel matrices for all features, thus its time and memory complexity scale quadratically with input data set size. Moreover, both algorithms separate feature selection and classiﬁcation, and require additional time and computation for training classiﬁers using the selected features. Several other works avoid expensive kernel computation while maintaining non-linearity. Grafting [18] combines l1 and l0 regularization with a non-linear classiﬁer based on a non-convex variant of the multi-layer perceptron. Feature Selection for Ranking using Boosted Trees [15] selects the top features with the highest relative importance scores. [27] and [9] use Random Forest. Finally, while not a feature selection method, [31] employ Gradient Boosted Trees to learn cascades of classiﬁers to reduce test-time cost by incorporating feature extraction budgets into the classiﬁer optimization.

3.2

The capped l1 norm

l1 regularization serves two purposes: It regularizes the classiﬁer against overﬁtting, and it induces sparsity for feature selection. Unfortunately, these two eﬀects of the l1 norm are inherently tied and there is no way to regulate the impact of either one. [33] introduce the capped l1 norm, deﬁned by the elementwise operation q (wi ) = min(|wi |, ). (2) Its advantage over the standard l1 norm is that once a feature is extracted, its use is not penalized further — i.e., it penalizes using many features does not reward small weights. This is a much better approximation of the l0 norm, which only penalizes feature use without interfering with the magnitude of the weights. When is small enough, i.e., ≤ mini |wi |, we can compute the exact number of features extracted with q (w)/ . In other words, penalizing q (w) is a close proxy for penalizing the number of extracted features. However, the capped l1 norm is not convex and therefore not easy to optimize. The capped l1 norm can be combined with a regular l1 (or l2 ) norm, where one can control the trade-oﬀ between feature extraction and regularization by adjusting the corresponding regularization parameters, µ, λ ≥ 0: min
w (xi ,yi )

(xi , yi , w) + λ|w|1 + µq (w).

(3)

Here q (w) denotes [q (w1 ), . . . , q (wd )].

4.

GRADIENT BOOSTED FEATURE SELECTION

3.

BACKGROUND

Throughout this paper we type vectors in bold (xi ), scalars in regular math type (k or C ), sets in cursive (S ) and ma1 In fact, if the storage of the input data is not counted, the memory complexity of GBFS scales as O(n).

The classiﬁer in Eq. (3) is better suited for feature selection than plain l1 regularization. However, it is still linear, which limits the ﬂexibility of the classifer. Standard approaches for incorporating non-linearity include the kernel learning [22] and boosting [3]. HSIC Lasso [32] uses kernel learning to discover non-linear feature interactions at a price of quadratic memory and time complexity. Our method uses boosting, which is much more scalable.

Boosting assumes that one can pre-process the data with limited-depth regression trees. Let H be the set of all possible regression trees. Taking into account limited precision and counting trees that obtain identical values on the entire training set as one and the same tree, one can assume |H| to be ﬁnite (albeit possibly large). Assuming that inputs are mapped into R|H| through φ(x) = [h1 (x), . . . , h|H| (x)] , we propose to learn a linear classiﬁer in this transformed space. Eq. (3) becomes min
β (φ(xi ),yi )

ized as t∗ = argmin ∇L(β )t .
t

(8)

In the remainder of this section we discuss approximate minimization strategies that does not require iterating over all possible trees. l1 -regularization. Since each step of the optimization increases a single dimension of β with a ﬁxed step-size α > 0, the l1 norm of β can be written in closed form as |β |1 = αT after T iterations. This means that penalizing the l1 norm of β is equivalent to early stopping after T iterations [7]. We therefore drop the λ|β |1 term and instead introduce T as an equivalent hyper-parameter.

(φ(xi ), yi , β ) + λ|β |1 + µq (β ).

(4)

Here, β is a sparse linear vector that selects trees. Although it is extremely high dimensional, the optimization in Eq. (4) is tractable because β is extremely sparse. Assuming, without loss of generalization, that the trees in H are sorted so that the ﬁrst T entries of β are non-zero, we obtain a ﬁnal classiﬁer
T

Gradient Decomposition.
To ﬁnd the steepest descent direction at iteration T +1, we decompose the (sub-)gradient into two parts, one for the loss function (), and one for the capped l1 norm penalty ∂ +µ ∂βt
d T

H (x ) =
t=1

βt ht (x).

(5)

Feature selection.
Eq. (4) has two penalty terms: plain l1 norm and capped l1 norm. The ﬁrst penalty term reduces overﬁtting while the second selects features. However, in its current form, the capped l1 norm selects trees rather than features. We therefore have to modify our setup to explicitly penalize the extraction of features. To model the total number of features extracted by an ensemble of trees, we deﬁne a binary matrix F ∈ {0, 1}d×T , where an entry Ff t = 1 if and only if the tree ht uses feature f . With this notation, we can express the total weight assigned to trees that extract feature f as
T

∇L(β )t =

∇q
f =1 t=1

Ff t β t .

(9)

(Hereafter we drop the absolute value around Ff t βt , since both Ff t and βt are non-negative.) The gradient of q ( t Ff t βt ) is not well-deﬁned at the cusp when t Ff t βt = . But we can take the right-hand limit, since βt never decreases,
T

∇q
t=1

Ff t βt =

Ff t , 0,

if if

t

Ff t β t < t Ff t β t ≥ .

(10)

|Ff t βt |.
t=1

(6)

We modify q (β ) to instead penalize the actual weight assigned to features. The ﬁnal optimization becomes
d T

If we set = α, where α > 0 is the step size, then t Ff t βt ≥ if and only if feature f has already been used in a tree from a previous iteration. Let φf = 1 indicate that feature f is still unused, and φf = 0 otherwise. With this notation we can combine the gradients from the two cases and replace T ∇q with φf Ff t . We obtain t=1 Ff t βt ∂ +µ ∂βt
d

min (β ) + λ|β |1 + µ
β f =1

q
t=1

|Ff t βt | .

(7) ∇L(β )t =

φf F f t .
f =1

(11)

As before, if is suﬃciently small ( ≤ minf | T t=1 Ff t βt |), we can set µ = 1/ and the feature selection penalty coincides exactly with the number of features used.

4.1

Optimization

Note that φf Ff t = 1 if and only if feature f is extracted for the ﬁrst time in tree t. In other words, the second term eﬀectively penalizes trees that use many new (previously not selected) features.

The optimization problem in Eq. (7) is non-convex and non-diﬀerentiable. Nevertheless, we can minimize it eﬀectively (up to a local ﬁxed point) with gradient boosting [7]. Let L(β ) denote the loss function to be minimized and ∇L(β )t the gradient w.r.t βt . Gradient boosting can be viewed as coordinate descent where we update the dimension with the steepest gradient at every step. We can assume that the set of all regression trees H is negation closed, i.e., for each h ∈ H, we also have −h ∈ H. This allows us to only follow negative gradients and always increase β . Thus there is always a non-negative optimal β . The search for the dimensions t∗ with the steepest negative gradient can be formal-

Greedy Tree construction.
With Eq. (11) we can compute the gradient with respect to any tree. But ﬁnding the optimal t∗ would still require searching all trees. In the remainder of this section, we transform the search for t∗ from a search over all possible dimensions t to a search for the best tree ht to minimize a pre-speciﬁed loss function. The new search can be approximated with the CART algorithm [2]. ∂ To this end, we apply the chain rule and decompose ∂β t into the derivative of the loss w.r.t. the current prediction evaluated at each input H (xi ) and the partial derivative

Algorithm 1 GBFS in pseudo-code. 1: Input: data {xi , yi }, learning rate , iterations T . 2: Initialize predictions H = 0 and selected feature set Ω = ∅. 3: for t = 1 to T do 4: Learn ht using greedy CART to minimize the impurity function in Eq. (14). 5: Update H = H + ht . 6: For each feature f used in ht , set φf = 0 and Ω = Ω ∪ f. 7: end for 8: Return H and Ω.

L1-LR feature selection Selected features: Ignored features: Test error :
30

GNFS feature selection (10 iterations) y x, z 54.05 %
30

Selected features: Ignored features: Test error :

x, y z 0%

20

20

y

10

y

10

0

0

−10

−10

−20 −10 0 10 20 30 40 50 60

−20 −10 0 10 20 30 40 50 60

x

x

∂H (xi ) , ∂βt n

∇L(β )t =
i=1

∂H (xi ) ∂ +µ ∂H (xi ) ∂βt

d

φf Ff t .
f =1

(12)

Figure 1: Feature selection and classiﬁcation performance on a simulated data set. GBFS clearly out-performs the l1 regularized logistic regression as it successfully captures the nonlinear relations between labels and features.

Note that H (xi ) = β h(xi ) is just a linear sum of all ht (xi ), (xi ) = ht (xi ). If the predictions over training data. Thus ∂H ∂βt ∂ we let gi denote the negative gradient gi = − ∂H , we can (xi ) reformulate Eq. (12) as
n d

ht = argmin
ht ∈H i=1

−gi ht (xi ) + µ
f =1

φf Ff t .

(13)

where feature extraction appears in stages. Extracting feature f makes feature g cheaper, but not free. One such application might be that of classifying medical images (e.g., MRI scans) where the features are raw pixels and feature groups are local regions of interest. In this case, φf (Ω) may reduce the “price” of pixels surrounding those in Ω to encourage feature selection with local focus.

Similar to [3], we restrict H to only normalized trees (i.e. 2 We can then add two constant terms i ht (xi ) = 1). 2 2 1 1 h ( x ) and i t i i gi to eq. (13), and complete the bi2 2 nomial equation. ht = argmin
ht ∈H

5.

RESULTS

1 2

n

gi − ht (xi )
i=1

2

d

+µ
f =1

φf Ff t .

(14)

This is now a penalized squared loss—an impurity function —and a good solutions can be found eﬃciently via the greedy CART algorithm for learning regression trees [7]. The ﬁrst term in Eq. (14) encourages feature splits to best match the negative gradient of the loss function, and the second term rewards splitting on features which have already been used in previous iterations. Algorithm 1 summarizes the overall algorithm in pseudo-code.

In this section, we evaluate GBFS against other state-ofthe-art feature selection methods on synthetic as well as realworld benchmark data sets. We also examine at its capacity for dealing with known sparsity patterns in a bioinformatics application. All experiments were conducted on a desktop with dual 6-core Intel i7 cpus with 2.66GHz, 96 GB RAM, and Linux version 2.6.32.x86 64.

5.1

Synthetic data

4.2

Structured Feature Selection

In many feature selection applications, one may have additional qualitative knowledge about acceptable sparsity patterns. Sometimes features can be grouped into bags and the goal is to select as few bags as possible. Prior work on handling structured sparsity include group lasso [10, 20] and generalized lasso [19]. Our framework can easily handle structured sparsity via the feature cost identity function φf . For example, we can deﬁne φf = 1 if and only if no feature from the same bag as f has been used in the past, and φf = 0 otherwise. The moment a feature from a particular bag is used in a tree, all other features in the same bag become “free” and the classiﬁer is encouraged to use features from this bag exclusively until it starts to see diminishing returns. In the most general setting, we can deﬁne φf : Ω → R+ 0 as a function that maps from the set of previously extracted features to a cost. For example, one could imagine settings

Figure 1 illustrates a synthetic binary classiﬁcation data set with three features. The data is not linearly separable in either two dimensions or three dimensions. However, a good nonlinear classiﬁer can easily separate the data using x and y . The z feature is simply a linear combination of x and y and thus redundant. We randomly select 90% of the instances for training and the rest for testing. Figure 1 (left panel) illustrates results from l1 -regularized logistic regression (L1-LR) [11, 16]. The regularization parameter is tuned on a hold-out set. Although L1-LR successfully detects and ignores the redundant feature z , it also assigns zero weight to x and only selects a single feature y . Consequently, it has poor classiﬁcation error rate on the test set (54.05%). In contrast, GBFS (Figure 1, right panel) not only identiﬁes the redundant feature z , but also detects that the labels are related to a nonlinear combination of x, y . It selects both x and y and successfully separates the data, achieving 0% classiﬁcation error.

5.2

Structured feature selection

In many applications there may be prior constraints on the sparsity patterns. Since GBFS can naturally incorporate pre-speciﬁed feature structures, we use it to perform

bag 1

2

3

4

5

6

7

8

9

L1-LR
# of features: 10 test error: 17.69%

Random Forest
# of features: 10 test error: 15.38%

HSIC Lasso
# of features: 13 test error: 21.85%

Group Lasso
# of features: 23 test error: 36.15%

GBFS
# of features: 4 test error: 15.38%

not only performs and generalizes poorly, but are also diﬃcult to interpret and justify. Figure 2 shows the selected features from one random split and classiﬁcation results averaged over 10 splits. Selected features are colored in green, and unselected ones are in blue. A bag is highlighted with a red/white box if at least one of its features is selected by the classiﬁer. We compare against l1 -regularized logistic regression (L1-LR) [11, 16], Random Forest feature selection (RF-FS) [9], HSIC Lasso [32] and Group Lasso [14]. As shown in Figure 2, because GBFS can incorporate the bag structures, it focusses on selecting features in one speciﬁc bag. Throughout training, it only selects features from bag 8 (highlighted with a red/white box). This conveniently reveals the association between diseases and gene clusters/bags. Similar to GBFS, Group Lasso with logistic regression can also deal with structured features. However, its l2 regularization has side eﬀects on feature weights, and thus results in much higher classiﬁcation error rate 36.15%. In contrast, l1 -regularized logistic regression, Random Forest and HSIC Lasso do not take bag information into consideration. They select scattered features from diﬀerent bags, making results diﬃcult to interpret. In terms of classiﬁcation accuracy, GBFS and Random Forest has the lowest test set error rate (15.38%), whereas l1 -regularized logistic regression (L1-LR) and HSIC Lasso achieve error rates of 17.69% and 21.85%, respectively. There are two reason why GBFS can be accurate with features from only a single bag. First, it is indeed the case that the genes in bag 8 are very predictive for the task of whether the tissue is malignant or benign (a result that may be of high biological value). Second, GBFS does not penalize further feature extraction inside bag 8 while other methods do; since bag 8 features are the most predictive, penalizing against them leads to a worse classiﬁer.

5.3

Benchmark data sets

Data sets.
Figure 2: Feature selection on structured feature data set. Selected features are colored in green, and unselected are in blue. The bag is highlighted with a red/white box if at least one of its features is selected by the classiﬁer. (Some bags may require zooming in to make the selected features visible.) structured feature selection on the Colon data set2 . In this dataset, 40 tumor and 22 normal colon tissues for 6500 human genes are measured using aﬀymetrix gene chips. [1] select 2000 genes that have the highest minimal intensity across the samples. [13] further analyze these genes and cluster them into 9 clusters/bags by their biological meaning. The task is to classify whether a tissue is normal or tumor. We random split the 62 tissues into 80/20 training and testing datasets, repeated over 10 random splits. We use the feature-bag cost function φf mentioned in section 4.2 to incorporate this side-information (setting the cost of all features in a bag to zero once the ﬁrst feature is extracted). Feature selection without considering these bag information
2 Available through the Princeton University gene expression project (http://microarray.princeton.edu/oncology/)

We evaluate GBFS on real-world benchmark data sets of varying sizes, domains and levels of diﬃculty. Table 1 lists data set statistics ordered by increasing numbers of training instances. We focus on data sets with a large number of training examples (n d). All tasks are binary classiﬁcation, though GBFS naturally extends to the regression setting, so long as the loss function is diﬀerentiable and continuous. Multi-class classiﬁcation problems can be reduced to binary ones, either by selecting the two classes that are most easily confused or (if those are not known) by grouping labels into two sets.

Baselines.
The ﬁrst baseline is l1 -regularized logistic regression (L1LR) [11, 16]. We vary the regularization parameter to select diﬀerent numbers of features and examine the test error rates under each setting. We also compare against Random Forest feature selection (RF-FS) [9], a non-linear classiﬁcation and feature selection algorithm. The learner builds many full decision trees by bagging training instances over random subsets of features. Features selection is done by ranking features based on their impurity improvement score, aggregated over all trees and all splits. Features with larger impurity improvements are

data set #training #testing #features

pcmac 1555 388 3289

uspst 1606 401 256

spam 3681 920 57

isolet 6238 1559 617

mnist3vs8 11982 1984 784

adult 32562 16282 123

kddcup99 4898431 311029 122

Table 1: Data sets statistics. Data sets are ordered by the number of training instances.

pcmac
20 20

uspst
20

spam

15 Error rates (in %) Error rates (in %)

15 Error rates (in %) 20 30 40 50 60 # of features 70 80 90 100

15

10

10

10

5

5

5

0 1 10

10 # of features

2

10

3

0 10

0 10

15

20 # of features

25

30

isolet
20 20

mnist 3vs8
20

adult

15 Error rates (in %) Error rates (in %)

15 Error rates (in %)

15

10

10

10

5

5

5

L1- LR (Lee et al., 2006) RF FS (Hastie et al., 2009) HSIC Lasso (Yamada et al., 2012) mRMR (Peng et al., 2005) GBFS
20 30 40 50 60 # of features 70 80 90 100

0 10

20

30

40

50 60 # of features

70

80

90

100

0 10

20

30

40

50 60 # of features

70

80

90

100

0 10

Figure 3: Classiﬁcation error rates (in %) vs. feature selection performance for diﬀerent algorithms on small to medium sized data sets. considered more important. For each data set, we train a Random Forest with 2000 trees and a maximum number of 20 elements per leaf node. After training all 2000 trees, we rank all features. Starting from top of the list, we re-train a random forest with only the top-k features and evaluate its accuracy on the test set. We increase the number of selected features until all features are included. The next state-of-the-art baseline is Minimum Redundancy Maximum Relevance (mRMR) [17], a non-linear feature selection algorithm that ranks features based on their mutual information with the labels. Again, we increase the selected feature set starting from the top of the list. At each stopping point, we train an RBF kernel SVM using only the features selected so far. The hyper-parameters are tuned on on 5 diﬀerent random 80/20 splits of the training data. The ﬁnal reported test error rates are based on the SVM trained on the full training set with the best hyper-parameter setting. Finally, we compare against HSIC Lasso [32], a convex extension to Greedy HSIC [23]. HSIC Lasso builds a kernel matrix for each feature and combines them to best match an ideal kernel generated from the labels. It encourages feature sparsity via an l1 penalty on the linear combination coeﬃcients. Similar to l1 -regularized logistic regression, we evaluate a wide range of l1 regularization parameters to sweep out the entire feature selection curve. Since HSIC Lasso is a two steps algorithm, we train a kernel SVM with the selected features to perform classiﬁcation. Similar to the mRMR experiment, we use cross-validation to select hyper-parameters and average over 5 runs. To evaluate GBFS, we perform 5 random 80/20 training/validation splits. We use the validation set to choose the depth of the regression trees and the number of iterations (maximum iterations is set to 2000). The learning rate is set to = 0.1 for all data sets. In order to show the whole error rates curve, we evaluate the algorithm at 10 values for the feature selection trade-oﬀ parameter µ in Eq. (7) (i.e., µ = {2−3 , 2−2 , 2−1 , 20 , 21 , 22 , 23 , 25 , 27 , 29 }).

Error rates.
Figure 3 shows the feature selection and classiﬁcation performance of diﬀerent algorithms on small and medium sized data sets. We select up to 100 features except for spam (d = 57) and pcmac (d = 3289). In general, l1 -regularized logistic regression (L1-LR), Random Forest (RF-FS) and GBFS easily scale to all data sets. RF-FS and GBFS both clearly out perform L1-LR in accuracy on all data sets due to their ability to capture nonlinear feature-label relationships. HSIC Lasso is very sensitive to the data size (both the number of training instance and the number of features), and only scales to two small data sets (uspst,spam ). mRMR is even more restrictive (more sensitive to the number of training instance) and thus only works for uspst. Both of them run out of memory on pcmac, which has the largest number of features. In terms of accuracy, GBFS clearly out-

kddcup99
10 L1−LR (Lee et al., 2006) RF FS (Hastie et al., 2009) −32 GBFS, µ = 2 GBFS, µ = 2 9 GBFS, µ = 2
−8

SMK-CAN-187
50 45 40 Error rates (in %) 35 30 25 20 L1−LR (Lee et al., 2006) RF FS (Hastie et al., 2009) HSIC Lasso (Yamada et al., 2012) GBFS

9.5

GBFS, µ = 2−2
2

Error rates (in %)

GBFS, µ = 28 8.5 GBFS, µ = 232

8

15
7.5

10 0 10

10

1

10 10 # of features

2

3

10

4

10

5

7 0

20

40 60 # of features

80

100

Figure 4: Feature selection and classiﬁcation error rates (in %) for diﬀerent algorithms on the large kddcup99 data set.
feature quality
20 L1−LR (Lee et al., 2006) RF FS (Hastie et al., 2009) HSIC Lasso (Yamada et al., 2012) mRMR (Peng et al., 2005) GBFS

Figure 6: Classiﬁcation error rates (in %) vs. feature selection performance for diﬀerent algorithms on a high dimensional (d n) data set.

15 Error rates (in %)

10

fore, we limit the number of trees to 100 and the maximum number of instances per leaf node to 500. Feature selection and classiﬁcation results are shown in Figure 4. For GBFS, instead of plotting the best performing results for each µ, we plot the whole feature selection iteration curve for multiple values of µ. GBFS obtains lower error rates than Random Forest (RF-FS) and l1 regularized logistic regression (L1LR) when few features are selected. (Note that due to the extremely large data set size, even improvements of < 1% are considered signiﬁcant.)

5

Quality.
To evaluate the quality of the selected features, we separate the contribution of the feature selection from the eﬀect of using diﬀerent classiﬁers. We apply all algorithms on the smallest data set (uspst ) to select a subset of the features and then train a SVM with RBF kernel on the respective feature subset. Figure 5 shows the error rates as a function of the number of selected features. GBFS obtains the lowest error rates in the (most interesting) regions of only few selected features. As more features are selected eventually all FS algorithms converge to similar values. It is worth noting that the linear classiﬁer (L1-LR) slightly outperforms most nonlinear methods when given enough features, which suggests that the uspst digits data requires a nonlinear classiﬁer for prediction but not for feature discovery. n scenario. While GBFS focusses on the scenario where the number of training data points is much larger than the number of features (n d), we also evaluate GBFS on a traditional feature selection benchmark data set SMK-CAN-187 [24], which is publicly available from [34]. This binary classiﬁcation data set contains 187 data points and 19, 993 features. We average results over 5 random 80/20 train-test splits. Figure 6 compares the results. GBFS out-performs l1 -regularized logistic regression (L1-LR), HSIC-Lasso and Random Forest feature selection (RF-FS), though by a small margin in some regions. d

0 10

20

30

40

50 60 70 # of features

80

90

100

110

Figure 5: Error rates (in %) of SVM-RBF trained on various feature subset obtained with diﬀerent features selection algorithms.

performs HSIC Lasso on spam but performs slightly worse on uspst. On all small and medium datasets, GBFS either outperforms RF-FS or matches its classiﬁcation performance. However, very diﬀerent from RF-FS, GBFS is a one step approach that selects features and learns a classiﬁer at the same time, whereas RF-FS requires re-training a classiﬁer after feature selection. This means that GBFS is much faster to train than RF-FS.

Large data set.
The last dataset in Table 1 (kddcup99 ) contains close to 5 million training instances. Training on such large data sets can be very time-consuming. We limit GBFS to T = 500 trees with the default hyper-parameters of tree depth = 4 and learning rate = 0.1. Training Random Forest with default hyper-parameters would take more than a week. There-

Computation time and complexity.
Not surprisingly, the linear method (L1-LR) is the fastest by far. Both mRMR and HSIC Lasso take signiﬁcantly more time than Random Forest and GBFS because they involve either mutual information or kernel matrix computation, which scales as O(d2 ) or O(n2 ). Random √ Forest builds full trees, requiring a time √ complexity of O( dn log n) per tree. The dependency on d is slightly misleading, as the number of trees required for Random Forests is √ also dependent on the number of features and scales as O( d) itself. In contrast, GBFS only builds limited depth (depth = 4, 5) trees, and the computation time complexity is O(dn). The number of iterations T is independent of the number of input features d; it is only a function of how the number of desired features. Empirically, we observe that the two algorithms are comparable in speed but GBFS is signiﬁcantly faster on data sets with many instances (large n). The training time ranged from several seconds to minutes on the small data sets to about one hour on the large data set kddcup99 (when Random Forest is trained with only 500 trees and large leaf sizes). Admittedly, the empirical comparison of training time is slightly problematic because our Random Forest implementation is based on highly optimized C++ code, whereas GBFS is implemented in MatlabTM . We expect that GBFS could be made signiﬁcantly faster if implemented in faster programming languages (e.g. C++) with the incorporation of known parallelization techniques for limited depth trees [28].

7.

ACKNOWLEDGMENTS

KQW was supported by NSF grants 1149882 and 1137211. KQW and ZEX were supported by NSF IIS-1149882 and IIS- 1137211. Part of this work was done while ZEX was an intern at Microsoft Research, Redmond.

8.

REFERENCES

6.

DISCUSSION

This paper introduces GBFS, a novel algorithm for nonlinear feature selection. The algorithm quickly train very accurate classiﬁers while selecting high quality features. In contrast to most prior work, GBFS is based on a variation of gradient boosting of limited depth trees [7]. This approach has several advantages. It scales naturally to large data sets and it combines learning a powerful classiﬁer and performing feature selection into a single step. It can easily incorporate known feature dependencies, a common setting in biomedical applications [1], medical imaging [6] and computer vision [4]. This has the potential to unlock interesting new discoveries in a variety of application domains. From a practitioners perspective, it is now worth investigating if a data set has inter-feature dependencies that could be provided as additional side-information to the algorithm. One bottleneck of GBFS is that it explores features using the CART algorithm, which has a complexity of O(dn). This may become a problem in cases with millions of features. Although this paper primarily focusses on the n d scenario, as future work we plan to consider improving the scalability with respect to d. One promising approach is to restrict the search to a random subsets of new features, akin to Random Forest. However, in contrast to Random Forest, the iterative nature of GBFS allows us to bias the sampling probability of a feature by its splitting value from previous iterations—thus avoiding unnecessary selection of unimportant features. We are excited by the promising results of GBFS and believe that the use of gradient boosted trees for feature selection will lead to many interesting follow-up results. This will hopefully spark new algorithmic developments and improved feature discovery across application domains.

[1] U. Alon, N. Barkai, D. A. Notterman, K. Gish, S. Ybarra, D. Mack, and A. J. Levine. Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays. Proceedings of the National Academy of Sciences, 96(12):6745–6750, 1999. [2] L. Breiman. Classiﬁcation and regression trees. Chapman & Hall/CRC, 1984. [3] O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Weinberger, Y. Zhang, and B. Tseng. Boosted multi-task learning. Machine learning, 85(1):149–173, 2011. [4] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005. [5] J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Eﬃcient projections onto the l1-ball for learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 272–279. ACM, 2008. [6] J. A. Etzel, V. Gazzola, and C. Keysers. An introduction to anatomical ROI-based fMRI classiﬁcation analysis. Brain Research, 1282:114–125, 2009. [7] J. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of Statistics, pages 1189–1232, 2001. [8] I. Guyon and A. Elisseeﬀ. An introduction to variable and feature selection. The Journal of Machine Learning Research, 3:1157–1182, 2003. [9] T. Hastie, R. Tibshirani, and J. H. Friedman. The elements of statistical learning. Springer, 2009. [10] J. Huang, T. Zhang, and D. Metaxas. Learning with structured sparsity. The Journal of Machine Learning Research, 12:3371–3412, 2011. [11] S. Lee, H. Lee, P. Abbeel, and A. Y. Ng. Eﬃcient l1 regularized logistic regression. In Proceedings of the National Conference on Artiﬁcial Intelligence, volume 21, page 401. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006. [12] Y. Liu, M. Sharma, C. Gaona, J. Breshears, J. Roland, Z. Freudenburg, E. Leuthardt, and K. Q. Weinberger. Decoding ipsilateral ﬁnger movements from ecog signals in humans. In Advances in Neural Information Processing Systems, pages 1468–1476, 2010. [13] S. Ma, X. Song, and J. Huang. Supervised group lasso with applications to microarray data analysis. BMC bioinformatics, 8(1):60, 2007. [14] L. Meier, S. Van De Geer, and P. B¨ uhlmann. The group lasso for logistic regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(1):53–71, 2008.

[15] F. Pan, T. Converse, D. Ahn, F. Salvetti, and G. Donato. Feature selection for ranking using boosted trees. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 2025–2028. ACM, 2009. [16] M. Y. Park and T. Hastie. L1-regularization path algorithm for generalized linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(4):659–677, 2007. [17] H. Peng, F. Long, and C. Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 27(8):1226–1238, 2005. [18] S. Perkins, K. Lacker, and J. Theiler. Grafting: Fast, incremental feature selection by gradient descent in function space. The Journal of Machine Learning Research, 3:1333–1356, 2003. [19] V. Roth. The generalized lasso. Neural Networks, IEEE Transactions on, 15(1):16–28, 2004. [20] V. Roth and B. Fischer. The group-lasso for generalized linear models: Uniqueness of solutions and eﬃcient algorithms. In Proceedings of the 25th international conference on Machine learning, pages 848–855. ACM, 2008. [21] Y. Saeys, I. Inza, and P. Larra˜ naga. A review of feature selection techniques in bioinformatics. bioinformatics, 23(19):2507–2517, 2007. [22] B. Sch¨ olkopf and A. Smola. Learning with kernels: Support vector machines, regularization, optimization, and beyond. MIT press, 2001. [23] L. Song, A. Smola, A. Gretton, J. Bedo, and K. Borgwardt. Feature selection via dependence maximization. The Journal of Machine Learning Research, 98888:1393–1434, 2012. [24] A. Spira, J. E. Beane, V. Shah, K. Steiling, G. Liu, F. Schembri, S. Gilman, Y.-M. Dumas, P. Calner, P. Sebastiani, et al. Airway epithelial gene expression in the diagnostic evaluation of smokers with suspect lung cancer. Nature medicine, 13(3):361–366, 2007. [25] S. Sra. Fast projections onto l1, q -norm balls for grouped feature selection. In Machine Learning and Knowledge Discovery in Databases, pages 305–317. Springer, 2011. [26] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288, 1996. [27] E. Tuv, A. Borisov, G. Runger, and K. Torkkola. Feature selection with ensembles, artiﬁcial variables, and redundancy elimination. The Journal of Machine Learning Research, 10:1341–1366, 2009. [28] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In WWW, pages 387–396. ACM, 2011. [29] Z. Xu, M. K., M. Chen, and K. Q. Weinberger. Cost-sensitive tree of classiﬁers. In S. Dasgupta and D. Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 133–141. JMLR Workshop and Conference Proceedings, 2013. [30] Z. Xu, M. Kusner, G. Huang, and K. Q. Weinberger. Anytime representation learning. In Proceedings of the

[31]

[32]

[33]

[34]

30th International Conference on Machine Learning (ICML-13), pages 1076–1084, 2013. Z. Xu, K. Weinberger, and O. Chapelle. The greedy miser: Learning under test-time budgets. In ICML, pages 1175–1182, 2012. M. Yamada, W. Jitkrittum, L. Sigal, E. P. Xing, and M. Sugiyama. High-dimensional feature selection by feature-wise non-linear lasso. arXiv preprint arXiv:1202.0515, 2012. T. Zhang. Multi-stage convex relaxation for learning with sparse regularization. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1929–1936. 2008. Z. Zhao, F. Morstatter, S. Sharma, S. Alelyani, A. Anand, and H. Liu. Advancing feature selection research. ASU Feature Selection Repository, 2010.

APPENDIX A. SUPPLEMENTARY RESULTS
We further extend our experimental results by incorporating more one-vs-one pairs from MNIST data set. We randomly pick 6 one-vs-one pairs from MNIST and evaluate GBFS and other feature selection algorithms. The ﬁrst baseline is l1 -regularized logistic regression (L1-LR). We vary the regularization parameter to select diﬀerent number of features and examine the error rates under these diﬀerent settings. We also compare against Random Forest feature selection [9]. Same to the procedure described in section 5, we run Random Forest with 2000 trees and a maximum number of 20 elements per leaf node. After training all 2000 trees, we rank all features. Starting from the most important feature, we re-train a random forest with only selected features and evaluate it on testing set. We gradually include less important features until we include all features. The other two baselines (include mRMR, HSIC-Lasso) do not scale on the MNIST data set. Figure 7 indicates that GBFS consistently matches Random Forest FS, and clearly out-performs l1 -regularized logistic regression.

MNIST 1vs7
5 L1-LR (Lee et al., 2006) RF FS (Hastie et al., 2009) GBFS Error rates (in %) 5

MNIST 6vs3

4 Error rates (in %)

4

3

3

2

2

1

1

0 10

20

30

40

50 60 # of features

70

80

90

100

0 10

20

30

40

50 60 # of features

70

80

90

100

MNIST 1vs8
5 5

MNIST 6vs4

4 Error rates (in %) Error rates (in %) 20 30 40 50 60 # of features 70 80 90 100

4

3

3

2

2

1

1

0 10

0 10

20

30

40

50 60 # of features

70

80

90

100

MNIST 1vs9
5 5

MNIST 6vs5

4 Error rates (in %) Error rates (in %) 20 30 40 50 60 # of features 70 80 90 100

4

3

3

2

2

1

1

0 10

0 10

20

30

40

50 60 # of features

70

80

90

100

Figure 7: Classiﬁcation error rates vs. feature selection performance for diﬀerent algorithms on 6 random chosen pairs of binary classiﬁcation tasks from MNIST data set.

