Maximum Variance Correction with Application to A∗ Search

Wenlin Chen Kilian Q. Weinberger Yixin Chen Washington University, One Brookings Dr., St. Louis, MO 63130 USA

wenlinchen@wustl.edu kilian@wustl.edu chen@cse.wustl.edu

Abstract
In this paper we introduce Maximum Variance Correction (MVC), which ﬁnds largescale feasible solutions to Maximum Variance Unfolding (MVU) by post-processing embeddings from any manifold learning algorithm. It increases the scale of MVU embeddings by several orders of magnitude and is naturally parallel. This unprecedented scalability opens up new avenues of applications for manifold learning, in particular the use of MVU embeddings as eﬀective heuristics to speed-up A∗ search. We demonstrate unmatched reductions in search time across several non-trivial A∗ benchmark search problems and bridge the gap between the manifold learning literature and one of its most promising high impact applications.

plication of great importance in many areas of real life. For example, GPS navigation systems need to ﬁnd the shortest path between two locations eﬃciently and repeatedly (e.g. each time a new traﬃc update has been received, or when the driver makes a wrong turn). As the processor capabilities of these devices and the patience of the users are both limited, the quality of the search heuristic is of great importance. This importance only increases as increasingly low powered embedded devices (e.g. smart-phones) are equipped with similar capabilities. Other applications include massive online multiplayer games, where agents need to identify the shortest path along a map which can change dynamically through actions by other users. For an embedding to be a A∗ heuristic, it must satisfy two properties: 1. admissible (distances are never overestimated ), 2. consistent (a triangular inequality like property is preserved). To be maximally eﬀective, a heuristic should have a minimal gap between its estimate and the true distance—i.e. all pair-wise distances should be maximized under the admissibility and consistency constraints. In the applications highlighted by Rayner et al. (2011), a heuristic must require small space to be broadcasted to the endusers. The authors show that the constraints of Maximum Variance Unfolding (MVU) (Weinberger & Saul, 2006)1 guarantee admissibility and consistency, while the objective maximizes distances and reduces space requirement of heuristics from O(n2 ) to O(dn). In other words, the MVU manifold learning algorithm is a perfect ﬁt to learn Euclidean heuristics for A∗ search. Unfortunately, it is fair to say that due to its semideﬁnite programming (SDP) formulation (Boyd & Vandenberghe, 2004), MVU is amongst the least scalable manifold learning algorithms and cannot embed state spaces beyond 4000 states—severally limiting the usefulness of the proposed heuristic in practice. Although there have been eﬀorts to increase the scala1 Throughout this paper we refer to MVU as the formulation with inequality constraints.

1. Introduction
Manifold learning has become a strong sub-ﬁeld of machine learning with many mature algorithms (Saul et al., 2006; Lee & Verleysen, 2007), often accompanied by large scale extensions (Platt, 2004; Silva & Tenenbaum, 2002; Weinberger et al., 2007) and thorough theoretical analysis (Donoho & Grimes, 2002; Paprotny et al., 2012). Until recently, this success story was not matched by comparably strong applications (Blitzer et al., 2005). Rayner et al. (2011) propose to use the Euclidean embedding of a search space graph as a heuristic for A∗ search (Russell & Norvig, 2003). The graph-distance between two states is approximated by the Euclidean distance between their respective embedded points. Exact A∗ search with informed heuristics is an apProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).

Maximum Variance Correction

bility of MVU (Weinberger et al., 2005; 2007), these lead to approximate solutions which no longer guarantee admissibility or consistency of heuristics. In this paper we propose a novel algorithm, Maximum Variance Correction (MVC), which improves the scalability of MVU by several orders of magnitude. In a nutshell, MVC post-processes embeddings from any manifold learning algorithm, to strictly satisfy the MVU constraints by rearranging embedded points within local patches. Hereby MVC combines the strict ﬁnite-size guarantees of MVU with the largescale capabilities of alternative algorithms. Further, it bridges the gap between the rich literature on manifold learning and what we consider its most promising and high-impact application to date—the use of Euclidean state-space embeddings as A∗ heuristics. Our contributions are summarized as follows: 1) We introduce MVC, a fully parallelizable algorithm that scales up and speeds up MVU by several orders of magnitudes. 2) We provide a formal proof that any solution of our relaxed problem formulation still satisﬁes all MVU constraints. 3) We demonstrate on several A∗ search benchmark problems that the resulting heuristics lead to impressive reductions in search-time—even beating the competitive diﬀerential heuristic (Ng & Zhang, 2002) by a large factor on all data sets.

2.1. Graph Embeddings We brieﬂy review MVU and Isomap as algorithms for proximity graph embedding. For a more detailed survey we recommend (Saul et al., 2006). Let G = (V, E ) denote the graph with undirected edges E and nodes V , with |V | = n. Edges (i, j ) ∈ E are weighted by some dij ≥ 0. Let δij denote the shortest path distance from node i to j . Manifold learning algorithms embed the nodes in V into a d-dimensional Euclidean space, x1 , . . . , xn ∈ Rd , such that xi − xj 2 ≈ δij . Maximum Variance Unfolding formulates this task as an optimization problem that maximizes the variance of the embedding, while enforcing strict constraints on the local edge distances:
n x1 ,...,xn ∈Rd

maximize

x2 i
i=1

subject to

||xi − xj ||2 ≤ dij
n

∀(i, j ) ∈ E

(1)

xi = 0
i=1

2. Background and related work
There have been several recent publications that increase the scalability of manifold learning algorithms. Vasiloglou et al. (2008); Weinberger et al. (2007); Weinberger & Saul (2006) directly scale up MVU by relaxing its constraints and restricting the solution to the space spanned by landmark points or the eigenvectors of the graph laplacian matrix. Silva & Tenenbaum (2002); Talwalkar et al. (2008) scale up Isomap (Tenenbaum et al., 2000b) with Nystr¨ om approximations. Our work is complementary as we reﬁne these embeddings to meet the MVU constraints while maximizing the variance of the embedding. Shaw & Jebara (2009) introduce structure preserving embedding, which learns embeddings that strictly preserve graph properties (such as nearest neighbors). Zhang et al. (2009) also focus on local patches of manifolds, however preserves discriminative ability rather than the ﬁnite-size guarantees of MVU. From a technical stand-point, our paper is probably most similar to Biswas & Ye (2004) which uses a semideﬁnite program for sensor network embedding. Due to the nature of their application, they deal with different constraints and objectives.

The last constraint centers the embedding at the origin, to remove translation as a degree of freedom in the optimization. Because the data is centered, the objective is identical to maximizing the variance, as 2 2 i x i = 0 .5 i,j xi − xj . Although (1) is nonconvex, Weinberger & Saul (2006) show that with a rank relaxation, x ∈ Rn , this problem can be rephrased as a convex semi-deﬁnite program by optimizing over the inner-product matrix K, with kij = xi xj : maximize
K

trace(K) ∀(i, j ) ∈ E (2)

subject to kii − 2kij + kjj ≤ d2 ij kij = 0
i,j

K

0.

The ﬁnal constraint K 0 ensures positive semideﬁniteness and guarantees that K can be decomposed into vectors x1 , . . . , xn with a straightforward eigenvector decomposition. To ensure strictly r−dimensional output, the ﬁnal embedding is projected into Rd with principal component analysis (PCA). (This is identical to composing the vectors xi out of the r leading eigenvectors of K.) The timecomplexity of MVU is O(n3 + c3 ) (where c is the number of constraints in the optimization problem), which makes it prohibitive for larger data sets. Graph Laplacian MVU (gl-MVU), Weinberger & Saul (2006); Wu et al. (2009), is an extension of MVU that reduces the size of K by matrix factorization,

Maximum Variance Correction

K = Q LQ. Here, Q are the bottom eigenvectors of the Graph Laplacian, also referred to as Laplacian Eigenmaps (Belkin & Niyogi, 2002). All local distance constraints are removed and instead added as a penalty term into the objective. The resulting algorithm scales to larger data sets but makes no exact guarantees about the distance preservations. Isomap, Tenenbaum et al. (2000a), preserves the global structure of the graph by directly preserving the graph distances between all pair-wise nodes:
x1 ,...,xn ∈Rd

ﬁnd a Euclidean Heuristic—however in its original formulation it can only scale to n ≈ 4000. In the following we will scale up MVU to much larger data sets.

3. Maximum Variance Correction
In this section, we introduce our MVC algorithm. Intuitively, MVC combines the scalability of gl-MVU and Isomap with the strong guarantees of MVU: It uses the former to obtain an initial embedding of the data and then post-processes it into a local optimum of the MVU optimization. The post-processing only involves re-optimizations of local patches, which is fast and can be decomposed into independent sub-problems. Initialization. We obtain an initial embedding ˆ1, . . . , x ˆ n of the graph with any (large-scale) manifold x learning algorithm (e.g. Isomap, gl-MVU or Eigenmaps). The resulting embedding is typically not a feasible solution to the exact MVU problem, because it violates many distance inequality constraints in (1). To make it feasible, we ﬁrst center it and then rescale the entire embedding such that all inequalities hold with at least one equality, xi = α(ˆ xi − 1 n
n

min

2 2 ((xi − xj )2 − δij ) . i,j

(3)

Tenenbaum et al. (2000a) show that (3) can be approximated as an eigenvector decomposition by applying multi-dimensional scaling (MDS) (Kruskal, 1964) on the shortest path distances δ (i, j ). The landmark extension (Silva & Tenenbaum, 2002) leads to signiﬁcant speed-ups with Nystr¨ om approximations of the graphdistance matrix. For simplicity, we refer to it also as “Isomap” throughout this paper. 2.2. Euclidean Heuristic The A∗ search algorithm ﬁnds the shortest path between two nodes in a graph. In the worst case, the complexity of the algorithm is exponential in the length of the shortest path, but the search time can be drastically reduced with a good heuristic, which estimates the graph distance between two nodes. Rayner et al. (2011) suggest to use the distance h(i, j ) = xi − xj 2 of the MVU graph embedding as such a heuristic, which they refer to as Euclidean Heuristic. A∗ with this heuristic provably converges to the exact solution, as the heuristic is admissible and consistent. More precisely, for all nodes i, j, k the following holds: Admissibility: Consistency: xi − xk xi − xj
2 2

ˆ i ), with α = min x
i=1

(i,j )∈E

dij . (6) ˆ ˆj xi − x

After the translation and rescaling in (6) we obtain a solution in the feasible set of MVU embeddings, and therefore also an admissible and consistent Euclidean Heuristic. In practice, this heuristic is of very limited use because it has a very large admissibility gap (4). In the following sections we explain how to transform the embedding to maximize the MVU objective, while remaining inside the MVU feasible region. 3.1. Local patching The (convex) MVU optimization is an SDP, which in their general formulation scale cubic in the input size n. To scale-up the optimization we therefore utilize a speciﬁc property of the MVU constraints: All constraints are strictly local as they only involve directly connected nodes. This allows us to divide up the graph embedding into local patches and re-optimize the MVU optimization on each patch individually. This approach has two clear advantages: the local patches can be made small enough to be re-optimized very quickly and the individual patch optimizations are inherently parallelizable—leading to even further speed-ups on modern multi-core computers. A challenge is to ensure that the local optimizations do not interfere with each other and remain globally feasible. Graph partitioning. There are several ways to di-

≤ δik ≤ δik + xk − xj
2

(4) (5)

The proof is straight-forward. As the shortest-path between nodes i and j in the embedding consists of edges which are all underestimated, it must be underestimated itself and so is xi − xj 2 (which implies admissibility). Consistency follows from the triangular inequality in combination with(4). The closer the gap in the admissibility inequality (4), the better is the search heuristic. The perfect heuristic would be the actual shortest path, h(i, j ) = δij (with which A∗ could ﬁnd the exact solution in linear time with respect to the length of the shortest path). The MVU objective maximizes all pairwise distances, and therefore minimizes exactly the gap in (4). Consequently, MVU is the perfect optimization problem to

Maximum Variance Correction

vide the graph G = (V, E ) into r mutually exclusive connected components. We use repeated breadth ﬁrst search (BFS) (Russell & Norvig, 2003) because of its simplicity, fast speed and guarantee that all partitions are connected components. Speciﬁcally, we pick a node i uniformly at random and apply BFS to identify the m closest nodes according to graph distance, that are not already assigned to patches. These nodes form a new patch Gp = (Vp , Ep ). The partitioning is continued until all nodes in V are assigned to exactly one partition, resulting in approximately r = n/m patches.2 The ﬁnal partitioning satisﬁes V = V1 ∪ · · · ∪ Vr and Vp ∩ Vq = {} for all p, q . We distinguish between two types of nodes within a partition Vp (illustrated in ﬁgure 1). A node i ∈ Vp is an inner point (blue circle) of Vp if all edges (i, j ) ∈ E connect it to other nodes j ∈ Vp ; i is an anchor point (red circle) of Vp if there exists an edge (i, j ) ∈ E to some j ∈ / Vp . Let Vpx denote the set of all inner nodes a and Vp the set of all anchor points in Vp . By deﬁnition, these sets are mutually exclusive and together contain all points, i.e. Vpx ∩ Vpa = {} and Vp = Vpx ∪ Vpa . Similarly, all edges in E can be divided into three mutual exclusive subsets (see ﬁgure 1): edges between inner points (E xx , blue); between anchor points (E aa , red); between anchor and inner points (E ax , purple). Optimization. We ﬁrst re-state the non-convex MVU optimization (1), slightly re-formulated to incorporate the graph partitioning. As each input is either an anchor point or an inner point of its respective patch, we can denote the set of all inner points as V x = p Vpx and the set of all anchor points as V a = p Vpa . If we re-order the summations and constraints by these sets, we can re-phrase the non-convex MVU optimization (1) as maximize
xi ,ak i∈ V
x

Gp

ak xi

xj

inner point anchor point

E xx E ax E aa

Figure 1. Drawing of a patch with inner and anchor points.

just as hard to solve. To reduce the computational complexity we make two changes: we remove the centering constraint and ﬁx the anchor points in place. The removal of the centering constraint is a harmless relaxation because the ﬁxed anchor points already remove translation as a degree of freedom and ﬁxate the solution very close to zero-mean. (The objective changes slightly, but in practice this has minimal impact on the solution.) The ﬁxing of the anchor points allows us to break down the optimization into r independent sub-problems. This can be seen from the fact that by deﬁnition all constraints in E xx never cross patch boundaries, and constraints in E ax only connect points within a patch with ﬁxed points. Constraints over edges in E aa can be dropped entirely, as edges between anchor points are necessarily ﬁxed also. We obtain r independent optimization problems of the following type: maximize x
xi ∈Vp i∈Vp

x2 i (8)

xx subject to ||xi − xj ||2 ≤ dij ∀(i, j ) ∈ Ep ax ||xi − ak ||2 ≤ dik ∀(i, k ) ∈ Ep .

x2 i +
k ∈V
a

a2 k
xx

The solutions of the r sub-problems (8) can be combined and centered, to form a feasible solution to (7). Convex patch re-optimization. Similar to the non-convex MVU formulation (1), optimization (8) is also non-convex and non-trivial to solve. However, with a change of variables and a slight relaxation we can transform it into a semi-deﬁnite program. Let np = |Vp |. Given a patch Gp , we deﬁne a matrix X = [x1 , . . . , xnp ] ∈ Rd×np , where each column corresponds to one embedded input of Vpx —the variables we want to optimize. Further, let us deﬁne the matrix K ∈ R(d+np )×(d+np ) as: K= I X X H where H = X X. (9)

subject to ||xi − xj ||2 ≤ dij ∀(i, j ) ∈ E ||ai − aj ||2 ≤ dij ∀(i, j ) ∈ E ai +
ai ∈V a

||xi − ak ||2 ≤ dik ∀(i, k ) ∈ E ax
aa

(7)

x i = 0.

x i ∈V x

For clarity, we denote all anchor points as ai ’s and inner points as xj ’s and with a slight abuse of notation write ai ∈ V a . Optimization by patches. The optimization (7) is identical to the non-convex MVU formulation (1) and
The exact number of patches and number of nodes per patch vary slightly, depending on the connectivity of the graph, but all |Vp | ≤ m.
2

The vector ei,j ∈ Rnp is all-zero except the ith element is 1 and the j th element is −1. The vector ei is all-zero except the ith element is −1. With this notation, we

Maximum Variance Correction

obtain (0; eij ) K(0; eij ) = xi − xj (ak ; ei ) K(ak ; ei ) = xi −
2 2 ak 2 2,

Algorithm 1 MVC (V,E) (10)
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: compute initial solution X with gl-MVU or Isomap center and rescale X according to (6) repeat identify r random sub-graphs (V1 , E1 ), . . . , (Vr , Er ) parfor p=1 to r do solve (12) for (Vp , Ep ) to obtain Xp end parfor concatenate all Xp into X and center. until variance of embedding X has converged. return X

where (0; eij ) ∈ R(d+np ) denotes the vector eij padded with zeros on top and (ak ; ei ) ∈ R(d+np ) the concatenation of ak and ei . Through (10), all constraints in (8) can be reformulated as a linear form of K (after squaring). The np objective reduces to trace(H) = i=1 x2 i . The resulting optimization problem becomes: max
X,H

that the relaxation from H = X X to H does not cause any constraint violations.

X X

trace(H) (0; eij ) (ak ; ei ) K(0; eij ) ≤ d2 ij K(ak ; ei ) ≤ d2 ik I X X H ∀(i, j ) ∈ ∀(i, k ) ∈
xx Ep ax Ep

s.t.

Lemma 1. The solution X of (12) satisﬁes all constraints in (8).
xx Proof. We ﬁrst focus on constraints on (i, j ) ∈ Ep . The ﬁrst constraint in (12) guarantees

H=X X K= .

Hii − 2Hij + Hjj ≤ d2 ij .

(13)

(11) The constraint H = X X ﬁxes the rank of H and is not convex. To mitigate, we relax it into H X X. In the following section we prove that this weaker constraint is suﬃcient to obtain MVU-feasible solutions. The Schur Complement Lemma (Boyd & Vandenberghe, 2004) states that H X X if and only if K 0, which we enforce as an additional constraint: max
X,H

The last constraint of (12) and the Schur Complement Lemma enforce that H − X X 0. Thus, eij (H − X X)eij ≥ 0 ⇔ ⇔ ⇔ x2 i eij (X X)eij ≤ eij Heij − 2xi xj + xi − xj 2 2 x2 j ≤ Hii − 2Hij + Hjj (15) ≤ Hii − 2Hij + Hjj . (14)

trace(H) (0; eij ) K(0; eij ) ≤ d2 ij (ak ; ei ) K(ak ; ei ) ≤ d2 ik K= I X X H 0.
xx ∀(i, j ) ∈ Ep ax ∀(i, k ) ∈ Ep

The ﬁrst result follows from the combination of (13) ax , the secand (15). Concerning constraints (i, j ) ∈ Ep ond constraint in (12) guarantees that
2 a2 k − 2ak xi + Hii ≤ dik .

s.t.

(16)

(12)

With a similar reasoning as for (14) we obtain ei (X X)ei ≤ ei Hei and therefore x2 i ≤ Hii . Combining this inequality with (16) leads to the result: ak − xi
2 2 2 ≤ a2 k − 2ak xi + Hii ≤ dik .

The optimization (12) is convex and scales O((np + d)3 ). It monotonically increases the objective in (7) and converges to a ﬁxed point. For a maximum patch-size m, i.e. np ≤ m for all p, each iteration of MVC scales linearly with respect to n, with complexity n (m + d)3 ). As the choice of m is independent of O( m n, it can be ﬁxed to a medium-sized value e.g. m ≈ 500 n for maximum eﬃciency. The r ≈ m sub-problems are completely independent and can be solved in parallel, leading to almost perfect parallel speed-up on computing clusters. Algorithm 1 states MVC in pseudo-code. 3.2. MVU feasibility We prove that the MVC algorithm returns a feasible MVU solution and consequently gives rise to a well deﬁned Euclidean Heuristic. First we need to show

Theorem 1. The embedding obtained with the MVC Algorithm 1 is in the feasible set of (1). Proof. We apply an inductive argument. The initial solution after centering and re-scaling according to (6) is MVU feasible by construction. By Lemma 1, the solution of (12) for each patch satisﬁes all constraints xx ax in Ep and Ep in (8). As each distance constraint in (7) is associated with exactly one patch, all its constraints in E xx and E ax are satisﬁed. Constraints in E aa are ﬁxed and satisﬁed by the induction hypothesis. Centering X satisﬁes the last constraint in (7) and leaves all distance constraints unaﬀected. As (7) is equivalent to (1), we obtain an MVU feasible solution at the end of each iteration in Algorithm 1, which concludes the proof.

Maximum Variance Correction
Isomap: var=5456
0.2

MVC: iter=1, var=8994
>0.2

MVC: iter=2, var=10274
>0.2

MVC: iter=4, var=11046

ξ

>0.2

>0.20
0.15 0.15 0.15 0.15

0.1

0.1

0.1

0.1

0.15

0.05

0.05

0.05

0.05

0

0

0

0

0.10

MVC: iter=8, var=11396

>0.2

MVC: iter=16, var=11422

>0.2

MVC: iter=46, var=11435

>0.2

MVU: var=11435

>0.2

0.15

0.15

0.15

0.15

0.05
0.1 0.1 0.1 0.1

0.05

0.05

0.05

0.05

0
0 0 0 0

Figure 2. Visualization of several MVC iterations on the 5-puzzle data set (m = 30). The edges are colored proportional to their relative admissibility gap ξ , as deﬁned in (17). The top left image shows the (rescaled) Isomap initialization. The successive graphs show that MVC decreases the edge admissibility gaps and increases the variance with each iteration (indicated in the title of each subplot) until it converges to the same variance as the MVU solution (bottom right).

4. Experimental Result
We evaluate our algorithm on a real world shortest path application data set and on two well-known benchmark AI problems. Game Maps is a real world map dataset with 3,155 states from the international success multi-player game Biowares Dragon Age: OriginsT M .3 A game map is a maze that consists of empty spaces (states) and obstacles. Cardinal moves take unit costs while diagonal moves cost 1.5. The search problem is to ﬁnd an optimal path between a given start and goal state, while avoiding all obstacles. Although not large-scale, this data set is a great example for an application where the search heuristic is of extraordinary importance. Speedy solvers are essential to reduce upkeep costs and to ensure a positive user experience. In the game, many player and non-player characters interact and search problems have to be solved frequently as agents move. The shortest path solutions cannot be cached as the map changes dynamically with player actions. M -Puzzle Problem (Jones, 2008) is a NP-hard sliding puzzle, often used as a benchmark problem for search algorithms/heuristics. It consists of a frame of M square tiles and one tile missing. All tiles are numbered and a state constitutes any order from which a path to the unique state with sorted (increasing) tiles exists. An action is to move a cardinal neighbor tile
3 http://en.wikipedia.org/wiki/Dragon_Age: _Origins

of the empty space into the empty space. The task is to ﬁnd a shortest action sequence from a pre-deﬁned start to a goal state. We evaluate our algorithm on the 5- (for visualization), 7- and 8-puzzle problem (3 × 2, 4 × 2 and 3 × 3 frames), which contain 360, 20160 and 181440 states respectively. Blocks World (Gupta & Nau, 1992) is a NP-hard problem with the goal to build several pre-deﬁned stacks out of a set of numbered blocks. Blocks can be placed on the top of others or on the ground. Any block that is currently under another block cannot be moved. The goal is to ﬁnd a minimum action sequence from a start state to a goal state. We evaluate our algorithm on block world problems with 6 blocks (4,051 states) and 7 blocks (37,633 states), respectively. Problem characteristics. The three types of problems not only feature diﬀerent sized state spaces but also have diﬀerent state space characteristics. Game maps has random obstacles that prevents movement for some state pairs, and thus has an irregular state space. The puzzle problems have a more regular search space (which lie on the surface of a sphere, see ﬁgure 2) with stable out-degree for each state. The state space of the blocksworld problems is also regular (it lies inside a sphere); however, the out-degree varies largely across states. For example, in 7-blocks, the state in which every block is placed on the ground has 42 edges, while the state in which all blocks are stacked in a single column has only 1 edge. We set dij = 1 for all edges in blocksworld and M -puzzle problems.

Maximum Variance Correction Table 1. Relative A search speedup over the diﬀerential heuristic (in expanded nodes) and embedding variance (×105 ). game map 6-blocksworld 7-puzzle 7-blocksworld 8-puzzle Method speedup var speedup var speedup var speedup var speedup var Diﬀ. Heuristic 1 N/A 1 N/A 1 N/A 1 N/A 1 N/A Eigenmap 0.32 0.88 0.66 0.058 0.81 3.52 0.61 0.50 0.76 13.47 Isomap 0.50 12.13 0.61 0.046 0.84 3.73 0.65 0.46 0.67 10.62 MVU 1.12 37.27 1.23 0.154 N/A N/A N/A N/A N/A N/A gl-MVU 0.41 7.54 1.18 0.138 1.14 6.66 1.05 1.20 0.88 17.79 MVC-10 (eigenmap) 0.88 31.31 1.49 0.22 1.41 9.59 1.33 1.88 1.47 43.48 MVC-10 (isomap) 1.09 36.96 1.56 0.22 1.43 9.62 1.25 1.71 1.45 43.08 0.90 32.98 1.96 0.27 1.45 9.82 1.67 2.27 1.52 45.75 MVC-10 (gl-mvu) MVC (eigenmap) 1.06 35.92 2.08 0.29 1.45 9.86 2.17 2.93 1.54 46.52 MVC (isomap) 1.12 37.22 2.22 0.30 1.47 9.85 2.22 2.95 1.54 46.58 1.11 36.47 2.27 0.30 1.45 9.86 2.22 2.95 1.61 49.06 MVC (gl-mvu)
∗

Experimental setting. Besides MVC, we evaluate four graph embedding algorithms: MVU (Weinberger & Saul, 2006), Isomap (Tenenbaum et al., 2000b), (Laplacian) Eigenmap (Belkin & Niyogi, 2002) and glMVU (Wu et al., 2009). The last three are used as initializations for MVC. Following Rayner et al. 2011, the embedding dimension is d = 3 for all experiments. For gl-MVU, we set the dimension of graph Laplacian to be 40. For datasets of size greater than 10K, we set 10K landmarks for Isomap. For M V C we use a patchsize of m = 500 throughout (for which problem (12) can be solved in less than 20s on our lab desktops). Visualization of MVC iterations (m = 30). Figure 2 visualizes successive iterations of the d = 3 dimensional MVC embedding of the 5−puzzle problem. All edges are colored proportionally to their relative admissibility gap, ξij = dij − ||xi − xj || . ||xi − xj || (17)

act distance from all states to a few pivot points in a (randomly chosen) set S ⊆ V . The graph distance between two states a, b is then approximated with maxs∈S |δ (a, s) − δ (b, s)| ≤ δ (a, b). In our experiments we set the number of pivots to 3 so that diﬀerential heuristics and embedding heuristics share the same memory limit. Figure 3 (right ) shows the total expanded nodes as a function of the solution length, averaged over 100 start/goal pairs for each solution length. The ﬁgure compares MVC with various initializations, the diﬀerential heuristic and the MVU algorithm on the 6-blocksworld puzzle. Speedups (reported in Table 1) measure the reduction in expanded states during search, relative to the diﬀerential heuristic, averaged over 100 random (start, goal) pairs across all solution lengths. Comprehensive evaluation. Table 1 shows the A∗ search performances of Euclidean heuristics obtained by the MVC initializations, MVC after only 10 iterations (MVC-10) and after convergence (bottom section). Table 1 also shows the MVU objective/variance, 2 x∈V xi , of each embedding. Several trends can be observed: 1. MVC performs best when initialized with gl-MVU—this is not surprising as gl-MVU has a similar objective and is likely to lead to better initializations; 2. all MVC embeddings lead to drastic speedups over the diﬀerential heuristics; 3. the variance is highly correlated with speedup—supporting Rayner et al. (2011) that the MVU objective is well suited to learn Euclidean heuristics; 4. even MVC after only 10 iterations already outperforms the diﬀerential heuristic on almost all data sets. The consistency of the speedups and their unusually high factors (up to 2.22) show great promise for MVC as an embedding algorithm for Euclidean heuristics. Exceeding MVU. On the 6-blocksworld data set in table 1, the variance of MVC actually exceeds that of MVU. In other words, the MVC algorithm ﬁnds a better solution for (1). This phenomenon can be explained by the fact that the convex MVU problem (2) is rank-relaxed and the ﬁnal embedding is ob-

The plot in the top left shows the original Isomap initialization after re-scaling, as deﬁned in (6). The plot in the bottom right shows the actual MVU embedding from (2)—which can be computed precisely because of the small problem size. Intermediate plots show the embeddings after several iterations of MVC. Two trends can be observed: 1. the admissibility gap decreases with MVC (all edges are blue in the ﬁnal embedding) and 2. the variance i x2 i of the embedding, i.e. the MVU objective, increases monotonically. The ﬁnal embedding has the same variance as the actual MVU embedding. The ﬁgure also shows that the 5−puzzle state space lies on a sphere, which is a beautiful example that visualization of states spaces in itself can be valuable. For example, the discovery of speciﬁc topological properties might lead to a better understanding of the state space structure and aid the development of problem speciﬁc heuristics. Setup. As a baseline heuristic, we compare all results with a diﬀerential heuristic (Ng & Zhang, 2002). The diﬀerential heuristic pre-computes the ex-

Maximum Variance Correction
4 x 10
4

2

x 10

4 diff MVU Eigenmap Isomap glïMVU MVC(eigenmap) MVC(isomap) MVC(glïmvu)

2

#Expanded Nodes

3 Variance

1.5

1

MVU Eigenmap Isomap glïMVU MVC(eigenmap) MVC(isomap) MVC(glïmvu)

1

0.5

0 0

20

Iterations

40

60

0 0

2

4 6 8 Optimal solution length

10

Figure 3. (Left ) the embedding variance of 6-blocksworld plotted over 30 MVC iterations. The variance increases monotonically and even outperforms the actual MVU embedding (Weinberger & Saul, 2006) after only a few iterations. (Right ) the number of expanded nodes in A∗ search as a function of the optimal solution length. All MVC solutions strictly outperform the Diﬀerential Heuristic (diﬀ ) and even expand fewer nodes than MVU. Table 2. Training time for MVU (Weinberger et al., 2005) and MVC, reported after initialization, the ﬁrst 10 iterations (MVC-10), and after convergence.
Method |V | MVU Eigenmap Isomap gl-MVU MVC-10 (eig) MVC-10 (iso) MVC-10 (glm) MVC (eig) MVC (iso) MVC (glm) game 3,155 3h 1s 14s 2m 20m 15m 21m 36m 17m 34m 6-block 4,051 10h 1s 57s 1m 2m 3m 3m 9m 8m 5m 7-puzz 20,160 N/A 4s 3m 1m 14m 20m 18m 72m 56m 26m 7-block 37,633 N/A 2m 4m 8m 9m 11m 14m 53m 51m 33m 8-puzz 181,440 N/A 7m 32m 15m 2h 3h 3h 6h 7h 7h

tained after projection into a d = 3 dimensional subspace. As MVC performs its optimization directly in this d-dimensional sub-space, it can ﬁnd a better rank-constrained solution. This eﬀect is further illustrated in Figure 3 (left ), which shows the monotonic increase of the embedding variance as a function of the MVC iterations (on 6-blocksworld). After only a few iterations, all three MVC runs exceeds the MVU solution. A similar eﬀect is utilized by Shaw & Jebara (2007), who optimize MVU in lower dimensional spaces directly (but cannot scale to large data sets). Figure 3 (left ) also illustrates the importance of initialization: MVC initialized with Isomap and gl-mvu converge to the same (possibly globally optimal) solution, whereas the run with Laplacian Eigenmaps initialization is stuck in a sub-optimal solution. Our ﬁndings are highly encouraging and show that we might not only approximate MVU eﬀectively on very large data sets, but actually outperform it if the intrinsic dimensionality of the data is higher than the desired embedding dimensionality d. Embedding time. Table 2 shows the training time required for the convex MVU algorithm (2), three MVC initializations (Eigenmap, Isomap, gl-MVU), the

time for 10 MVC iterations and the time until MVC converges, across all ﬁve data sets. Note that for real deployment, such as GPS systems, MVC only needs to be run once to obtain the embedding. The online calculations of Euclidean heuristics are very fast. All embeddings were computed on an oﬀ-the-shelve desktop with two 8-core Intel(R) Xeon(R) processors of 2.67 GHz and 128GB of RAM. Our MVC implementation is in MATLABT M and uses CSDP (Borchers, 1999) as the SDP solver. We parallelize each run of MVC on eight cores. All three initializations require roughly similar time (although Laplacian Eigenmaps is the fastest on all data sets), which is only a small part of the overall optimization. Whereas MVU requires 10 hours for graphs with |V | = 4051 (and cannot be executed on larger problems), we can ﬁnd a superior solution to the same problem in 5 minutes and are able to run MVC on 45× larger problems in only 7 hours.

5. Conclusion
We have presented MVC, an iterative algorithm to transform graph embeddings into MVU feasible solution. On several small-sized problems where MVU can ﬁnish, we show that MVC gives comparable or even better solutions than MVU. We apply MVU on data sets of unprecedented sizes (n = 180, 000) and, because of linear scalability, expect future (parallel) implementations to scale to graphs with millions of nodes. By satisfying all MVU constraints, MVC embeddings are provably well-deﬁned Euclidean heuristics for A∗ search and unleash an exciting new area of applications to all of manifold learning. We hope it will fuel new research directions in both ﬁelds. Acknowledgements. WC and YC are supported in part by NSF grants CNS-1017701 and CCF-1215302. KQW is supported by NIH grant U01 1U01NS07345701 and NSF grants 1149882 and 1137211. The authors thank Lawrence K. Saul for many helpful discussions.

Maximum Variance Correction

References
Belkin, M. and Niyogi, P. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15:1373–1396, 2002. Biswas, P. and Ye, Y. Semideﬁnite programming for ad hoc wireless sensor network localization. In Proceedings of the 3rd international symposium on Information processing in sensor networks, IPSN ’04, pp. 46–54, New York, NY, USA, 2004. ACM. Blitzer, J., Weinberger, K.Q., Saul, L. K., and Pereira, F. C. N. Hierarchical distributed representations for statistical language modeling. In Advances in Neural and Information Processing Systems, volume 17, Cambridge, MA, 2005. MIT Press. Borchers, B. Csdp, ac library for semideﬁnite programming. Optimization Methods and Software, 11(1-4):613– 623, 1999. Boyd, S. and Vandenberghe, L. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004. Donoho, D.L. and Grimes, C. When does isomap recover the natural parameterization of families of articulated images? Department of Statistics, Stanford University, 2002. Gupta, N. and Nau, D.S. On the complexity of blocksworld planning. Artiﬁcial Intelligence, 56(2):223–254, 1992. Jones, M.T. Artiﬁcial Intelligence: A Systems Approach: A Systems Approach. Jones & Bartlett Learning, 2008. Kruskal, J.B. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964. Lee, J.A. and Verleysen, M. Nonlinear dimensionality reduction. Springer, 2007. Ng, T.S.E. and Zhang, H. Predicting internet network distance with coordinates-based approaches. In INFOCOM 2002. Twenty-First Annual Joint Conference of the IEEE Computer and Communications Societies. Proceedings. IEEE, volume 1, pp. 170–179. IEEE, 2002. Paprotny, A., Garcke, J., and Fraunhofer, S. On a connection between maximum variance unfolding, shortest path problems and isomap. In Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statistics. MIT Press, 2012. Platt, J.C. Fast embedding of sparse music similarity graphs. Advances in Neural Information Processing Systems, 16:571578, 2004. Rayner, C., Bowling, M., and Sturtevant, N. Euclidean Heuristic Optimization. In Proceedings of the Twenty-Fifth National Conference on Artiﬁcial Intelligence (AAAI), pp. 81–86, 2011. Russell, S. J. and Norvig, Peter. Artiﬁcial Intelligence: A Modern Approach. Pearson Education, 2 edition, 2003.

Saul, L.K., Weinberger, K.Q., Ham, J. H., Sha, F., and Lee, D. D. Spectral methods for dimensionality reduction, chapter 16, pp. 293–30. MIT Press, 2006. Shaw, B. and Jebara, T. Minimum volume embedding. In Proceedings of the 2007 Conference on Artiﬁcial Intelligence and Statistics. MIT press, 2007. Shaw, B. and Jebara, T. Structure preserving embedding. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pp. 937–944, New York, NY, USA, 2009. ACM. Silva, V. and Tenenbaum, J.B. Global versus local methods in nonlinear dimensionality reduction. Advances in neural information processing systems, 15:705–712, 2002. Talwalkar, A., Kumar, S., and Rowley, H. Large-scale manifold learning. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pp. 1–8. IEEE, 2008. Tenenbaum, J. B., Silva, V., and Langford, J. C. A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science, 290(5500):2319–2323, 2000a. Tenenbaum, J.B., De Silva, V., and Langford, J.C. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000b. Vasiloglou, N., Gray, A.G., and Anderson, D.V. Scalable semideﬁnite manifold learning. In Machine Learning for Signal Processing, 2008. MLSP 2008. IEEE Workshop on, pp. 368–373. IEEE, 2008. Weinberger, K.Q. and Saul, L.K. Unsupervised learning of image manifolds by semideﬁnite programming. International Journal of Computer Vision, 70:77–90, 2006. ISSN 0920-5691. Weinberger, K.Q., Packer, B. D., and Saul, L. K. Nonlinear dimensionality reduction by semideﬁnite programming and kernel matrix factorization. In Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, Barbados, West Indies, 2005. Weinberger, K.Q., Sha, F., Zhu, Q., and Saul, L. Graph laplacian regularization for large-scale semideﬁnite programming. In Advances in Neural Information Processing Systems 19. MIT Press, Cambridge, MA, 2007. Wu, X., So, A. Man-Cho, Li, Z., and Li, S.R. Fast graph laplacian regularized kernel learning via semideﬁnite quadratic linear programming. In Advances in Neural Information Processing Systems 22, pp. 1964–1972. 2009. Zhang, T., Tao, D., Li, X., and Yang, J. Patch alignment for dimensionality reduction. Knowledge and Data Engineering, IEEE Transactions on, 21(9):1299–1313, 2009.

