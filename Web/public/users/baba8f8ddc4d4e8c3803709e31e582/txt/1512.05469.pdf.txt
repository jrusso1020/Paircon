Private Causal Inference

Matt J. Kusner Yu Sun Washington University in St. Louis Cornell University mkusner@wustl.edu ys646@cornell.edu

Karthik Sridharan Kilian Q. Weinberger Cornell University Cornell University sridharan@cs.cornell.edu kqw4@cornell.edu

arXiv:1512.05469v2 [stat.ML] 20 Aug 2016

Abstract
Causal inference deals with identifying which random variables “cause” or control other random variables. Recent advances on the topic of causal inference based on tools from statistical estimation and machine learning have resulted in practical algorithms for causal inference. Causal inference has the potential to have signiﬁcant impact on medical research, prevention and control of diseases, and identifying factors that impact economic changes to name just a few. However, these promising applications for causal inference are often ones that involve sensitive or personal data of users that need to be kept private (e.g., medical records, personal ﬁnances, etc). Therefore, there is a need for the development of causal inference methods that preserve data privacy. We study the problem of inferring causality using the current, popular causal inference framework, the additive noise model (ANM) while simultaneously ensuring privacy of the users. Our framework provides diﬀerential privacy guarantees for a variety of ANM variants. We run extensive experiments, and demonstrate that our techniques are practical and easy to implement.

many cases, and the ambiguity of conditional independence testing [32, 25]. In the absence of interventions, it attempts to discover the underlying causal relationships of a set of random variables entirely based on samples from their joint distribution. The ﬁeld of causal inference is now a mature research area, covering learning topics as diverse as supervised batch inference [19, 23, 26], time-series causal prediction [10], and linear dynamical systems [30]. Many inference methods require only a regression technique and a way to compute the independence between two distributions given samples [13, 16]. One would hope that researchers could publicly release their causal inference ﬁndings to inform individuals and policy makers. One of the primary roadblocks to doing so is that often causal inference is performed on data that individuals may wish to keep private, such as data in the ﬁelds of medical diagnosis, fraud detection, and risk analysis. Currently, no causal inference method has formal guarantees about the privacy of individual data, which may be able to be inferred via attacks such as reconstruction attacks [3]. Arguably one of the best notion of privacy is diﬀerential privacy, introduced by Dwork et al. [6] and since used throughout machine learning [4, 15, 20, 2, 5]. Differential privacy guarantees that the outcome of an algorithm only reveals aggregate information about the entire dataset and never about the individual. An individual who is considering to participate in a study can be reassured that his/her personal information cannot be recovered with extremely high probability. To our knowledge, this paper is the ﬁrst to investigate private causal inference. We show that it is possible to privately release the quantities produced by the highlysuccessful additive noise model (ANM) framework by adding small amounts of noise, as dictated by diﬀerential privacy. Furthermore, these private quantities, with high probability, do not change the causal inference result, so long as it is conﬁdent enough. We demonstrate on a set of real-world causal inference datasets how our privacy-preserving methods can be readily and usefully applied.

1

Introduction

Causal identiﬁcation allows one to reason about how manipulations of certain random variables (the causes) aﬀect the outcomes of others (the eﬀects). Uncovering these causal structures has implications ranging from creating government policies to informing health-care practices. Causal inference was motivated by the impossibility of randomized intervention experiments in
Appearing in Proceedings of the 19th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR: W&CP volume 51. Copyright 2016 by the authors.

Private Causal Inference

2

Related Work

ANM framework assumption is deﬁned as follows. Deﬁnition 1. Two random variables X, Y with joint density p(x, y ) are said to ‘satisfy an ANM’ X → Y if there exists a non-linear function f : R → R and a random noise variable NY , independent from X , i.e. X⊥ ⊥ NY , such that Y = f (X ) + NY . As deﬁned, an ANM X → Y implies a functional relationship mapping X to Y , alongside independent noise. In order for this model to be useful for causal inference we would like the induced joint distribution PX,Y for this ANM to be somehow identiﬁably diﬀerent from the one induced by the ANM Y → X . If so, we say that the causal direction is identiﬁable [23]. If not, we have no hope of recovering the causal direction purely from samples under the ANM. Hoyer et al. [13] showed that ANMs are generically identiﬁable from i.i.d. samples from PX,Y (except for a few special cases of non-linear functions f and noise distributions). The intuition behind this is for the X → Y ANM, consider for most non-linear f and (for simplicity) 0-mean NY , the density p(y |x) has mean f (x) with distribution given by NY . This implies that p(y − f (x)|x) has distribution NY that is independent of X . However, p(x − f −1 (y )|y ) is for many choices of f and NY not independent of y . Algorithm 1 ANM Causal Inference [23] m 1: Input: train/test data {xi , yi }n i=1 , {xi , yi }i=1 ˆ 2: Regress on training data, to yield f , g ˆ, such that: ˆ(xi ) ≈ yi , g 3: f ˆ(yi ) ≈ xi , ∀i 4: Compute residuals on test data: ˆ(x ), r := x − g ˆ(y ) 5: rY := y − f X 6: Calculate dependence scores: 7: sX→Y := s(x , rY ), sY →X := s(y , rX ) 8: Return: sX→Y , sX→Y , and D , where X →Y if sX→Y < sY →X 9: D = Y → X if sX→Y > sY →X

Discovering the causal nature between random events has captivated researchers and philosophers long before the formal developments of statistics. This interest was formalized by Reichenbach & Reichenbach [28] who argued that all statistical correlations in data arise from underlying causal structures between the concerned random variables. For example, the correlation between smoking and lung cancer was found to arise from a direct causal link [7]. One of the most popular causal inference alternatives to conditional independence testing is the Additive Noise Model (ANM) approach developed by Hoyer et al. [13] and used in many recent works [35, 21, 18, 1]. ANMs, originally designed for inferring whether X → Y or Y → X and later extended to large numbers of random variables, work under the assumption that the eﬀect is a non-linear function of the cause plus independent noise. ANMs are one of many proposed causal inference methods in recent literature [16, 9, 19, 29] Work by Spirtes et al. [32], Pearl [25] shows how to determine if X → Y when these variables are a part of a larger ‘causal network’, via conditional independence testing. One downside to conditional independence based approaches is that inherently they cannot distinguish between Markov-equivalent graphs. Thus it may be possible that a certain set of conditional independences imply both X → Y and Y → X . Furthermore, if X and Y are the only variables in the causal network there is no conditional independence test to determine whether X → Y or Y → X .

3

Background

Our aim is to protect the privacy of individuals who submit personal information about two random variables of interest X and Y . Their information should remain private when it is used to infer whether X causes Y (X → Y ), or Y causes X (Y → X ) using the ANM framework. This personal information comes in the form of i.i.d. samples {(xi , yi )}n i=1 from the joint distribution PX,Y . We will assume that, 1. There is no confounding variable Z that commonly causes or is a common eﬀect of X and Y . 2. X and Y do not simultaneously cause each other. 3.1 Additive Noise Model

3.2

Inferring Causality

Deciding on the causal direction between two variables X and Y from a ﬁnite sample set has motivated an array of research [8, 17, 33, 13, 35, 22, 16, 18, 19]. Perhaps one of the most popular results is the Additive Noise Model (ANM) proposed by Hoyer et al. [13]. The

Mooij et al. [23] give a practical algorithm for determining the causal relationship between X and Y (i.e., either X → Y or Y → X ), as shown in Algorithm 1. The ﬁrst step is to partition the i.i.d. samples into a training and a testing set. We use the trainˆ: X→Y ing set to train the regression functions f and g ˆ : Y → X . We use the testing set to compute ˆ(x ) and r := x − g the residuals rY = y − f ˆ(y ). X If we have an ANM X → Y then the residual rY is an estimate of the noise NY which is assumed to be

Matt J. Kusner, Yu Sun, Karthik Sridharan, Kilian Q. Weinberger

independent of X . Therefore, we calculate the dependence between the residual rY and the input x , sX→Y := s(x , rY ), and sY →X := s(y , rX ), using a dependence score s(·, ·). If sX→Y is less than sY →X , then we declare X → Y , otherwise Y → X . 3.3 Dependence Scores

which makes the variance score ill suited to preserve diﬀerential privacy. IQR Score. We introduce a robust version of this score by replacing the variance of the random variables with their interquartile range (IQR). The IQR is the diﬀerence between the third and ﬁrst quartiles of the distribution and can be estimated empirically. We deﬁned the following IQR-based score: s(a, b) := log IQR(a) + log IQR(b). 3.4 Diﬀerential Privacy (2)

Crucially, the ANM approach hinges on the choice of dependence score s(·, ·). There have been many proposals, and we give a quick review of the most popular methods (for a detailed review see Mooij et al. [23]). Spearman’s ρ is a rank correlation coeﬃcient that describes the extent to which one random variable is a monotonic function of the other. Speciﬁcally, imagine independently sorting the observations {a1 , . . . , am } and {b1 , . . . , bm } by value in increasing order. Let oa i be the rank of ai in the a-ordering, and similarly, ob i for bi in the b-ordering. Then Spearman’s ρ is, s(a, b) := 1 − 6 i=1 d2 i m(m2 − 1)
m

We assume that the data set D = {(xi , yi )} contains sensitive data that should not be inferred from the release of the dependence scores. One of the most widely accepted mechanisms for private data release is diﬀerential privacy [6]. In a nutshell it ensures that the released scores can only be used to infer aggregate information about the data set and never about an individual datum (xi , yi ). Let us deﬁne the Hamming distance between two data ˜ ) between two data sets D and D ˜ as the sets dH (D, D number of elements in which these two sets diﬀer. If a ˜ , a distance dH (D, D ˜) ≤ 1 data set D is changed to D implies that at most one element was added, removed, or substituted. Deﬁnition 2. A randomized algorithm A is ( , δ )diﬀerentially private for , δ ≥ 0 if for all O ∈ ˜ with Range(A) and for all neighboring datasets D, D ˜ ) ≤ 1 we have that dH (D, D ˜ ) = O + δ. Pr A(D) = O ≤ e Pr A(D (3)

b where di := (oa i − oi ) are the rank diﬀerences for a, b.

Kendall’s τ . Similar to Spearman’s ρ, the Kendall τ rank score calls a pair of indices (i, j ) concordant if it is the case that ai > aj and bi > bj . Otherwise (i, j ) is called discordant. Then the dependence score is deﬁned as s(a, b) :=
1 2 m(m

|C − D | − 1)

where C is the number of concordant pairs and D is the number of discordant pairs. HSIC Score. The ﬁrst proposed score for the ANM causal inference is based on the Hilbert-Schmidt Independence Criterion (HSIC) [11], which was used by Hoyer et al. [13]. They compute an estimate of the p-value of the HSIC under the null hypothesis of independence, selecting the causal direction having the lower p-value. Alternatively, one can use an estimator to the HSIC value itself: s(a, b) := HSICkθ(a) ,kθ(b) (a, b) (1)

One of the most popular methods for making an algorithm ( , 0)-diﬀerentially private is the Laplace mechanism [6]. For this mechanism we must deﬁne an intermediate quantity called the global sensitivity, ∆A describing how much A changes when D changes, ∆A :=
˜ ˜ )≤1 D ,D⊆X s.t. dH (D ,D

max

˜ )|. |A(D) − A(D

where kθ is a kernel with parameters θ. Mooij et al. [23] show that under certain assumptions the algorithm in section 1 with the HSIC dependence score is consistent for estimating the causal direction in an ANM. Variance Score. When the noise variables in the ANM are Gaussian, the variance score was proposed in B¨ uhlmann et al. [1], and deﬁned as s(a, b) := log V(a) + log V(b). Changes to a single input value can induce arbitrarily large changes to this score,

The Laplace mechanism hides the output of A with a small amount of additive random noise, large enough to hide the impact of any single datum (xi , yi ). Deﬁnition 3. Given a dataset D and an algorithm A, the Laplace mechanism returns A(D)+ω , where ω is a noise variable drawn from Lap(0, ∆A / ), the Laplace distribution with scale parameter ∆A / . It may be that the global sensitivity of an algorithm A is unbounded in general, but can be bounded in the ˜. context of a speciﬁc data set D over all neighbors D For such datasets we can bound the local sensitivity ∆(D)A :=
˜ ˜ )≤1 D⊆X s.t. dH (D ,D

max

˜ )|. |A(D) − A(D

Private Causal Inference

Table 1: Dependence scores and their privacy. A checkmark indicates that there exist meaningful bounds on either the global or local sensitivity.
Test Global Local Sense. Sense. Training Global Local Sense. Sense. -

Theorem 1. The rank correlation coeﬃcients have the following global sensitivities, 1. Let ρ(·, ·) be Spearman’s ρ score, then |ρ(x , rY ) − ρ(˜ x ,˜ rY )| ≤ 30 m

Score Spearman’s ρ Kendall’s τ HSIC IQR

2. Let τ (·, ·) be Kendall’s τ score, then |τ (x , rY ) − τ (˜ x ,˜ rY )| ≤ 4 m

-

If an algorithm has bounded global sensitivity it certainly has bounded local sensitivity. Nissim et al. [24], Dwork & Lei [4], Jain & Thakurta [14] show how to use the local sensitivity to cleverly produce private quantities for datasets with bounded local sensitivity.

4

Test Set Privacy

The data is partitioned into training and test set, which are used in diﬀerent ways. We therefore introduce mechanisms to preserve training and test set privacy respectively, which can be used jointly. Specifically, we show how to privatize the dependence scores sX →Y , sY →X . The reason for this is four-fold: 1. Privatizing the dependence score immediately privatizes the causal direction D, because operations on diﬀerentially private outputs preserve privacy (so long as they do not touch the data). 2. Releasing the scores indicates how conﬁdent the ANM method is about the causal direction, which is absent from the binary output D. 3. It is unclear which dependence score is best for a particular dataset, so we privatize multiple scores and leave this choice to the practitioner. In this section we begin with test set privacy and describe training set privacy in Section 5. Table 1 gives an overview of test and training set privacy results for the dependence scores that we consider. ˜ ) be the Let (x , y ) be the initial test data and (˜ x ,y test data after a single change in the dataset. Let ˜ = [ x 1 , . . . , x k −1 , x x ˜k , xk+1 , . . . , xm ] and similarly ˜ so that this single change occurs at some index for y k . The key to preserving privacy is to show that the selected dependence score s(·, ·) can be privatized. We show that if our dependence score is a rank correlation coeﬃcient (Spearman’s ρ, Kendall’s τ ) or the HSIC score [11], we can readily bound its test set global sen˜ ). As the sitivity when applied to (x , y ) versus (˜ x ,y IQR score has bounded test set local sensitivity we can apply the algorithm of Dwork & Lei [4] for privacy. 4.1 Rank Correlation Coeﬃcients

Proof. Our goal is to bound the following global sensitivity in both scores: |s(x , rY ) − s(˜ x ,˜ rY )|. For Spearman’s ρ, suppose the change is on ak and bk , it is easy to verify that 1) di changes by at most 2, for i = k ; 2) dk changes by at most m − 1; 3) di ≤ m − 1 for all i. 2 Since d2 i − (di − 2) = 4(di − 1) ≤ 4(m − 2) for i = k , the maximum change inside the summation is upper bounded by (m − 1)(4m − 8) + (m − 1)2 . Therefore, global sensitivity of ρ is bounded by 30 6(m − 1)(5m − 3) ≤ 2 m(m − 1) m . For Kendall’s τ we can aﬀect at most (m − 1) pairs by moving a single element of x , as well as (m − 1) pairs for changing rY (either from concordant pairs to discordant pairs, or vice versa). Therefore, the global sensitivity of Kendall’s τ is |s(x , rY ) − s(˜ x ,˜ rY )| ≤
1 2 m(m

2(m − 1) 4 ≤ m − 1)

The bound on the global sensitivity ∆ of our scores enables us to apply the Laplace mechanism [6] to produce (2 , 0)-diﬀerentially private scores: pX →Y , pY →X . Speciﬁcally, we add Laplace noise Lap(0, ∆/ ) to our Spearman’s ρ and Kendall’s τ scores to preserve privacy w.r.t. the test set. Moreover, as a general property of diﬀerential privacy we can compute any functions on these private scores and, so long as they do not touch the data, the outputs of these functions are also private. This means that we can compute the inequality pX →Y < pY →X to decide if X causes Y or vice-versa privately. An important consideration is to what degree the addition of noise aﬀects the true decision: sX →Y < sY →X . Importantly, we can prove that, in certain cases, the addition of Laplace noise required by the mechanism is small enough to not change the direction of causal inference. These are cases in which there is a large ‘margin’ between the scores sX →Y and sY →X . So long

We ﬁrst demonstrate global sensitivity for the two rank correlation scores in Section 3.

Matt J. Kusner, Yu Sun, Karthik Sridharan, Kilian Q. Weinberger

as this margin is large enough and in the correct order the addition of Laplace noise has no eﬀect on the inference with high probability. Theorem 2. Given two random variables X, Y who have w.l.o.g. the causal relationship X → Y , assume that they produce correctly-ordered scores: sX →Y < sY →X , with margin γ = sY →X − sX →Y . Let pX →Y , pY →X be these scores after applying the Laplace mechanism [6] with scale σ = ∆/ then the probability of correct inference with these private scores is, P(pX →Y < pY →X ) = 1 − γ + 2σ − γ e σ. 4σ

To bound the inﬁnity norms, let L = HLH , then |Lij | = Lij − ≤4 as Lij ≤ 1 (this inequality also holds for HKH ). Finally, note that as there is only a single-element difference between (x , rY ) and (˜ x ,˜ rY ), we have that ˜ ˜ ). K − K 1 ≤ 2m − 1 (and also for L, L
m−11 In fact, we can improve this bound to 12 (m−1)2 using trace identities. We leave the proof of this to the appendix. Given this global sensitivity bound we can use Theorem 2 to guarantee that under certain conditions the Laplace mechanism w.h.p. does not change the direction of causal inﬂuence. m a=1

Laj

m

−

m b=1

Lib

m

+

m a,b=1 m2

Lab

We leave the proof to the appendix. Note that the probability of incorrect inference decreases nearly exponentially as the margin γ increases. This is a particularly nice property as the margin essentially describes the conﬁdence of the (non-private) causal inference prediction: large γ corresponds to high conﬁdence in the inference. Additionally, there is an exponential decrease as m and grow. In section 6, we show on real-world causal inference data that we can accurately recover the true causal direction for a variety settings. 4.2 HSIC Score

4.3

IQR Score

Unfortunately the IQR does not have a bounded global sensitivity, as there exist datasets for which the IQR can change by an unbounded amount. Instead, Dwork & Lei [4] oﬀer an eﬃcient technique to privately release the IQR. We give a slightly modiﬁed version of their Algorithm in the appendix. First the algorithm deﬁnes two intervals B1 and B2 which both contain IQR(X). If the IQR were to be pushed out of both of these intervals it would imply that the IQR changed by a factor of e. Therefore we loop over both intervals and calculate the number of points Aj that an adversary would need to change to push the IQR out of B1 or B2 . Note that Aj is itself a data-sensitive query and so, to preserve privacy of this query, we can add Laplace noise to it. Then, if one of these noisy estimates Rj = Aj + z , where z ∼ Lap(0, 1/ ) is larger than some threshold, it implies that with high probability (exactly 1−δ ), that the IQR(X) has multiplicative sensitivity of at most e, for the speciﬁc dataset X. Note that this is precisely the local sensitivity as deﬁned in Section 3, as it is speciﬁc to X. This means that we can add Laplace noise z to log IQR(X). If neither of the Rj are above the threshold then the algorithm returns null: ⊥. This algorithm was shown to be (3 , δ )-diﬀerentially private. In our case we would like to release four private IQR scores. Note that we must look at x three separate times: for IQR(x ), IQR(rY ), and IQR(rX ) (and three times as well for y ). Therefore for both x and y we are composing three diﬀerentially private outputs. Under simple composition this would lead to (9 , 3δ ) diﬀerential privacy for both x and y . However, we can make use of Corollary 3.21 in Dwork & Roth [5] to give ( , 3δ + δ )-diﬀerential privacy, for 0 < < 1 and

We begin by deﬁning the empirical estimate of the HSIC score given kernels k, l: HSICk,l (x , rY ) := 1 trace(KHLH ) (m − 1)2 (4)

where Kij = k (xi , xj ), Lij = l(rY,i , rY,j ) and Hij = δ{i=j } − 1/m. We assume k, l are bounded above by 1 (e.g., the squared exponential kernel, the Matern kernel [27]). Our goal is to show that when we replace ˜ ) the global sensitivity is small. (x , y ) with (˜ x ,y Speciﬁcally we prove the following theorem. Theorem 3. The score in eq. (4) has a global sensi16m−8 tivity of at most ( m−1)2 . Speciﬁcally, |HSICk,l (x , rY ) − HSICk,l (˜ x ,˜ rY )| ≤ 16m − 8 (m − 1)2

Proof. For simplicity deﬁne H(·, ·) := HSICk,l (·, ·). Note that, as the trace is cyclic: trace(KHLH ) = ˜ L ˜ be the kernels detrace(HKHL). Further, let K, ˜ ). Then as the data ﬁned on the modiﬁed data (˜ x ,y is represented purely through the kernel matrices and the trace is Lipschitz w.r.t. these matrices, we can apply the triangle inequality to yield, |H(x , rY ) − H(˜ x ,˜ rY )| ≤ ˜ 1 ˜ HLH ∞ K − K HKH ∞ L − L + (m − 1)2 (m − 1)2

1

Private Causal Inference

δ > 0, over three repeated mechanisms by ensuring each private mechanism is (3 , δ )-private, where 3 = /(2 6 log(1/δ )). The remaining question is whether this noise addition causes one to infer the incorrect causal direction. Again, as long as there is a signiﬁcant margin between the scores, we can preserve the correct causal inference with high probability as follows. Theorem 4. Let Qx = log IQR(x ), and similarly for Qy , QrX , QrY , be the true log-IQR scores. As well let Px , Py , PrX , PrY be the private versions, multiplied by ez noise where z ∼ Lap(0, 1/ ). The the following results hold: 1. [4] If the number of data-points needed to significantly change the IQR, Aj , is less than e then, the probability that any one of the private IQR P∗ is released is small: P P∗ =⊥ |A1 or A2 ≤ e ≤ 3δ . 2

space corresponding to the kernel function used. Similar to other work on private regression [34] we assume that |x|, |y | ≤ 1. The ridge regression algorithm can now be written as: λ w = argmin w w∈H 2
2 H

1 + n

n

(w φ(xi ) − yi )2 ,
i=1

(5)

where H is the corresponding Hilbert space. Practically speaking, even though w may be inﬁnitedimensional, because it always appears in an inner product with the feature mapping φ(x) we can utilize the ‘kernel trick’: k (xi , xj ) = φ(xi ) φ(xj ) to avoid having to represent w explicitly. ˆ(w∗ , ·) and f ˆ(w ˜ ∗ , ·) be the classiﬁers resulting Let f from the optimization problem in eq. (5) when trained ˜ ), respectively (and similarly for on (x, y) and (˜ x, y g ˆ). We show that the residuals in Algorithm 1 are bounded. Theorem 5. Say λ ≤ 1. Given that the classiˆ(w∗ , ·), f ˆ(w ˜ ∗ , ·) are the result of the optimizaﬁers f tion problem in eq. (5), the residuals of these functions rY are bounded as, rY , ˜ |ri,Y − r ˜i,Y | ≤ 8 nλ3/2 (6)

2. If all private log-IQR scores are released, and the relationship between the true scores holds Qx + QrY < Qy + QrX (which implies X → Y ), then the probability that we make the correct causal inference from the private scores is large, P[Px + PrY < Py + PrX ] = eσ 48σ 3 + 33σ 2 γ + 9σγ 2 + γ 3 1− 96σ 3 where γ = Qy + QrX − Qx + QrY , and σ = 1/ . The proof of these results is in the appendix. The ﬁrst result says that the probability that we release an IQR score just because too much noise was added to Aj is small. The second result says that with high probability we recover the true causal direction, depending on the size of the dataset.
−γ

for all i, where ri,Y , r ˜i,Y are the ith elements of rY , ˜ rY and m is the size of the test set. This bound holds equally for rX , ˜ rX . The proof of the above is inspired by the work of Shalev-Shwartz et al. [31] and Jain & Thakurta [14]. We place the proof in the appendix for the interested reader. As far as we are aware this is the tightest bound for the optimization problem in eq. (5), with a non-Lipschitz loss. In the following, we use this bound to preserve training set privacy for the dependence scores considered in the previous section. 5.1 Rank Correlation Coeﬃcients

5

Training Set Privacy

˜ ) be the Let (x, y) be the initial training data and (˜ x, y training data after a change in the dataset. Note that ˜ diﬀer in at most one element (similarly for x and x ˜ ). The length of both training datasets is n. y and y From Algorithm 1, the only way the training set can aﬀect the dependency scores sX →Y , sY →X is through ˆ, g the regression functions f ˆ, used to compute test set residuals rY , rX . We use the kernel ridge regression ˆ (and g method and so the functions f ˆ) can be writˆ ten in the form: f (w, x) = w φ(x), where φ(x) is a (possibly inﬁnite) feature space mapping to the Hilbert

Note that the bound in Theorem 5 directly implies that the ranking dependence scores have global sensitivity 1 (equal to the size of their ranges). To see this note that we can consider an adversarial situation in which the rank of every element of the residual rY changes when the training set is altered in one element (as all the residual elements may change). This means that the Laplace mechanism cannot guarantee useful privacy. Instead, note that both ranking scores may still have reasonably bounded local sensitivity. Speciﬁcally, if we consider the list of sorted residuals, it may be that there are large gaps between neighboring residuals. If this is the case then changing the training set by one point may not change the residual rankings. Thus, the

Matt J. Kusner, Yu Sun, Karthik Sridharan, Kilian Q. Weinberger

dataset id

Spearman's

⇢
161

Kendall's

⌧
4031

HSIC
2967

IQR

✏ ✏ ✏ ✏ Figure 1: Probability of correctly identifying the causal direction on datasets selected from the Cause-Eﬀect Pairs Challenge [12]. Datasets for which the scores perform well were selected in order to isolate the eﬀect of privatization on the scores.
dataset id

prob. of correct inference

161

↵ = 0.1
4031

↵=1

HSIC
4031

↵=2
4031

best

prob. of correct inference

4031

✏ Figure 2: Training set privacy for the HSIC score. The three left-most plots show how λ aﬀects the probability of correctly inferring the causal direction, while the right-most plot depicts this probability when the best λ is selected over a ∈ [0.1, 10]. See text for more details.
ranking scores are in some sense stable to changes in the training set (for certain sets). Deﬁnition 4. We call a function f k -stable on dataset D if modifying any k elements in D does not change the value of f . Speciﬁcally, f (D) = f (D∗ ) for all D∗ such that D can be transformed into D∗ with a minimum of k element substitutions. We say f is unstable on D if it is not even 1-stable on D. The distance to instability of a dataset D w.r.t. a function f is the number of elements that must be changed to reach an unstable dataset. With these deﬁnitions, we will use a modiﬁcation of the Propose-Test-Release framework that makes use of this stability as described in Algorithm 13 in Dwork & Roth [5]. Theorem 6. [5] Algorithm 13 [5] is ( , δ )diﬀerentially private. Further, for all β > 0 if s(x , rY ) is log(1/δ)+log(1/β ) -stable on rY , then Algorithm 13 releases s(x , rY ) w.p. at least 1 − β . A lower bound on the distance to instability d is easily given by noting that s(x , rY ) always outputs the same result as long as none of the ranks of rY change. Let γ be the smallest absolute distance between any two ranks. Then a lower bound on d is, d > nγλ3/2 /16 . This is the largest number of training points that may change so that the closest ranks moving towards each other do not overlap (given that they change by at most the amount in eq. 6). This lower-bound is suﬃcient to use Algorithm 13 [5] to privatize the ranking dependence scores. 5.2 HSIC Score

Theorem 7. For m ≥ 2, with kernels k, l ≤ 1 where l is Ll -Lipschitz, the HSIC score has a training set sensitivity as follows, √ 32Ll m HSICk,l (x , rY ) − HSICk,l (x , ˜ rY ) ≤ R n where R =
8 . λ3/2

The proof follows directly from Theorem 5 and Lemma 16 in Mooij et al. [23]. Thus, the Laplace mechanism gives us ( , 0)-diﬀerential privacy and Theorem 2 gives us our utility guarantee. 5.3 IQR Score

Similar to the test set privacy section we will use propose-test-release to give a useful, private IQR score. In fact, we will use IQR algorithm almost identically, except that we will deﬁne Aj as the number of training points required to move the IQR out of an interval. Note that a lower bound on Aj is simply the number of points required to move every input less than the median to the left and every input larger than the median

Private Causal Inference

Table 2: The non-private accuracies of the ANM model on a subset of the Cause-Eﬀect Pairs Challenge [12], as well as the probability of correct causal inference after privatization.
dataset ids size Spearman’s ρ Kendall’s τ HSIC [11] IQR [1] Spearman’s ρ Kendall’s τ HSIC [11] IQR [1] Spearman’s ρ Kendall’s τ HSIC [11] IQR [1] Spearman’s ρ Kendall’s τ HSIC [11] IQR [1] 4031 7713 0.50 ± 0.53 0.50 ± 0.53 1.00 ± 0.00 0.50 ± 0.53 0.56 ± 0.45 0.54 ± 0.48 0.68 ± 0.17 0.50 ± 0.00 0.50 ± 0.53 0.50 ± 0.53 0.85 ± 0.16 0.54 ± 0.04 0.50 ± 0.53 0.50 ± 0.53 0.92 ± 0.09 0.58 ± 0.09 597 7748 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.03 ± 0.00 0.00 ± 0.00 0.49 ± 0.00 0.50 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.39 ± 0.03 0.48 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.29 ± 0.04 0.46 ± 0.01 2209 7766 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00 0.10 ± 0.32 0.20 ± 0.02 0.00 ± 0.00 0.60 ± 0.01 0.50 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 0.98 ± 0.00 0.49 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00 0.49 ± 0.01 161 2132 1656 7782 7784 7803 = ∞ (non-private accuracies) 0.70 ± 0.48 0.90 ± 0.32 1.00 ± 0.00 0.00 ± 0.00 0.70 ± 0.48 0.80 ± 0.42 1.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00 0.70 ± 0.48 0.60 ± 0.52 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 0.00 ± 0.00 = 0 .1 0.57 ± 0.10 0.61 ± 0.06 0.92 ± 0.02 0.40 ± 0.06 0.69 ± 0.38 0.78 ± 0.24 1.00 ± 0.00 0.12 ± 0.09 0.50 ± 0.00 0.50 ± 0.01 0.50 ± 0.00 0.52 ± 0.00 0.50 ± 0.00 0.51 ± 0.00 0.50 ± 0.00 0.50 ± 0.00 =1 0.69 ± 0.43 0.91 ± 0.17 1.00 ± 0.00 0.06 ± 0.07 0.70 ± 0.48 0.81 ± 0.40 1.00 ± 0.00 0.00 ± 0.00 0.52 ± 0.01 0.55 ± 0.06 0.50 ± 0.01 0.66 ± 0.02 0.52 ± 0.00 0.58 ± 0.01 0.51 ± 0.01 0.48 ± 0.00 =2 0.69 ± 0.47 0.93 ± 0.17 1.00 ± 0.00 0.01 ± 0.02 0.70 ± 0.48 0.80 ± 0.42 1.00 ± 0.00 0.00 ± 0.00 0.55 ± 0.01 0.59 ± 0.11 0.51 ± 0.01 0.78 ± 0.02 0.54 ± 0.01 0.65 ± 0.02 0.52 ± 0.02 0.47 ± 0.01 2967 7771 901 7820 0.30 ± 0.48 0.80 ± 0.42 0.40 ± 0.52 0.90 ± 0.32 0.34 ± 0.21 0.76 ± 0.41 0.43 ± 0.06 0.50 ± 0.00 0.30 ± 0.41 0.80 ± 0.42 0.21 ± 0.25 0.50 ± 0.00 0.31 ± 0.45 0.80 ± 0.42 0.20 ± 0.26 0.51 ± 0.01 3484 7853 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00 0.00 ± 0.00 0.01 ± 0.00 0.00 ± 0.00 0.66 ± 0.03 0.50 ± 0.00 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.01 0.47 ± 0.01 0.00 ± 0.00 0.00 ± 0.00 1.00 ± 0.00 0.45 ± 0.01 1627 7862 1.00 ± 0.00 1.00 ± 0.00 0.10 ± 0.32 1.00 ± 0.00 0.82 ± 0.02 1.00 ± 0.00 0.50 ± 0.00 0.50 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 0.49 ± 0.01 0.51 ± 0.00 1.00 ± 0.00 1.00 ± 0.00 0.48 ± 0.02 0.52 ± 0.01

to the right (or the reverse of these), using the bound on r in eq. (6). The aforementioned privacy and utility results of the IQR propose-test-release framework apply here. The only diﬀerence is we just need to add noise to the IQR scores computed on the residuals, which implies (6 , 2δ )-privacy and that the results of Theorem 4 can be tightened.

6

Results

We test our methods for private release of causal inference statistics on a small subsets from the Cause-Eﬀect Pairs Competition collection Guyon [12]. Speciﬁcally, we randomly select 10 of the largest 25 datasets that have a causal direction either X → Y or Y → X . We average over 10 random 50/50 train/test splits of the data. Table 2 shows the non-private accuracy of the four dependence scores over these datasets. We show the probability of correct causal inference changes as these scores are made private w.r.t. the test set. Note that these scores are often complementary, with the ranking-based scores performing well on datasets in which HSIC does worse, and vice-versa. Figure 1 shows the eﬀect of privatization on the dependence scores: HSIC and IQR. Note that, for low (increased privacy), the probability of correct inﬂuence is lower as the amount of noise required blurs the true dependence scores. However, as increases, so does this probability, in some cases drastically. For the IQR score, recall that there is a probability that the algorithm returns null: ⊥, if Rj is less than a threshold controlled by δ . We investigated this probability, by varying δ ∈ [10−5 , 10−2 ] and sampling 10, 000 points from the appropriate Laplace distribution. We found that, for the IQR dataset in ﬁgure 1 every sample did not move Rj below the null threshold. Therefore, the probability of null is essentially 0.

The three left-most plots in Figure 2 demonstrate how λ, which has a large eﬀect on the training set sensitivity (as described in eq. 6) aﬀects the probability of correct inference. We perform this experiment for diﬀerent settings of , and each one produces a distinctive ‘hump’ shape. This is because for small λ the sensitivity bound (6) is too large to produce meaningful causal inference. Similarly, for large λ the kernelized regression algorithm (5) is overly-regularized, which produces a poor regressor and poor dependence scores. Only when λ is within a certain range do we balance the size of the sensitivity bound with the size of the regularization. This range grows larger as increases as the privacy setting becomes less strict (requiring less noise). The right-most plot shows the correct inference probability using the best λ for a range of ∈ [0.1, 10]. With proper selection of λ we can achieve high-quality causal inference that maintains privacy w.r.t. the training set.

7

Conclusion

We have presented, to the best of our knowledge, the ﬁrst work towards diﬀerentially private causal inference. There are numerous directions of future work including privatizing other causal inference frameworks (e.g. IGCI [16]), analyzing that ANM algorithm without train/test splits, as well as other dependence scores. As there is signiﬁcant overlap in the applications of causal inference and private learning we believe this work constitutes an important step towards making causal inference practical.

Acknowledgments
KQW and MJK are supported by NSF grants IIA1355406, IIS-1149882, EFRI-1137211. We thank the anonymous reviewers for their useful comments.

Matt J. Kusner, Yu Sun, Karthik Sridharan, Kilian Q. Weinberger

References
[1] B¨ uhlmann, Peter, Peters, Jonas, Ernest, Jan, et al. Cam: Causal additive models, high-dimensional order search and penalized regression. The Annals of Statistics, 42(6):2526–2556, 2014. [2] Chaudhuri, Kamalika, Monteleoni, Claire, and Sarwate, Anand D. Diﬀerentially private empirical risk minimization. JMLR, 12:1069–1109, 2011. [3] Dinur, Irit and Nissim, Kobbi. Revealing information while preserving privacy. In Proceedings of the SIGMOD-SIGACT-SIGART symposium on principles of database systems, pp. 202–210. ACM, 2003. [4] Dwork, Cynthia and Lei, Jing. Diﬀerential privacy and robust statistics. In Proceedings of the forty-ﬁrst annual ACM symposium on Theory of computing, pp. 371–380. ACM, 2009. [5] Dwork, Cynthia and Roth, Aaron. The algorithmic foundations of diﬀerential privacy. Theoretical Computer Science, 9(3-4):211–407, 2013. [6] Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and Smith, Adam. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography, pp. 265–284. Springer, 2006. [7] for Disease Control, Centers, Prevention, et al. How tobacco smoke causes disease: The biology and behavioral basis for smoking-attributable disease: A report of the surgeon general. Centers for Disease Control and Prevention (US), 2010. [8] Friedman, Nir and Nachman, Iftach. Gaussian process networks. In Proceedings of the Sixteenth conference on Uncertainty in artiﬁcial intelligence, pp. 211–219. Morgan Kaufmann Publishers Inc., 2000. [9] Geiger, Philipp, Janzing, Dominik, and Sch¨ olkopf, Bernhard. Estimating causal eﬀects by bounding confounding. In Proceedings of the 30th Conference on Uncertainty in Artiﬁcial Intelligence, pp. 240–249, 2014. [10] Geiger, Philipp, Zhang, Kun, Schoelkopf, Bernhard, Gong, Mingming, and Janzing, Dominik. Causal inference by identiﬁcation of vector autoregressive processes with hidden components. In ICML, pp. 1917– 1925, 2015. [11] Gretton, Arthur, Bousquet, Olivier, Smola, Alex, and Sch¨ olkopf, Bernhard. Measuring statistical dependence with hilbert-schmidt norms. In Algorithmic learning theory, pp. 63–77. Springer, 2005. [12] Guyon, I. Cause-eﬀect pairs kaggle competition, 2013. URL https://www.kaggle.com/c/ cause-effect-pairs/. [13] Hoyer, Patrik O, Janzing, Dominik, Mooij, Joris M, Peters, Jonas, and Sch¨ olkopf, Bernhard. Nonlinear causal discovery with additive noise models. In Advances in neural information processing systems, pp. 689–696, 2009. [14] Jain, Prateek and Thakurta, Abhradeep. Diﬀerentially private learning with kernels. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pp. 118–126, 2013. [15] Jain, Prateek, Kothari, Pravesh, and Thakurta, Abhradeep. Diﬀerentially private online learning. COLT, 2012.

[16] Janzing, Dominik, Mooij, Joris, Zhang, Kun, Lemeire, Jan, Zscheischler, Jakob, Daniuˇ sis, Povilas, Steudel, Bastian, and Sch¨ olkopf, Bernhard. Informationgeometric approach to inferring causal directions. Artiﬁcial Intelligence, 182:1–31, 2012. [17] Kano, Yutaka and Shimizu, Shohei. Causal inference using nonnormality. In Proceedings of the International Symposium on Science of Modeling, the 30th Anniversary of the Information Criterion, pp. 261– 270, 2003. [18] Kpotufe, Samory, Sgouritsa, Eleni, Janzing, Dominik, and Sch¨ olkopf, Bernhard. Consistency of causal inference under the additive noise model. In ICML, 2014. [19] Lopez-Paz, David, Muandet, Krikamol, Sch¨ olkopf, Bernhard, and Tolstikhin, Iliya. Towards a learning theory of cause-eﬀect inference. In ICML, 2015. [20] McSherry, Frank and Talwar, Kunal. Mechanism design via diﬀerential privacy. In FOCS, pp. 94–103. IEEE, 2007. [21] Mooij, Joris M, Stegle, Oliver, Janzing, Dominik, Zhang, Kun, and Sch¨ olkopf, Bernhard. Probabilistic latent variable models for distinguishing between cause and eﬀect. In Advances in Neural Information Processing Systems, pp. 1687–1695, 2010. [22] Mooij, Joris M, Janzing, Dominik, Heskes, Tom, and Sch¨ olkopf, Bernhard. On causal discovery with cyclic additive noise models. In Advances in neural information processing systems, pp. 639–647, 2011. [23] Mooij, Joris M, Peters, Jonas, Janzing, Dominik, Zscheischler, Jakob, and Sch¨ olkopf, Bernhard. Distinguishing cause from eﬀect using observational data: methods and benchmarks. arXiv preprint arXiv:1412.3773, 2014. [24] Nissim, Kobbi, Raskhodnikova, Sofya, and Smith, Adam. Smooth sensitivity and sampling in private data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pp. 75–84. ACM, 2007. [25] Pearl, Judea. Causality: models, reasoning, and inference. 2000. [26] Peters, Jonas, Mooij, Joris M, Janzing, Dominik, and Sch¨ olkopf, Bernhard. Causal discovery with continuous additive noise models. The Journal of Machine Learning Research, 15(1):2009–2053, 2014. [27] Rasmussen, Carl Edward and Williams, Christopher K. I. Gaussian processes for machine learning. 2006. [28] Reichenbach, Hans and Reichenbach, Maria. The direction of time. Univ of California Press, 1956. [29] Sgouritsa, Eleni, Janzing, Dominik, Hennig, Philipp, and Sch¨ olkopf, Bernhard. Inference of cause and eﬀect with unsupervised inverse regression. In AISTATS, pp. 847–855, 2015. [30] Shajarisales, Naji, Janzing, Dominik, Shoelkopf, Bernhard, and Besserve, Michel. Telling cause from eﬀect in deterministic linear dynamical systems. In ICML, 2015. [31] Shalev-Shwartz, Shai, Shamir, Ohad, Srebro, Nathan, and Sridharan, Karthik. Stochastic convex optimization. In COLT, 2009.

Private Causal Inference [32] Spirtes, Peter, Glymour, Clark N, and Scheines, Richard. Causation, prediction, and search, volume 81. MIT press, 2000. [33] Sun, Xiaohai, Janzing, Dominik, and Sch¨ olkopf, Bernhard. Causal reasoning by evaluating the complexity of conditional densities with kernel methods. Neurocomputing, 71(7):1248–1256, 2008. [34] Talwar, Kunal, Thakurta, Abhradeep, and Zhang, Li. Private empirical risk minimization beyond the worst case: The eﬀect of the constraint set geometry. arXiv preprint arXiv:1411.5417, 2014. [35] Zhang, Kun and Hyv¨ arinen, Aapo. On the identiﬁability of the post-nonlinear causal model. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, pp. 647–655. AUAI Press, 2009.

