Learning with Marginalized Corrupted Features

Laurens van der Maaten lvdmaaten@gmail.com Delft University of Technology, Mekelweg 4, 2628 CD Delft, THE NETHERLANDS Minmin Chen Stephen Tyree Kilian Q. Weinberger Washington University, St. Louis, MO 63130, USA mc15@cec.wustl.edu swtyree@wustl.edu kilian@wustl.edu

Abstract
The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (inﬁnite) training data sets that capture all variations in the data distribution. In the case of ﬁnite training data, an eﬀective solution is to extend the training set with artiﬁcially created examples—which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution— essentially learning with inﬁnitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classiﬁers can be trained eﬃciently, may generalize substantially better to test data, and are more robust to feature deletion at test time.

this paper, we propose an algorithm to train a classiﬁer from inﬁnite training data, by corrupting the existing ﬁnite training examples with a ﬁxed noise distribution. So instead of approximating the exact statistics of P with ﬁnite data, we approximate a slightly modiﬁed data distribution P with inﬁnite data. Our augmented data distribution P follows a simple stochastic rule: pick one of the ﬁnite training examples uniformly at random and transform it with some predeﬁned corrupting distribution. Many corrupting distributions are possible, but we focus on: i) Poisson corruption and ii) blankout / dropout corruption (random deletion of features). The Poisson corruption model is of interest when the data comprises count vectors, e.g., in document classiﬁcation. It is particularly appealing as it introduces no additional hyper-parameters and, in our results, improves the test accuracy on almost all data sets and loss functions. Robustness to blankout corruption is of interest in settings with heavy-tailed feature distributions, and in the “nightmare at test time” scenario (Globerson & Roweis, 2006) in which some of the features are blanked out during testing (e.g., due to sensors failing or because the feature computation exceeds a time budget). Previous work (Burges & Sch¨ olkopf, 1997) explicitly augments the training set with additional examples that are corrupted through similar transformations. Although the simplicity of such an approach is appealing, it lacks elegance and the computational cost of processing the additional corrupted training examples is prohibitive for most real-world problems. We show that it is eﬃcient to train predictors on an inﬁnite amount of corrupted copies of the training data by marginalizing out the corrupting distribution. In particular, we focus on empirical risk minimization and derive analytical solutions for a large family of corrupting distributions and a variety of loss functions.

1. Introduction
In the hypothetical scenario with inﬁnite data drawn from the data distribution P , even a simple classiﬁer such as nearest neighbor (Cover & Hart, 1967) becomes close to optimal (viz. its error is twice the Bayes error). In the more realistic scenario with a ﬁnite training set, some variations in the data distribution will not be captured and the learned classiﬁer performs worse at test time than during training. In
Appearing in Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. Copyright 2013 by the author(s)/owner(s).

Learning with Marginalized Corrupted Features

In summary, we make the following contributions: i) we introduce learning with marginalized corrupted features (MCF), a framework that regularizes classiﬁers by marginalizing out feature corruptions; ii) we derive analytical solutions for quadratic, exponential, and logistic loss functions for a range of corrupting distributions; and iii) on several real-world data sets, we show that training with MCF may lead to better classiﬁers than training with common l1 or l2 -norm regularizers.

2. Related work
Next to work that explicitly corrupts training data (Burges & Sch¨ olkopf, 1997), several prior studies consider implicit approaches to classifying objects that are subject to corruptions. Most of these studies minimize the loss under an adversarial worst-case scenario. In particular, Globerson & Roweis (2006) and Dekel & Shamir (2008) propose minimax formulations in which D the maximum loss of an example over all K possible corrupted examples that blank out K features is minimized. Others (Bhattacharyya et al., 2004; Shivaswamy et al., 2006; Trafalis & Gilbert, 2007; Xu et al., 2009) also use minimax approaches that minimize the loss under a worst-case scenario, but corrupt the data by adding a constant that is uniformly drawn from [−u, u] to each of the features. Chechik et al. (2008) propose an algorithm that maximizes the margin in the subspace of the observed features for each training instance to deal with randomly deleted features. Teo et al. (2008) generalize the worstcase scenario to obtain invariances to transformations such as image rotations or translations. Their framework incorporates several prior formulations on learning with invariants as special cases; e.g., Herbrich & Graepel (2004), who generalized SVMs to be invariant under polynomial input transformations. Br¨ uckner et al. (2012) study a worst-case scenario in which an adversary changes the data distribution to minimize a function that may be antagonistic to the loss. Prior work diﬀers from MCF in that the corruption is not computed analytically in expectation. In particular, existing approaches have two disadvantages: i) they are complex and computationally expensive and ii) they minimize the loss of a worst-case scenario that is unlikely to be encountered in practice. By contrast, MCF scales linearly in the number of training examples and considers an average-case instead of a worstcase scenario. Moreover, MCF can readily be used with a variety of loss functions and corruption models. Bishop (1995) and Webb (1994) proposed approaches that can be viewed as approximations to MCF for ad-

ditive noise with small variance. By contrast, MCF is exact and can be used with noise distributions with potentially very high variance as well. MCF was inspired by recent successes of denoising autoencoders (Glorot et al., 2011; Vincent et al., 2008). Since autoencoders are non-linear, the marginalization over the corrupting distribution cannot be performed analytically in such models. Linear denoising autoencoders (Chen et al., 2012) can be viewed as a special case of MCF that aim to minimize the expected value of the reconstruction error under blankout corruption.

3. Learning with Marginalized Corrupted Features (MCF)
To derive the marginalized corrupted features (MCF) framework, we start by deﬁning a corrupting distribution that speciﬁes how training observations x are ˜ . We assume transformed into corrupted versions x that the corrupting distribution factorizes over dimensions1 and that each individual distribution is a member of the natural exponential family:
D

p(˜ x|x) =

d=1

PE (˜ xd |xd ; ηd ),

(1)

where ηd indicates (user-deﬁned) parameters of the corrupting distribution on dimension d. We also assume that the corrupting distribution is unbiased, i.e. that E[˜ x]p(˜ x|x) = x. This assumption is necessary because biases in the corrupting distribution may lead to undesired scale changes in the parameters ηd . Most corrupting distributions of interest are unbiased or can easily be made so; examples for PE are the unbiased “blankout” noise (Vincent et al., 2008), Gaussian noise, Laplace noise, and Poisson noise. Explicit corruption. A simple approach to improving the generalization of a classiﬁer using a corrupting distribution is to follow the spirit of Burges & Sch¨ olkopf (1997) by selecting each element of the training set D = {(xn,yn )}N n=1 and corrupting it M times, following (1). For each xn , this results in correspond˜ nm (with m = 1, . . . , M ). ing corrupted observations x ˜ of This leads to the construction of a new data set D ˜ = M N . This extended data set can be used size |D| for training by minimizing: ˜ ; Θ) = L(D 1 M n=1
N M

L(˜ xnm , yn ; Θ),
m=1

˜ nm ∼ p(˜ with x xnm |xn ), Θ the set of model parameters, and L(x, y ; Θ) the loss function of the model.
1 For Gaussian noise models, this assumption is unnecessary: we can also work with non-isotropic Gaussian noise.

Learning with Marginalized Corrupted Features

Distribution Blankout noise Gaussian noise Laplace noise Poisson noise

PDF p(˜ xnd = 0) = qd 1 p(˜ xnd = 1− qd xnd ) = 1 − qd p(˜ xnd |xnd ) = N (˜ xnd |xnd , σ 2 ) p(˜ xnd |xnd ) = Lap(˜ xnd |xnd , λ) p(˜ xnd |xnd ) = P oisson(˜ xnd |xnd )

E[˜ xnd ]p(˜ xnd |xnd ) xnd xnd xnd xnd

V [˜ xnd ]p(˜ xnd |xnd )
1 2 1−qd xnd

σ2 2λ2 xnd

Table 1. The probability density function (PDF), mean, and variance of corrupting distributions of interest. These quantities can be plugged into Eq. (3) to obtain the expected value under the corrupting distribution of the quadratic loss.

Implicit corruption. Although such an approach is eﬀective, it lacks elegance and comes with high com˜ ; Θ) scales putational costs, as the minimization of L(D linearly in the number of corrupted observations. It is, however, of interest to consider the limiting case in which M → ∞. In this case, we can apply the weak law M 1 xm , ym ; Θ) of large numbers and rewrite M m=1 L(˜ as its expectation (Duda et al., 2001, §2.10.2):
N

where V [x] is a diagonal D×D matrix storing the variances of x, and all expectations are under p(˜ xn |xn ). Eq. (3) is convex irrespective of what corruption model is used; the optimal solution w∗ is given by:
N −1 N

w∗ =
n=1

E[˜ xn ]E[˜ xn ]T + V [˜ xn ]
n=1

yn E [˜ xn ] .

L(D; Θ) =

E[L(˜ xn , yn ; Θ)]p(˜ xn |xn ) .
n=1

(2)

Minimizing the expected value of the loss under the corruption model leads to a new approach for training predictors that we refer to as learning with marginalized corrupted features (MCF). 3.1. Speciﬁc loss functions The tractability of (2) depends on the choice of loss function and corrupting distribution PE . In this section, we show that for linear predictors that employ a quadratic or exponential loss function, the required expectations under p(˜ x|x) in (2) can be computed analytically for all corrupting distributions in the natural exponential family. For linear predictors with logistic loss, we derive a practical upper bound on the expected loss under p(˜ x|x), which serves as surrogate loss.

To minimize the expected quadratic loss under the corruption model, we only need to compute the variance of the corrupting distribution, which is practical for all exponential-family distributions. The mean is always xnd because our corrupting distributions are unbiased. Table 1 gives an overview of the variances of corrupting distributions of interest. (In blankout corruption, 1 we scale the value of “preserved” features by 1− qd to ensure that the corrupting distribution is unbiased.) An interesting setting of MCF with quadratic loss occurs when the corrupting distribution p(˜ x|x) is an isotropic Gaussian distribution with mean x and variance σ 2 I. For such a Gaussian corruption model, we obtain as special case (Chapelle et al., 2000):
N

L(D; w) = wT
N

xn xT n w
n=1 T

Quadratic loss. Assuming2 a label variable y ∈ {−1, +1}, the expected value of the quadratic loss under corrupting distribution p(˜ x|x) is given by:
N 2

−2

yn xn
n=1

w + σ 2 N wT w + N,

L(D; w) =

E
n=1 N T

˜ n − yn wT x
T

which is the standard l2 -regularized quadratic loss with regularization parameter σ 2 N . Interestingly, using MCF with Laplace noise also leads to ridge regression (with regularization parameter 2λ2 N ). Exponential loss. The expected value of the exponential loss under corruption model p(˜ x|x) is:
N

p(˜ xn |xn )

=w

E[˜ xn ]E[˜ xn ] + V [˜ xn ] w
n=1 N T

−2
2

yn E [˜ xn ]
n=1

w + N,

(3)

L(D; w) = =

E e−yn w
n=1 N D

T

˜n x p(˜ xn |xn )

The same derivations are applicable to regression settings in which y is a continuous variable.

˜nd E e−yn wd x n=1 d=1

p(˜ xnd |xnd )

,

(4)

Learning with Marginalized Corrupted Features

Distribution Blankout noise Gaussian noise Laplace noise Poisson noise

E[exp(−yn wd x ˜nd )]p(˜ xnd |xnd ) 1 qd + (1 − qd ) exp(−yn wd 1− qd xnd ) 1 2 2 2 exp(−yn wd xnd + 2 σ yn wd ) 2 2 −1 (1 − λ2 yn wd ) exp(−yn wd xnd ) exp(xnd (exp(−yn wd ) − 1))

upper bound of the logistic loss (5), where inputs are corrupted with the Poisson distribution:
N D

L(D; w) ≤

log 1 + exp
n=1 d=1

xnd (e−yn wd − 1)

.

Table 2. Moment-generating functions (MGFs) of various corrupting distributions. These quantities can be plugged into equations (4) and (5) to obtain the expected value of the loss (or surrogate) under the corrupting distribution of the exponential and logistic loss functions, respectively.

Remarkably, this regularized loss does not have any additional hyper-parameters. Further, the sum over all features can still be computed eﬃciently for sparse data by summing only over non-zero entries in xn . Computational complexity. The use of MCF does not impact the computational complexity of training: the complexity of the training algorithms remains linear in N . The additional training time for minimizing quadratic loss with MCF is negligible, because the computation time is dominated by the inversion of a D × D-matrix. For exponential and logistic loss, we empirically found that computing the gradient of the MCF loss was 2× and 10× slower than computing the gradient of the “normal” loss, respectively.

where we used the assumption that the corruption is independent across features. The above equation can be recognized as a product of moment-generating functions E[exp(tnd x ˜nd )] with tnd = −yn wd . The momentgenerating function (MGF) can be computed for all corrupting distributions in the natural exponential family. An overview of these MGFs is given in Table 2. Because the expected exponential loss is a convex combination of convex functions, it is itself convex irrespective of what corruption model is used. The derivation above can readily be extended to multiclass exponential loss (Zhu et al., 2006) (with K classes) by replacing the weight vector w by a D × K weight matrix W, and by replacing the labels y by K K label vectors y = {1, − K1 with k=1 yk = 0. −1 } Logistic loss. In the case of the logistic loss, the solution to (2) cannot be computed in closed form. Instead, we derive an upper bound, which can be minimized as a surrogate loss:
N

4. Experiments
We perform experiments comparing MCF predictors with standard predictors on three tasks: i) document classiﬁcation based on word-count features using MCF with blankout and Poisson corruption; ii) image classiﬁcation based on bag-of-visual-word features using MCF with blankout and Poisson corruption; and iii) classiﬁcation of objects in the “nightmare at test time” scenario using MCF with blankout corruption. The three experiments are described separately below. Code to reproduce the results of our experiments is available from http://homepage. tudelft.nl/19j49/mcf. 4.1. Document classiﬁcation We ﬁrst test MCF predictors with blankout and Poisson corruption on document classiﬁcation tasks. Speciﬁcally, we focus on three data sets: the Dmoz data set, the Reuters data set, and the Amazon review benchmark set (Blitzer et al., 2007). Data sets. The Dmoz open directory (http://www. dmoz.org) contains a large collection of webpages arranged into a tree hierarchy. The subset we use contains N = 8, 980 webpages from the K = 16 categories in the top level of the hierarchy. Each webpage is represented by a bag-of-words representation with D = 16, 498 words. The Reuters data set is a collection of documents that appeared on the Reuters newswire in 1987. Documents with multiple labels were removed from the data, resulting in a set of N = 8, 293 documents from K = 65 categories. The bag-of-words

L(D; w) =
N

E log 1 + e−yn w
n=1 D ˜nd E e−yn wd x d=1

T

˜n x p(˜ xn |xn )

≤

log 1 +
n=1

p(˜ xnd |xnd )

.

(5)

Herein, we have made use of Jensen’s inequality to upper-bound E[log z ]. In the upper bound, we again recognize a product of MGFs, which can be computed in closed-form for corrupting distributions in the natural exponential family (see Table 2). The upper bound on the expected logistic loss is convex whenever the moment-generating function is log-linear in wd , e.g., for blankout corruption. Again, the above derivation can readily be extended to multi-class logistic loss (by redeﬁning the labels y K to be label vectors y ∈ {0, 1}K with k=1 yk = 1; and by deﬁning the loss as the logarithm of the softmaxprobability of making the correct prediction). Case study. As an illustrative example, we show the

Learning with Marginalized Corrupted Features
0.15

Amazon / Electronics
Quadratic loss Exponential loss Logistic loss Blankout MCF (Qua.) Blankout MCF (Exp.) Blankout MCF (Log.) Poisson MCF (Qua.) Poisson MCF (Exp.) Poisson MCF (Log.)

0.18

Amazon / DVD

0.18

Amazon / Books

0.14

0.17 0.17 0.16 0.16

0.13

0.15

0.12

Classiﬁcation error

0.15 0.11 0 0.2 0.4 0.6 0.8 1 0 0.48 0.46 0.44 0.42 0.2 0.4 0.6 0.8 1

0.14

0.13 0 0.18 0.16 0.14 0.12

0.2

0.4

0.6

0.8

1

Amazon / Kitchen
0.12

DMOZ

Reuters

0.11

0.4 0.38 0.36 0.1 0.08 0.06 0

0.1 0

0.2

0.4

0.6

0.8

1

0.34 0

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.8

1

Blankout corruption level q

Figure 1. Classiﬁcation errors of MCF predictors using blankout and Poisson corruption – as a function of the blankout corruption level q – on the Amazon, Dmoz, and Reuters data sets for l2 -regularized quadratic, exponential, and quadratic loss functions. Classiﬁcation errors are represented on the y -axis, whereas the blankout corruption level q is represented on the x-axis. The case of MCF with blankout corruption and q = 0 corresponds to a standard l2 -regularized classiﬁer. Figure best viewed in color.

representation contains D = 18, 933 words for each document. The four Amazon data sets consist of approximately N = 6, 000 reviews of four types of products: books, DVDs, electronics, and kitchen appliances. Each review is represented by a bag-of-words representation of the D = 20, 000 most common words. On the Dmoz and Reuters data sets, the task is to classify the documents into one of the predeﬁned categories. On the Amazon data set, the task is to decide whether a review is positive or negative. Setup. On the Dmoz and Reuters data sets, we use a ﬁxed training set of 75% of the data, and evaluate the performance of our predictors on a ﬁxed test set of 25% of the data. On the Amazon data set, we follow the experimental setup of Blitzer et al. (2007) by using a ﬁxed division of the data into approximately 2, 000 training examples and about 4, 000 test examples (the exact numbers vary slightly between tasks). All experiments are performed using linear classiﬁers that are trained with l2 -regularization; the amount of l2 regularization is determined via cross-validation. We consider quadratic, exponential, and logistic loss functions (both with and without MCF). The minimization of the (expected) exponential and logistic losses is performed by running Mark Schmidt’s minFunc-

implementation of L-BFGS until convergence or until a predeﬁned maximum number of iterations is reached. All predictors included a bias term that is neither regularized nor corrupted. In our experiments with MCF using blankout corruption, we use the same noise level for each feature, i.e. we assume that ∀d : qd = q . On all data sets, we ﬁrst investigate the performance of MCF as a function of the corruption level q (but we still cross-validate over the l2 -regularizer). In a second set of experiments, we cross-validate over the blankout corruption parameter q and study to what extent the performance (improvements) of MCF depend on the amount of available training data. (MCF with Poisson corruption has no additional hyper-parameters, as a result of which it requires no extra cross-validations.) Results. Figure 1 shows the test error of our MCF predictors on all data sets as a function of the blankout corruption level q . Herein, corruption level q = 0 corresponds to the baseline predictors, i.e. to predictors that do not employ MCF. The results show: i) that MCF improves over standard predictors for both blankout corruption (for all corruption levels q ) and Poisson corruption on ﬁve out of six tasks; ii) that MCF with Poisson corruption leads to signiﬁ-

Learning with Marginalized Corrupted Features

cant performance improvements over standard classiﬁers whilst introducing no additional hyperparameters; and iii) that the best performance tends to be achieved by MCF with blankout corruption with high corruption levels, i.e. when q is in the order of 0.8. The best-performing MCF classiﬁers reduce the test errors by up to 22% on the Amazon data if q is properly set. In many of the experiments with MCF-trained losses (in particular, when blankout corruption is used), we also observe that the optimal level of l2 -regularization is 0. This shows that MCF has a regularizing eﬀect in itself, rendering additional regularization superﬂuous. Figure 2 presents the results of a second set of experiments on Dmoz and Reuters in which we study how the performance of MCF depends on the amount of training data. For each training set size, we repeat the experiment ﬁve times with randomly sub-sampled training sets; the ﬁgure reports the mean test errors and the corresponding standard deviations. The results show that classiﬁers trained with MCF (solid curves) significantly outperform their counterparts without MCF (dashed curves). The performance improvement is consistent irrespective of the training set size, viz. up to 25% on the Dmoz data set. Explicit vs. implicit feature corruption. Figure 3 shows the classiﬁcation error on Amazon (books) when a classiﬁer without MCF is trained on the data set with additional explicitly corrupted samples, as formulated in (3). Speciﬁcally, we use the blankout corruption model with q set by cross-validation for each setting, and we train the classiﬁers with quadratic loss and l2 -regularization. The graph shows a clear trend that the error decreases when the training set contains more corrupted versions of the original training data, i.e. with higher M in eq. (3). The graph illustrates that the best performance is obtained as M approaches inﬁnity, which is equivalent to MCF with blankout corruption (marker in the bottom right; q = 0.9). 4.2. Image classiﬁcation

Blankout / DMOZ
0.8 0.7 0.6 0.5 0.4 100 200 400 800 1600 3200 7184 Quadratic loss (L2) Exponential loss (L2) Logistic loss (L2) Blankout MCF (qua.) Blankout MCF (exp.) Blankout MCF (log.)

Blankout / Reuters
0.3 0.25 0.2 0.15

Classiﬁcation error

0.1 100 200 400 800 1600 3200 5946

Poisson / DMOZ
0.8 0.7 0.6 0.5 0.4 100 200 400 800 1600 3200 7184 Quadratic loss (L2) Exponential loss (L2) Logistic loss (L2) Poisson MCF (qua.) Poisson MCF (exp.) Poisson MCF (log.)

Poisson / Reuters
0.3 0.25 0.2

We performed image-classiﬁcation experiments on the CIFAR-10 data set (Krizhevsky, 2009), which is a subset of the 80 million tiny images (Torralba et al., 2008). The data set contains RGB images with 10 classes of size 32 × 32, and consists of a ﬁxed training and test set of 50, 000 and 10, 000 images, respectively. Setup. We follow the experimental setup of Coates et al. (2011): we whiten the training images and extract a set of 7 × 7 image patches on which we apply k -means clustering (with k = 2048) to construct a codebook. Next, we slide a 7 × 7 pixel window over each image and identify the nearest prototype in the

0.15 0.1 100 200 400 800 1600 3200 5946

# of labeled training data

Figure 2. The performance of standard and MCF classiﬁers with blankout and Poisson corruption models as a function of training set size on the Dmoz and Reuters data sets. Both the standard and MCF predictors employ l2 regularization. Figure best viewed in color.

Learning with Marginalized Corrupted Features
0.19 Explicit corruption Implicit corruption (MCF) 0.18
Error Classiﬁcation error

0.17 0.16 0.15 0.14 0.13 0

No MCF Poisson MCF Blankout MCF

Quadr. 32.6% 29.1% 32.3%

Expon. 39.7% 39.5% 37.9%

Logist. 32.5% 30.0% 29.4%

Table 3. Classiﬁcation errors obtained on the CIFAR-10 data set with MCF classiﬁers trained on simple spatialpyramid bag-of-visual-words features.

1

2

4

8

16 32 64 128 256

…...

Noise # of corrupted copies

1 1 inf

time” scenario, we perform experiments on the MNIST handwritten digits data set. The MNIST data set contains N = 60, 000 training and 10, 000 test images of size D = 28 × 28 = 784 pixels with K = 10 classes. Setup. We train our predictors on the full training set, and evaluate their performance on versions of the test set in which a certain percentage of the pixels are randomly blanked out, i.e. set to zero. We compare the performance of our MCF-predictors (using blankout corruption) with that of standard predictors that use l1 or l2 -regularized quadratic, exponential, logistic, and hinge loss. As before, we use cross-validation to determine the optimal value of the regularization parameter. For MCF predictors, we also cross-validate over the blankout corruption level q (again, we use the same noise level for each feature, i.e. ∀d : qd = q ). In addition to the comparisons with standard predictors, we also compare the performance of MCF with that of FDROP (Globerson & Roweis, 2006), which is a state-of-the-art algorithm for the “nightmare at test time” setting that minimizes the hinge loss under an adversarial worst-case scenario. The performances are reported as a function of the feature-deletion percentage in the test set, i.e. as a function of the probability with which a pixel in the test set is switched oﬀ. Following the experimental setting of Globerson & Roweis (2006), we perform the cross-validation for each deletion percentage independently, i.e. we create a small validation set with the same feature-deletion level and use it to determine the best regularization parameters and blankout corruption level q for that percentage of feature deletions. Results. Figure 4 shows the performance of our predictors as a function of the percentage of deletions in the test images. The ﬁgure shows the performance for all three loss functions with MCF (solid lines) and without MCF (dashed lines). The performance of a standard predictor using hinge loss is shown as a red dashed line; the performance of FDROP is shown as a black dashed line. The results presented in Figure 4 clearly illustrate the ability of MCF with blankout corruption to produce predictors that are robust to the “nightmare at test time” scenario: MCF improves the performance substantially for all three loss func-

Figure 3. Comparison between MCF and explicitly adding corrupted examples to the training set (for quadratic loss) on the Amazon (books) data using blankout corruption. Training with MCF is equivalent to using inﬁnitely many corrupted copies of the training data.

codebook for each window location. We construct an image descriptor3 by subdividing the image into four equally sized quadrants and counting the number of times each prototype occurs in each quadrant. This leads to a descriptor of dimensionality D = 4 × 2048. We do not normalize the descriptors, because all images have the same size. We train MCF predictors with blankout and Poisson corruption on the full set of training images, cross-validating over a range of l2 regularization parameters. The generalization error is evaluated on the test set. Results. The results are reported in Table 3. The baseline classiﬁers (without MCF) are comparable to the 68.8% accuracy reported by Coates et al. (2011) with exactly the same experimental setup (except for exponential loss). The results illustrate the potential of MCF classiﬁers to improve the prediction performance on bag-of-visual-words features, in particular, when using quadratic or logistic loss in combination with a Poisson corruption model. Although our focus in this section is to merely illustrate the potential of MCF on image classiﬁcation tasks, it is worth noting that the best results in Table 3 match those of a highly non-linear mean-covariance RBMs trained on the same data (Ranzato & Hinton, 2010), despite our use of very simple visual features and of linear classiﬁers. 4.3. Nightmare at test time To test the performance of our MCF predictors with blankout corruption under the “nightmare at test
This way of extracting the image features is referred to by Coates et al. (2011) as k-means with hard assignment, average pooling, patch size 7 × 7, and stride 1.
3

Learning with Marginalized Corrupted Features
0.5 Quadratic loss (L2) Exponential loss (L2) Logistic loss (L2) Hinge loss (L2) Hinge loss (FDROP) MCF quadratic loss MCF exponential loss MCF logistic loss

0.4

0.3

0.2

0.1

formances without introducing any additional hyperparameters. As a disclaimer, care must be taken when applying MCF with Poisson corruption on data sets with outliers. Poisson corruption may emphasize outliers in the expected loss because the variance of a Poisson distribution is equal to its mean, and because our loss functions are not robust to outliers. A solution to this problem may be to redeﬁne the corruption distribution to p(˜ xd |xd ) = P ois(˜ xnd | min{xnd , u}) for some cutoﬀ parameter u ≥ 0.
40% 50% 60% 70% 80% 90%

0%

10%

20%

30%

Figure 4. Classiﬁcation errors of standard and MCF predictors with a blankout corruption model – trained using three diﬀerent losses – and of FDROP (Globerson & Roweis, 2006) on the MNIST data set using the “nightmare at test time” scenario. Classiﬁcation errors are represented on the y -axis, whereas the percent of features that are deleted at test time is represented on the x-axis. Figure best viewed in color.

tions considered. For instance, in the case in which 50% of the pixels in the test images is deleted, the performance improvements obtained using MCF for quadratic, exponential, and logistic loss are 40%, 47%, and 60%, respectively. Further, the results also indicate that MCF-losses may outperform FDROP: our MCF logistic loss outperforms FDROP’s worst-case hinge loss across the board4 . This is particularly impressive as the standard hinge loss performs surprisingly better than standard logistic loss on this data set (the improvement of FDROP over the generic hingeloss is relatively modest). This result suggests that it is better to consider an average-case than a worst-case scenario in the “nightmare at test time” setting.

In most of our experiments with MCF, the l2 regularizer parameter (which was set by crossvalidation) ended up very close to zero. This implies that MCF in itself has a regularizing eﬀect. At the same time, MCF with blankout corruption also appears to prevent weight undertraining (Sutton et al., 2005): it encourages the weight on each feature to be non-zero, in case this particular feature survives the corruption. Our experiments also reveal that MCF with blankout corruption produces predictors that are more robust to the “nightmare at test time” scenario, making it useful in learning settings in which features in the test data may be missing. Learning with MCF is quite diﬀerent from previous approaches for this setting (Dekel & Shamir, 2008; Globerson & Roweis, 2006): it does not learn under a worst-case scenario, but the (arguably) more common average-case scenario by considering all possible corrupted observations. This has the advantage that it is computationally much cheaper and that it allows for incorporating prior knowledge. For instance, if the data is generated by a collection of unreliable sensors, knowledge on the sensor reliability may be used to set the qd -parameters. In future work, we intend to explore extensions of MCF to regression and structured prediction, as well as to investigate if MCF can be employed for kernel machines. We also plan to explore in more detail what corruption models p(˜ x|x) are useful for what types of data, and how these corruption models regularize classiﬁers (Ng, 2004). Further, MCF could be used in the training of neural networks with a single layer of hidden units: blankout noise on the hidden nodes can improve the performance of the networks (LeCun et al., 1990; Sietsma & Dow, 1991; Hinton et al., 2012) and can be marginalized out analytically. A ﬁnal interesting direction is to investigate the eﬀect of marginalizing corrupted labels (Lawrence & Sch¨ olkopf, 2001).
Acknowledgements LvdM is supported by FP7 Social Signal Processing (SSPNet). KQW, MC, and ST are supported by NIH grant U01 1U01NS073457-01 and NSF grants 1149882 and 1137211. The authors thank Fei Sha for helpful discussions.

5. Discussion and Future Work
We presented an approach to learn classiﬁers by marginalizing corrupted features (MCF). Speciﬁcally, MCF trains predictors by introducing corruption on the training examples, which is marginalized out in the expectation of the loss function. We minimize the expected loss with respect to the model parameters. Our experimental results show that MCF predictors with blankout and Poisson corruption perform very well in the context of bag-of-words features. MCF with Poisson corruption is particularly interesting for such count features, as it improves classiﬁcation per4 Quadratic and exponential losses perform somewhat worse because they are less appropriate for linear classiﬁers, but even they outperform FDROP for large numbers of feature deletions in the test data.

Learning with Marginalized Corrupted Features

References
Bhattacharyya, C., Pannagadatta, K.S., and Smola, A.J. A second order cone programming formulation for classifying missing data. In Advances in Neural Information Processing Systems, pp. 153–160, 2004. Bishop, C.M. Training with noise is equivalent to tikhonov regularization. Neural Computation, 7(1):108–116, 1995. Blitzer, J., Dredze, M., and Pereira, F. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classiﬁcation. In Association for Computational Linguistics, volume 45, pp. 440, 2007. Br¨ uckner, M., Kanzow, C., and Scheﬀer, T. Static prediction games for adversarial learning problems. Journal of Machine Learning Research, 12:2617–2654, 2012. Burges, C.J.C. and Sch¨ olkopf, B. Improving the accuracy and speed of support vector machines. Advances in Neural Information Processing Systems, 9:375–381, 1997. Chapelle, O., Weston, J., Bottou, L., and Vapnik, V. Vicinal risk minimization. In Advances in Neural Information Processing Systems, pp. 416–422, 2000. Chechik, G., Heitz, G., Elidan, G., Abbeel, P., and Koller, D. Max-margin classiﬁcation of data with absent features. Journal of Machine Learning Research, 9(Jan): 1–21, 2008. Chen, M., Xu, Z., Weinberger, K.Q., and Sha, F. Marginalized denoising autoencoders for domain adaptation. In Proceedings of the International Conference on Machine Learning, pp. 767–774, 2012. Coates, A., Lee, H., and Ng, A.Y. An analysis of singlelayer networks in unsupervised feature learning. In Proceedings of the International Conference on Artiﬁcial Intelligence & Statistics, JMLR W&CP 15, pp. 215–223, 2011. Cover, T. and Hart, P. Nearest neighbor pattern classiﬁcation. IEEE Transactions on Information Theory, 13 (1):21–27, 1967. Dekel, O. and Shamir, O. Learning to classify with missing and corrupted features. In Proceedings of the International Conference on Machine Learning, pp. 216–223, 2008. Duda, R.O., Hart, P.E., and Stork, D.G. Pattern Classiﬁcation. Wiley Interscience Inc., 2001. Globerson, A. and Roweis, S. Nightmare at test time: Robust learning by feature deletion. In Proceedings of the International Conference on Machine Learning, pp. 353– 360, 2006. Glorot, X., Bordes, A., and Bengio, Y. Domain adaptation for large-scale sentiment classiﬁcation: A deep learning approach. In Proceedings of the International Conference on Machine Learning, pp. 513–520, 2011. Herbrich, R. and Graepel, T. Invariant pattern recognition by semideﬁnite programming machines. In Advances in Neural Information Processing Systems, volume 16, pp. 33, 2004.

Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.R. Improving neural networks by preventing co-adaptation of feature detectors, 2012. Krizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Lawrence, N.D. and Sch¨ olkopf, B. Estimating a kernel Fisher discriminant in the presence of label noise. In Proceedings of the International Conference in Machine Learning, pp. 306–313, 2001. LeCun, Y., Denker, J.S., and Solla, S.A. Optimal brain damage. In Advances in Neural Information Processing Systems, pp. 598–605, 1990. Ng, A.Y. Feature selection, l1 vs. l2 regularization, and rotational invariance. In Proceedings of International Conference on Machine Learning, pp. 78–85, 2004. Ranzato, M. and Hinton, G.E. Modeling pixel means and covariances using factorized third-order Boltzmann machines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2551–2558, 2010. Shivaswamy, P.K., Bhattacharyya, C., and Smola, A.J. Second order cone programming approaches for handling missing and uncertain data. Journal of Machine Learning Research, 7:1283–1314, 2006. Sietsma, J. and Dow, R.J.F. Creating artiﬁcial neural networks that generalize. Neural Networks, 4:67–79, 1991. Sutton, C., Sindelar, M., and McCallum, A. Feature bagging: Preventing weight undertraining in structured discriminative learning. Technical Report IR-402, University of Massachusetts, 2005. Teo, C.H., Globerson, A., Roweis, S., and Smola, A. Convex learning with invariances. Advances in Neural Information Processing Systems, 20:1489–1496, 2008. Torralba, A., Fergus, R., and Freeman, W.T. 80 million tiny images: A large dataset for non-parametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958–1970, 2008. Trafalis, T. and Gilbert, R. Robust support vector machines for classiﬁcation and computational issues. Optimization Methods and Software, 22(1):187–198, 2007. Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. Extracting and composing robust features with denoising autoencoders. In Proceedings of the International Conference on Machine Learning, pp. 1096–1103, 2008. Webb, A.R. Functional approximation by feed-forward networks: a least-squares approach to generalization. IEEE Transactions on Neural Networks, 5(3):363–371, 1994. Xu, H., Caramanis, C., and Mannor, S. Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10:1485–1510, 2009. Zhu, J., Rosset, S., Zou, H., and Hastie, T. Multi-class AdaBoost. Technical Report 430, Department of Statistics, University of Michigan, 2006.

