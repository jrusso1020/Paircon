Collaborative Email-Spam Filtering with the Hashing-Trick
Joshua Attenberg
Polytechnic Institute of NYU Five MetroTech Center Brooklyn NY, 11201

Kilian Weinberger, Anirban Dasgupta Alex Smola, Martin Zinkevich
Yahoo! Research 4401 Great America Pkwy. Santa Clara, CA 95054 USA
{kilian, anirban, smola, maz}@yahoo-inc.com dictionary can use up a signiﬁcant portion of a servers’ memory. Furthermore, the dynamic nature of language in both spam and not-spam requires that the dictionary adapt to new words, essentially growing over time. The hashing-trick overcomes this obstacle by rendering the dictionary unnecessary; words are hashed directly to indices. While hash collisions may result in several words being mapped into the same index, and therefore being falsely considered identical, such collisions rarely aﬀect classiﬁcation results. The decision whether an email is spam and not-spam is rarely based on a single word but on a combination of many slightly indicative words. In fact, without a dictionary much more memory is available to store the weights of the classiﬁer, extra space that may be used to improve classiﬁer performance, as demonstrated empirically [7, 2, 8]. In [7], we used the hashing trick to hash multiple classiﬁers into the same space, allowing us to obtain a large number of local classiﬁers with negligible additional computational or memory requirements, and established it empirically with real data. The resulting hybrid classiﬁer overcomes the tough choice between local and global classiﬁcation. In this extension, we also show (using the TREC data set with synthetic labels) how this hybrid solution can handle malicious noise.

josh@cis.poly.edu

ABSTRACT
This paper delves into a recently proposed technique for collaborative spam ﬁltering [7] that facilitates personalization with ﬁnite-sized memory guarantees. In large scale open membership email systems most users do not label enough messages for an individual local classiﬁer to be eﬀective, while the data is too noisy to be used for a global ﬁlter across all users. Our hybrid global/individual classiﬁer is particularly eﬀective at absorbing the inﬂuence of users who label emails very diﬀerently from the general public – because of strange taste or malicious intent. We can accomplish this while still providing suﬃcient classiﬁer quality to users with few labeled instances. Our proposed technique can be used with a variety of classiﬁers and can be implemented in a few lines of code. We verify the eﬃcacy of our proposed technique on a popular web spam benchmark data set.

1.

INTRODUCTION

Collaborative spam ﬁltering in open membership systems, such as Yahoo MailTM , depends on user generated label information. Users provide feedback by labeling emails as spam or not-spam. These labels are then used to train a spam ﬁlter. Although the majority of users provide very little data, as a collective the amount of training data is very large (many millions of emails per day). Unfortunately, there is substantial deviation in users’ understanding of what constitutes spam and non-spam. As a result, spam ﬁltering based on a global classiﬁer will be sub-optimal. Conversely, there is often insuﬃcient personal information to train an individual classiﬁer for all users. In this paper we adopt the hashing-trick [7, 8] to build a personalized global classiﬁer. The hashing-trick maps the global and all personal classiﬁers into a single low-dimensional feature space, in which we train a single weight vector capturing the individual aspects of each user. The hashing-trick is a natural ﬁt for spam ﬁltering. Nearly all commonly used spam classiﬁers, such as Na¨ ıve Bayes, logistic regression and support-vector machines [3] require emails be represented in the bag-of-words format. A major disadvantage of the bag-of-words representation is the necessity of a dictionary data structure for mapping words to vector indices. In collaborative spam ﬁltering, the set of possible word tokens is generally very large. As a result, the
CEAS 2009 - Sixth Conference on Email and Anti-Spam July 16-17, 2009, Mountain View, California USA. A full version of this work is available on the website of the primary author.

2.

HASHING-TRICK

The hashing-trick [7, 8] is a method to scale up linear learning algorithms. The main idea is quite simple. Instead of generating bag-of-word feature vectors through a dictionary that maps tokens to word indices, one uses a hash-function that hashes words directly into a feature vector. The hash function h : {Strings} → [1..m] operates directly on strings and should be approximately uniform1 . In [7] we propose to use a second independent hash function ξ : {Strings} → {−1, 1}, which determines if the particular hashed-dimension of a token should be incremented or decremented. This causes the hashed feature vectors to be unbiased. Algorithm 2 shows a pseudo-code implementation of the hashing-trick that generates a hashed bag-of-words feature vector for an email.

Personalization:
The above hashing-trick can be eﬀectively used to train spam classiﬁers with bounded memory requirements [7]. HowFor the experiments in this paper used the public domain implementation http://burtleburtle.net/bob/hash/doobs.html
1

we from

hashingtrick([string] email) x=0 for word in email do i = h(word) xi = xi + ξ (word) end for return x

ing each token twice: once in it’s original form, and once with the labeler’s user-id prepended to it. See Algorithm 2 for details. Intuitively, this adds individualized tokens for each particular user. Imagine for example that user “barney” does like emails containing the word “viagra”, whereas the majority of users won’t. The personalized hashing trick will learn that “viagra” itself is a spammy word, whereas “viagra barney” is a not-spammy word. personalized hashingtrick(string userid, [string] email) x=0 for word in email do i = h(word) xi = xi + ξ (word) j = h(word ◦ userid) xj = xj + ξ (word ◦ userid) end for return x

ever, even more powerful is the extension allowing the formation of multiple classiﬁers on a data set [7]. In the spamclassiﬁcation setting, this technique provides a handle to tackle the elusive goal of personalization of spam-ﬁlters in the presence of widely uneven amounts of labeled data per user. The main issues in tackling personalization are twofold – the large number of users in a typical email system create an enormous blowup in the number of feature-weights needed to be stored, and the “cold-start” problem, where new or unengaged users have insuﬃcient labeled emails to generate an eﬀective personal spam ﬁlter. In practice, both problems become egregious for large-scale email-providers — they have millions of email users and so the resulting storage increases million-fold when trying to personalize, yet most email users are notoriously lazy in labeling emails as “spam” or “not-spam”, leaving them forever in the “coldstart” phase. Using a single global classiﬁer, on the other hand, is both eﬃcient in terms of the space-complexity and in terms of providing new users with a reasonable spam classiﬁer. While providing eﬃcient solutions to these two issues, a single global classiﬁer is problematic for spam-ﬁltering due to the issue of noisy feedback. It is well known that users diﬀer signiﬁcantly in their view of spam and not-spam, especially in the case of bulk and business emails. Furthermore, it is fairly regular for spammers to inﬁltrate the user base using many diﬀerent accounts so as to be able to provide feedback that is designed to confuse the spam-classiﬁer. As our experiments will show, the global classiﬁer is susceptible to even a small fraction of users giving noisy feedback. We combat the above problems by using the hashing-trick to eﬃciently create a large instance of multitask learning– a setting where multiple related yet slightly diﬀerent concepts are learned simultaneously. For the thousands of users, U , who users who participate in the ﬁlter’s feedback system labeling messages as spam or not-spam, an individualized spam ﬁlter, wu is trained. Additionally, a global classiﬁer (w0 ) is trained from the aggregate labels compiled from all users. This provides a consensus ﬁlter used to overcome the sparsity in training data resulting from the disparity in the amount of labeled content per user. An email can now classiﬁed according to the additive scores of w0 and wu . If the total number of features is d, then storing all the above vectors would need O(d × (|U | + 1)) space. However, as words appear in accordance with Zipf’s law, most users will only encounter a small fraction of the total vocabulary. As a result, the |U | + 1 classiﬁers necessary for a hybrid global/local spam ﬁlter are extremely compressible. We will compress them by hashing all weight vectors into a single weight vector w. In order to this, leverage a major strength of the hashing trick– approximate preservation of orthogonal vectors. This allows us to learn many email ﬁlters simultaneously in a single hashed space. In the context of personalization, we make use of this property by hash-

3.

EXPERIMENTAL SETUP AND RESULTS

To assess the validity of our proposed techniques, we conduct a series of experiments on the freely distributed trec07p data set, a standard benchmark set for spam ﬁlter performance evaluation. This dataset contains 75, 419 labeled and chronologically ordered emails taken from a single email server over four months in 2007 and compiled for TREC spam ﬁltering competitions [1]. E-mails are either spam (or positive) or ham (or negative). We divided the TREC data set temporally, using the ﬁrst 75% of the e-mails as training, and testing on the last 25% of the e-mails. For all our experiments we used the Vowpal Wabbit [5] (VW) implementation of stochastic gradient descent on a square-loss2 . In order to appraise the performance of the hash-trick, we tokenize the bodies of the e-mails and create binary features indicating the presence or absence of each feature in the email body. We then use VW on these tokenized e-mails, using a hashtable of a size between 25 and 212 . For each of these sets of hashed feature vectors, we train and test a single, global classiﬁer, and then analyze the receiver operating characteristic (ROC) curve [6]. We evaluated classiﬁers by choosing the point on the curve where the ham misclassiﬁcation rate (the fraction of ham e-mails classiﬁed as spam e-mails, false positive rate, or HMR) was 1%.3 All ﬁgures presented below display the spam catch rate (the fraction of spam e-mails that were classiﬁed as spam emails, true positive rate, or SCR) when the HMR is 1%. We believe this metric accurately reﬂects a realistic email ﬁlter setting where misclassifying ham messages as spam creates an extremely negative overall system experience. While this metric is highly correlated with the area under the ROC We would like to point out that the hashing-trick is independent of the type of classiﬁer and could be applied to almost all commonly used spam ﬁlters. Although we are only presenting results obtained with the Vowpal Wabbit, similar trends should be observed with other classiﬁers. As hash function we use the public-domain implementation by Jenkins [4] 3 In order to tune the algorithm to get a false positive rate of 1%, it is necessary in practice to reserve a holdout set from the training set.
2

Figure 1: Spam catch rate for TREC07p with the number of hash bins r ranging from 25 to 212 . curve (AUC), the AUC encodes much additional irrelevant information for this particular task. Figure 1 presents the SCR at 1% HMR. Note using 28 hash bins will still result in achieving 80% SCR. This increases to over 90% when using 210 hash bins. With 212 hash bins, the classiﬁer achieves a 99.57% SCR with a 1% HMR, with an area under the ROC curve (AUC) of 0.99829. Given that the full TREC data set has a total of 508, 531 unique terms this means that using a weight vector of 0.8% the size of the full set obtains essentially the same performance as one using all dimensions.

Figure 2: Performance of global and hybrid classiﬁers at varying degrees of maliciousness. We used 220 hash bins in the experiments. who is habitually malicious, we exclude these users from the test set. In this way, we are strictly measuring the malicious users’ inﬂuence on classiﬁer accuracy as seen by legitimate users in various spam ﬁltering scenarios. With p = 0.2, the global classiﬁer with 212 hash bins is reduced to 86% SCR at 1% HMR. With the quantities of spam emails sent daily, this is unacceptable.

3.2

Mitigating Malicious Users by Hashing

3.1

Simulating Malicious Labeling Activity

In order to replicate the inﬂuence of malicious email labelers that abound in a real-world spam ﬁltering setting, we chose to actively modify the label assignments from those provided by human judges in the TREC data set. In order to mimic the eﬀects of this adversarial labeling, a number of “quality” users are initially chosen at random, and their labeled emails are assembled to create a baseline data set. The remaining users are then considered “malicious”. The emails of these malicious users are re-labeled, with labels chosen uniformly at random.4 New data sets can now be created with the inclusion of varying portions, p, of malicious users. For the purpose of this paper, we chose p ∈ {0, 0.1, 0.2, 0.3, 0.4, 0.5}. By varying the number of malicious users, we see trends in spam ﬁlter performance. All modiﬁed data sets used in this project are publicly available at http://cis.poly.edu/~josh/spam/. Label modiﬁcation for our adversarial simulation is performed at the granularity of an individual user — a given user either has their natural labeling assignment or all of their labels are randomly set. By delineating the data set according to users, we simulate the realistic setting where many users label according to their own preference for clean inboxes, while some users are in fact artiﬁcial software constructs, programmed to degrade spam ﬁltering systems such that favored emails will escape detection and reach the remaining benevolent user community. Because it is diﬃcult to deﬁne what constitutes a good label assignment for a user Note that random labels are nastier to deal with than simply ﬂipped labels-if the labels are ﬂipped, then the classiﬁer “just” learns which users has ﬂipped labels, and then ﬂip them back.
4

We note that in our experimental framework, as in most reasonable real-world open email systems, the quality of labeling varies from person to person. By utilizing individual classiﬁers to discriminate spam, consistently malicious or otherwise low-quality labelers can only exert inﬂuence on their local classiﬁer, eﬀecting the labeling of spam which they may see. However, most users label one or two e-mails– too few emails to train an eﬀective individual classiﬁer. In order to achieve the beneﬁts of personalized classiﬁcation while maintaining good general performance for most users, we adopt a hybrid, global + individual classiﬁer. As discussed in Section 2, feature hashing allows simple personalization by projecting multiple classiﬁers onto a single feature space in Rr with little interaction. Figure 2 presents a comparison in classiﬁer performance under varying malicious loads by global and hybrid classiﬁers. On the unmodiﬁed TREC data set, with 0% malicious users, global and hybrid classiﬁers both oﬀer similarly excellent performance. However, as the level of malicious activity rises to 30%, the global classiﬁer degrades more rapidly: with 30% malicious users, the global classiﬁer has 83% accuracy, and the hybrid classiﬁer has 94% accuracy. The purely local classiﬁer performs very badly, mostly because several users have no training data. Note that in this experiment r = 220 was used to cope with the increased information presented by individualized classiﬁers. While this is larger than the experiment presented in Figure 1, if each weight requires storage as a double precision ﬂoating point number, the total space requirement is only 8MB.

3.3

Personalization and Hashing

In Section 2 we have discussed how personalization can be used to mitigate the eﬀect of noisy labels from users. In implementing such a setting, a dimensionality reduction tech-

Since the hybrid classiﬁer has more tokens, the number of hash-collisions is also correspondingly larger. With 212 hash bins, for instance, the average number of collisions per hash bucket is over 60, 000. Correspondingly, the hybrid classiﬁer does worse than the global one for 212 hash bins. However, the situation quickly is quickly remedied by oﬀering increased storage size; with 218 hash bins, the hybrid classiﬁer achieves approximately 90% SCR, beating the 80% SCR achieved by the global-only classiﬁer. Further increases in the number of hash bins does not improve the classiﬁcation performance signiﬁcantly. (a) Global Classiﬁer

4.

CONCLUSION

This work demonstrates the hashing-trick as an eﬀective method for collaborative spam ﬁltering. It allows spam ﬁltering without the necessity of a memory-consuming dictionary and strictly bounds the overall memory required by the classiﬁer. Further, the hashing-trick allows the compression of many (hundreds of thousands of) classiﬁers into a single ﬁnite-sized weight vector. This allows us to run personalized and global classiﬁcation together with very little additional computational overhead. We provide strong empirical evidence that the resulting classiﬁer is more robust against noise and absorbs individual preferences that are common in the context of open-membership spam classiﬁcation. (b) Hybrid Classiﬁer Figure 3: Inﬂuence of r on global and hybrid classiﬁer performance with malicious users nique such as the hashing-trick becomes critical– without such a technique, even for the trec07 dataset, with around 5000 users and over 500,000 tokens, the total number of dimensions needed would be over 2.5 billion. However, the hashing-trick comes with a compromise of its own – one must ensure that the number of hash bins is suﬃciently large, and the hash-function is suﬃciently random so as not to avoid a lot of collisions. Since the classiﬁers are now trained on the hash-space, a collision of a spam-indicative feature with one that indicates ham, would be detrimental to the performance. In [7] we show that under certain assumptions on the hashing function and the number of hash bins, we can prove theoretical bounds on the distortion that is caused by the hashing. The number of hash bins that we need in practice is much less than our theoretical bounds. In this section we vary both the number of hash bins and the fraction of malicious users. Again we consider the SCR when the HMR is bounded by 1% – Figure 3 shows our ﬁndings. Figure 3(a) presents the spam catch rate for a global spam ﬁlter for diﬀerent numbers of hash bins, the three colored lines indicate three diﬀerent levels of malicious activity: the fraction p of malicious users being {0%, 20%, 40%}. Figure 3(b) represents the same experiment only with the combined hybrid classiﬁer. Focusing on the case of p = 0%, we note that, as before, both classiﬁers do very well. The hybrid classiﬁer actually does slightly worse than the global classiﬁer with r = 212 , this is likely due to the vastly increased number of tokens being mapped into each hash bin in the hybrid ﬁlter. This barrage of collisions may result in conﬂicting signals, and therefore lowered accuracy. With malicious users, both global and hybrid classiﬁers require more hash bins to achieve near-optimum performance.

5.

REFERENCES

[1] G. Cormack. TREC 2007 spam track overview. In The Sixteenth Text REtrieval Conference (TREC 2007) Proceedings, 2007. [2] K. Ganchev and M. Dredze. Small statistical models by random feature mixing. In Workshop on Mobile Language Processing, Annual Meeting of the Association for Computational Linguistics, 2008. [3] T. Hastie, R. Tibshirani, J. Friedman, T. Hastie, J. Friedman, and R. Tibshirani. The elements of statistical learning. Springer New York, 2001. [4] R. Jenkins. Algorithm alley. Dr. Dobb’s Journal, September 1997. Source code available at http://burtleburtle.net/bob/hash/doobs.html. [5] J. Langford, L. Li, and A. Strehl. Vowpal wabbit online learning project. Technical report, http://hunch.net/?p=309, 2007. [6] K. Spackman. Signal detection theory: Valuable tools for evaluating inductive learning. In Proceedings of the sixth international workshop on Machine learning table of contents, pages 160–163. Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 1989. [7] K. Weinberger, A. Dasgupta, J. Attenberg, J. Langford, and A. Smola. Feature hashing for large scale multitask learning. In ICML, 2009. [8] W. S. Yerazunis. Sparse binary polynomial hashing and the CRM114 Discriminator. In MIT Spam Conference, 2003.

