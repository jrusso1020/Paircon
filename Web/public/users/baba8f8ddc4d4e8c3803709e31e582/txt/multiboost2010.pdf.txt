Multi-Task Learning for Boosting with Application to Web Search Ranking
Olivier Chapelle chap@yahoo-inc.com
Washington University Saint Louis, MO Yahoo! Labs Sunnyvale, CA

Kilian Weinberger kilian@wustl.edu

pks2103@cs.columbia.edu Ya Zhang
Shanghai Jiao Tong University Shanghai, China

Dept of Computer Science Columbia University, NY

Pannagadatta Shivaswamy

Srinivas Vadrevu svadrevu@yahooinc.com
Yahoo! Labs Sunnyvale, CA Yahoo! Labs Sunnyvale, CA

Belle Tseng

yazhang@sjtu.edu.cn

belle@yahoo-inc.com

ABSTRACT
In this paper we propose a novel algorithm for multi-task learning with boosted decision trees. We learn several different learning tasks with a joint model, explicitly addressing the speciﬁcs of each learning task with task-speciﬁc parameters and the commonalities between them through shared parameters. This enables implicit data sharing and regularization. We evaluate our learning method on web-search ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from diﬀerent countries vary largely in size because of the cost of editorial judgments. Our experiments validate that learning various tasks jointly can lead to signiﬁcant improvements in performance with surprising reliability.

Categories and Subject Descriptors
I.2.6 [Artiﬁcial intelligence]: Learning; H.3.3 [Information storage and retrieval]: Information search and retrieval

General Terms
Algorithms

1.

INTRODUCTION

Multi-task learning algorithms [2] aim to improve the performance of several learning tasks through shared models. Previous work focussed primarily on neural networks, knearest neighbors [2] and support vector machines [6]. In this paper, we introduce a novel multi-task learning algorithm for gradient boosting. This is motivated by our interest in web search ranking: gradient boosted decision trees are indeed among the state-of-the-art algorithms for largescale web-search ranking [11, 19].

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. KDD’10, July 25–28, 2010, Washington, DC, USA. Copyright 2010 ACM 978-1-4503-0055-1/10/07 ...$10.00.

Web search ranking is often treated as a supervised machine learning problem [12]: each query-document pair is represented by a high-dimensional feature vector and its label indicates the document’s degree of relevance to the query. Like many other supervised learning problems, machine learned ranking requires a large number of labeled training examples, which are time consuming and expensive to obtain. This problem becomes more acute with specialization: most major search engines oﬀer indeed specialized rankings for diﬀerent countries or regions. The problem of high editorial cost is prominent if one attempts to build many such specialized, country-speciﬁc ranking functions – as building each ranking function requires its own set of hand-labeled data. On the other hand, a large fraction of queries are region-insensitive. Thus, it seems worthwhile to treat the diﬀerent countries as tasks that are not completely independent of one another as they share some commonalities, yet, diﬀer enough that one cannot na¨ ıvely combine their training data sets. A common related line of research is domain adaptation (DA). Here, one assumes a source domain with large training data and a target domain with very little training data. The test case is exclusively in the target domain. The main principle behind DA is to learn a model for the source and adapt it to the target domain. In the web-search example the source could be a well established country and the target a new country where the search engine is still relatively new. Gao et al. [8] address this particular case with boosted decision trees through model interpolation and reﬁnement. Related to our work, but fundamentally diﬀerent, is multitask boosting for face veriﬁcation [17]. It learns a set of boosted classiﬁers and is based on a probabilistic model where a multinomial variable indicates how much each boosted classiﬁer contributes to each task. The learning algorithm involves Expectation-Maximization (EM) to learn both the multinomial random variables as well as the classiﬁers. Dai et al. [5] developed a variation of AdaBoost [15] that can incorporate training data from a diﬀerent distribution than the test set. Instead of learning multiple models, their approach down-weights data points that are not representable for the test set. A diﬀerent method of cross domain learning is discussed in [3], which uses both feature level and instance level knowledge transfer across domains. The feature

1189

level knowledge transfer is modeled as an optimization function in RankingSVM [9] by learning the underlying common feature representation across domains. The instance level knowledge transfer is achieved by sample weighting. Compared to all these approaches, in this paper, we propose a novel algorithm to capture task speciﬁcs and commonalities simultaneously. Given data from T diﬀerent tasks, the idea is to learn T + 1 models – one for each speciﬁc task and one global model that captures the commonalities amongst them. The algorithm is derived systematically based on the connection between boosting and ℓ1 regularization [14]. To the best of our knowledge we are not aware of any work that jointly models several tasks by explicitly learning both the commonalities and idiosyncrasies through gradient boosted regression. Our contribution in this paper is three-fold: 1. We introduce a novel multi-task learning algorithm based on gradient boosted decision trees - the ﬁrst of its kind. 2. We exploit the connections between boosting and ℓ1 regularization to motivate the algorithm. 3. Driven by the success of gradient boosted decision trees on web-search, we perform a detailed evaluation on web-scale datasets. Although we mainly focus on multi-task learning for ranking across diﬀerent countries, it is important to point out that our algorithm can readily be used in other scenarios. In the context of web-search one might encounter diﬀerent multi-task problems, such as customizing ranking functions for diﬀerent query types by modeling both the commonalities across the query types and the idiosyncrasies of each query set. Further, our algorithm is not speciﬁc to web search ranking and is equally applicable to any standard machine learning task such as classiﬁcation or regression. The rest of the paper is organized as follows. We formally introduce the multi-task learning problem in section 2 and propose our approach in section 3. The connection between boosting and ℓ1 regularization serves as motivation for our derivations. In sections 4, 5 and 6, we evaluate our method on several real-world large scale web-search ranking data sets. Section 7 presents the conclusions and future directions for our work.

We also overload the deﬁnition of C t to allow it to be deﬁned as a function of the parameters of the function to be learned. For instance, in case of a linear class of functions, C t (w) := C t (. . . , w, xi , . . . )i∈I t . (2)

Previous work.
Previous work has mainly focused on Neural Networks [2, 4] or Support Vector Machines [6]. This latter work is of particular interest because it inspired the algorithm presented in this paper. In this SVM based multi-task learning, a classiﬁer wt is speciﬁcally dedicated for the task t. In addition, there is a global classiﬁer w0 that captures what is common among all the tasks. The joint optimization problem is then to minimize the following cost: min
T X t=0

w0 ,w1 ,...,wT

λt w t

2 2

+

T X t=1

C t (w 0 + w t )

(3)

In [6], all the λi , i ≥ 1 have the same value, but λ0 can be diﬀerent. Also, a classiﬁcation task is considered: the labels are yi ∈ {±1} and the loss function is X C t (w ) = max(0, 1 − yi w, xi ). (4)
i∈I t

Note that the relative value between λ0 and the other λi controls the strength of the connection between the tasks. In the extreme case, if λ0 → +∞, then w0 = 0 and all tasks are decoupled; on the other hand, when λ0 is small and λi → +∞, ∀i ≥ 1, we obtain wi = 0 and all the tasks share the same decision function with weights w0 .

Kernel-trick.
In practice, the formulation (3) suﬀers from the draw-back that it only allows linear decision boundaries. This can be very limiting especially for more complex real-world problems. A standard method to avoid this limitation is to apply the kernel-trick [16] and map the input vectors indirectly into a high dimensional feature space, xi → φ(xi ), where, with careful choice of φ, the data is often linearly separable. The kernel-trick is particularly powerful, because the mapping φ is explicitly chosen such that the inner-product between two vectors φ(xi )⊤ φ(xj ) can be pre-computed very eﬃciently – even if the dimensionality of φ(xi ) is very high or inﬁnite. The kernel-trick has been adapted to multi-task learning by [6]. Unfortunately, for many real-world problems, the quadratic time and space complexity on the number of input vectors is often prohibitive.

2.

BACKGROUND

Notation and setup.
Assume that we are given learning tasks t ∈ {1, 2, . . . , T }. Further, the data for these tasks, {(x1 , y1 ), . . . , (xn , yn )}, is also given to us. Each task t is associated with a set of indices I t that denotes the data for this particular task. These index sets form a partition of {1, . . . , n} (i.e., I t ∩ t I s = ∅ when t = s and ∪T t=1 I = {1, . . . , n}). We also deﬁne I 0 = {1, . . . , n}. At this point, we assume that all the tasks share the same feature space; we will later show how this assumption can be relaxed. Finally, we suppose that we are given a cost function C t deﬁned as a function of the predicted values for all points in I t . For instance, in regression setting, we might consider squared loss: X C t (. . . , ui , . . . )i∈I t := (yi − ui )2 . (1)
i∈I t

Hashing-trick.
In certain domains, such as text classiﬁcation, it can be the case that the data is indeed linearly separable – even without the application of the kernel-trick. However, the input space X is often already so high dimensional, that the dense weight vectors wt become too large for learning to be feasible – especially when the number of tasks T becomes very large. Recently, the authors of [18] applied the hashingtrick to a non-regularized variation of (3) and mapped the input data of all tasks into a single lower dimensional feature space. Similar to the kernel-trick, the high-dimensional representation is never computed explicitly and all learning happens in the compact representation.

1190

3.

MULTI-BOOST

into a single vector β ∈ RJ (T +1) , deﬁned as β = [β 0 , . . . , β T ]⊤ , C (β ) :=
⊤ ⊤ T X t=1

In this paper we focus on the case where the data is too large to apply the kernel-trick and not linearly separable, which is a key assumption for the hashing-trick. As already noted in the introduction, boosted decision trees are very well suited for our web search ranking problem and we now present our algorithm, multi-boost for multi-task learning with boosting.

C t (β 0 + β t ).

(7)

This reduces (6) to the much simpler optimization problem
β λ ≤µ

min C (β ),

(8)

3.1 Boosting-trick
Instead of mapping the input features into a high dimensional feature space with cleverly chosen kernel functions, we propose to use a set of non-linear functions H = {h1 , . . . , hJ } to deﬁne φ : X → RJ as φ(xi ) = [h1 (xi ), . . . , hJ (xi )]⊤ . Instead of assuming that we can compute inner-products eﬃciently (as in the kernel-trick), we assume that we are provided with an oracle O that solves the least-squared regression problem eﬃciently up to ǫ accuracy: X O({(xi , zi )}) ≈ argmin (h(xi ) − zi )2 , (5)
h∈H i

P t where we deﬁne the norm β λ = T t=0 λt β 1 . The goal in this section is to ﬁnd an algorithm that solves (8) without ever computing any vector φ(xi ) explicitly. ǫ-boosting. As a ﬁrst step, let us deﬁne a simple iterative algorithm to solve (8) that [14] refer to as ǫ − boosting . Intuitively, the idea is to follow the regularization path as µ is slowly increased from 0 to the desired value in tiny ǫ > 0 increments. This is possible under the assumption that the optimal vector β in (8) is a monotonic function of µ componentwise. At each iteration, the vector β is updated only incrementally by an additive factor of ∆β , with ∆β λ ≤ ǫ. More precisely, ∆β is found through the following optimization problem: min C (β + ∆β )
∆β

for some targets zi . For the sake of the analysis we assume that |H| = J is ﬁnite, but in practice, we used regression trees and H is inﬁnite. Even though J may be very large, it is possible to learn linear combinations of functions in H using the so-called boosting trick. Viewing boosting as a coordinate descent optimization in a large space is of course not new and was ﬁrst pointed out in [13]. The contribution of this paper is the adaptation of this insight to multi-task learning. Let us apply the boosting-trick to the optimization problem (3). For disambiguation purposes, we denote the weight vector for task t in RJ as β t . As J can be very large, we can only store vectors β ∈ RJ if they are extremely sparse. For this reason, we change the regularization in (3) from an ℓ2 norm to an ℓ1 -norm. We can state our modiﬁed multi-task learning formulation as min
T X t=1

s.t.

∆β

λ

≤ǫ

(9)

Following [14], it can be shown that, under the monotonicity assumption stated above, solving (9) for the right number of iterations does in fact solve (7). Therefore, ǫboosting satisﬁes the ℓ1 regularization constraint from (6) implicitly. Because the ℓ1 -norm of the vector β increases by at most ǫ during each iteration, the regularization is not controlled by the upper bound µ but instead by the number of iterations S for which this ǫ update is performed.

Multitask ǫ-boosting.
As we are only moving in very tiny ǫ steps, it is fair to approximate C (·) with the ﬁrst-order Taylor expansion. By “unstacking” the representation from (7), this leads to C ( β + ∆β ) ≈ C ( β ) +
t with gj := T X ˙ t=0

β 0 ,β 1 ,...,β T

C t (β 0 + β t )

s.t.

T X t=0

λt β t

1

≤ µ, (6)

where, as in (2), C t (β ) is deﬁned as C t (. . . , β, φ(xi ) , · · · )i∈It . A minor technical diﬀerence from (3) is that the regularizer is introduced as a constraint. We do not make any explicit assumptions on the loss functions C t (·), except that it needs to be diﬀerentiable. Similar to the use of the kernel-trick, our new feature representation φ(xi ) forces us to deal with the problem that in most cases the feature space RJ will be extremely high dimensional. For example, for our experiments in the result section, we set H to be the set of regression trees [1] – here |H| is inﬁnite and φ(xi ) cannot be explicitly computed. To the rescue comes the fact that we will never actually have to compute φ(xi ) explicitly and that the weight vector β t can be made suﬃciently sparse with the ℓ1 -regularization in (6).

¸ ∆β t , g t ,

(10)

Using the chain-rule,

∂C . t ∂βj Let us deﬁne the outputs at the training points as ˙ ¸ ˙ ¸ ui = β 0 , φ(xi ) + β t , φ(xi ) for i ∈ I t , t > 0.
t gj = n X ∂C (u) ∂ui . t ∂ui ∂βj i=1

(11)

On the other hand, ∂ui = t ∂βj  hj (xi ) 0 if i ∈ It otherwise.

3.2 Boosting and ℓ1 Regularization
In this section we will derive an algorithm to solve (6) eﬃciently. In particular we will follow previous literature by [14] and ensure that our solver is in fact an instance of gradient boosting [13]. To simplify notation, let us ﬁrst transform the multi-task optimization (6) into a traditional single-task optimization problem by stacking all parameters

Combining the above equations, we ﬁnally obtain:
t gj =

i∈I

X ∂C (u) hj (xi ). ∂ui t

(12)

1191

We can now rewrite our optimization problem (9) with the linear approximation (10) as: min
∆β t T X ˙ t=0

∆β , g

t

t

¸

s.t.

T X t=0

β2 β0 β1

β3

λ t ∆β

t

1

≤ ǫ.

(13)

t t ˜j With a change of variables β ← λt βj , the problem becomes one of minimizing an inner product under ℓ1 constraint. If we make the additional assumption that the class of functions H is closed under negation (h ∈ H ⇒ −h ∈ H) then it is easy to show that the solution of (13) is given by: 8 t gj < ǫ if (t, j ) = argmin t ∆βj = (14) λt (t,j ) : 0 otherwise.

β4

Intuitively, (14) ﬁnds the direction with steepest descent, across all tasks and all functions in H, and takes an ǫ-step. It remains to show that we can compute the single nonzero index of (14) eﬃciently with the help of the oracle (5). Assuming that the weighted functions h ∈ H are normalized P 2 1 over the input, i.e. i∈I t h(xi ) = 1 , we can express (5) ∂C (u) with zi = − ∂ui as, X t argmin (hj (xi ) − zi )2
j i∈I t

Figure 1: A layout of four ranking tasks that are learned jointly. The four countries symbolize the diﬀerent ranking functions that need to be learned, where β 1 , . . . , β 4 are the parameter vectors that store the speciﬁcs of each individual task. The various tasks interact through the joint model, symbolized as a globe with parameter vector β 0 . intersection of the various feature sets. That does not change the algorithm fundamentally; the˙ main diﬀerence ¸ ˙ is that now ¸ ui in equation (11) is deﬁned as β 0 , φ0 (xi ) + β t , φt (xi ) , where φ0 and φt are deﬁned with respect to the functions of H0 and Ht respectively.

= argmin
j

i∈I t

X

−hj (xi )zi

t = argmin gj . j

Second order information.
Instead of performing an ǫ-gradient step, we follow the framework of [19] and perform an approximate Newton step at each iteration. At the core of this approach is the com2 C , and putation of the second derivatives of the loss, ti = ∂ ∂u2 i the use of a weighted least square algorithm as an oracle with weights ti .

:=ˆ  (t ) ˆ,  ˆ)) with The optimal couple (t, j ) from (14) is thus (t ˆ(t X ˆ = argmax 1 t h (15) ˆ(t) (xi )zi . λt t t
i∈T

The parametrization in terms of β is just and ˙ conceptual ¸ in practice we update the function F t (·) := β t , φ(·) instead of β : F t (·) ← F t (·) + ǫh ˆ) (·). ˆ(t
ˆ ˆ

Weights.

(16) is 1. in of

We ntroduce a weight ct for eachP task t such that the new global objective function is C := ct C t . We experiment with two choices for ct : ct = 1 and ct = |I1t | . Algorithm 1 Multi-boost (S iterations) F t = 0 ∀0 ≤ t ≤ T for s ← 1 to S do (u ) zi = − ∂C ∀1 ≤ i ≤ n ∂ui X t ˆ h ← argmin (h(xi ) − zi )2 , 0 ≤ t ≤ T.
h∈H i∈I t

The algorithm multi-boost with the update-rule (16) summarized in Algorithm 1 and is illustrated in Figure The computational complexity of this algorithm is linear the number of training samples as well as in the number tasks.

3.3 Generalizations
For the sake of simplicity, we have tried to keep the above derivation as simple as possible, but there are in fact several extensions that we have implemented:

X t ˆ (x i )z i . ˆ ← argmax 1 t h λt t t
i∈T ˆ ˆ ˆ ˆt F t ← F t + ǫh ˆ ˆt ui ← ui + ǫh (x i ) ∀ i ∈ I t end for Predict a new example x of task t as F 0 (x) + F t (x)

Different feature sets.
The training points from diﬀerent tasks may have diﬀerent features, and in fact, that is the case in our web search ranking application. To address this issue, we introduce, for each task t, a set of functions Ht deﬁned over the features for that task. H0 is the set of functions deﬁned over the There is a ﬂaw in our reasoning since in general this equation cannot hold for all t simultaneously. However, we will later present an extension where the functions h depend on t. That will solve this problem.
1

4. EXPERIMENTAL SETUP
In this section we present the experimental setup for our experiments. The data sets include large-scale web ranking training sets for various countries. All the data sets contain randomly sampled queries from the search engine query logs.

1192

Country A B C D E F G H I J K L M N O

Train 72k 64k 74k 108k 162k 74k 57k 137k 95k 166k 62k 307k 474k 194k 401k

Examples Valid 7k 10k 4k 12k 14k 11k 5k 11k 12k 12k 10k – – 16k –

Test 11k – 11k 14k 11k 11k 15k 12k 12k 11k 20k – – 12k –

Train 3486 4286 5992 7027 7204 7295 7356 7644 8153 11145 11301 12850 15666 18331 33680

Queries Valid 477 563 298 383 586 486 238 807 835 586 548 – – 541 –

Test 600 – 600 600 600 600 600 600 600 600 600 – – 600 –

Table 1: Details of the subset of data used in experiments. The countries have been sorted in increasing size of the number of training queries.

11 most important features such as a static rank for the page on the web graph, a text match feature and the output of a spam classiﬁer. Also we did not use in this section the validation and test sets described in Table 1. Instead we split the given training set into a smaller training set, a validation set and a test set. The proportions of that split are, in average over all countries, 70%, 15% and 15% for respectively the training, validation and test sets. The reason for this construction will become clearer in the next section; in particular, the test sets from Table 1 were constructed by judging the documents that our candidates functions retrieved and cannot thus be used for experimentation but only as a ﬁnal test. We initially discuss the correlation between train MSE and test DCG. Later, we compare several baseline ranking models and discuss the eﬀect of sample weighting. For each experiment, we calculated the DCG on both validation set and the test set after every iteration of boosting. All parameters – the number of iterations, number of nodes in regression trees and the step size ǫ – were selected to maximize the DCG on the validation set and we report the corresponding test DCG. Train MSE and Test DCG: We show how the train MSE and test DCG change for a typical run of the experiment in Figure 2. The training loss always decreases with more iterations. The test DCG improves in the beginning and but the model starts overﬁtting at some point, and the DCG slighlty deteriorates after that. Thus it is important to have a validation set to pick the right number of iterations as we have done. In the rest of the experiments in this section, we tuned the model parameters with the validation set and report the improvements over the test.
10 9.8
Test DCG

For each query, a set of URLs is sampled from the results retrieved by several search engines. For each query-url pair, an editorial grade containing 0 − 4 is obtained that describes the relevance of the url to the query. Each query-url pair is represented with several hundred features. The size in terms of number of queries and the number of query-url pairs in the training, test and validation sets for each country is shown in Table 1. The country names are anonymized for conﬁdentiality purposes. Note that there are some empty cells for some countries which indicate that the test and validation sets are not available for them. The performance of various ranking models is measured on the test set using Discounted Cumulative Gain [10] at ﬁfth position, which we refer to as DCG-5. We experimented with two diﬀerent types of loss functions: the squared loss as in GBDT [7] and a pairwise preference loss as in RankSVM [9] and GBRank [19]: if xi is to be preferred to xj , the corresponding loss for that pair is max(0, 1 − (f (xi ) − f (xj )))2 . We thus refer to GBDT and GBRank as our learning methods for ranking both with and without multi-task learning. More background information on learning to rank can be found in a recent survey paper [12]. The experimental results have been divided into two sections. In the next section we present preliminary results on a small subset of the complete feature set. For the largescale experiments in Section 6, we present the results with the complete feature set containing more than 500 features such as text match scores, spam scores, link based features, click features and page classiﬁer outputs. In most of the experiments, the parameters of the algorithms are tuned using a validation set. But for some of them – that we will point out – some parameters are set to default values which in general give good performances. These values are 20 for the number of nodes per tree, 1200 for the number of trees and 0.05 for the shrinkage rate [7].

9.6 9.4 9.2 9

1.1

Training MSE

1.04

1.01

1 0

100

200

300 400 Iterations

500

600

700

Figure 2: Train MSE and test DCG as a function of the number of iterations. Baseline Experiments: We ﬁrst did a smaller experiment on six countries. The aim in this experiment was to compare with the following baseline methods: • independent: Each country trained on its own data;

5.

PRELIMINARY EXPERIMENTS

In this section we present preliminary experimental results on a subset of data sets with a smaller feature set containing

• cold-start: Model trained using all the countries other than the local itself. The aim of this baseline was to see how much other countries could help a given country;

1193

Country A C D E M N

weighted 0.561 1.135 -0.043 0.222 -2.385 -0.036

unweighted 1.444 1.295 -0.233 0.342 -0.029 0.705

pooling -0.320 0.972 -1.096 -2.873 -1.724 -1.160

cold-start -0.282 1.252 -2.378 -3.624 -6.376 -3.123

Table 2: Percentage change in DCG over independent ranking models for various baseline ranking models.

Country A B C D E F H L M N O

% gain +4.21 +2.06 +1.70 +2.95 +0.35 +1.43 +1.11 +0.57 +0.45 +1.00 +0.61

Best Countries CDFHLMN N AM CHLN BCFLO ABEHLN ABDEFL ABCEFM ACN AFL AF

• pooling: All the countries are used in training. We ensured that the total weight on the local country was equal to that of all the other countries put together. Table 2 summarizes the results relative to the independent baseline. The two heuristic schemes – cold-start and pooling – did not show any improvement overall (in fact, most DCG values were lower). Hence in all of the experiments that follow, we used the independent ranking model as the baseline and show that our multi-task learning algorithm can improve over the independent models as well. As described in Section 3.3, we can provide a weight for each data set in the multi-task learning scenario that we proposed. In this table the unweighted scheme refers to setting the weight as 1 for each example and weighted refers to weighting each data set by the inverse number of samples in the data set. Thus weighted gives equal weight to each data set, while unweighted has no weight on each sample, so eﬀectively larger data sets have higher weight in the unweighted setting. The results indicate that the average performance of the unweighted scheme seems better than the weighted one. Note that a relative improvement of 1% is considered to be substantial in the web search ranking domain.2 Steps taken by the two weighting schemes: Typical behavior of the steps taken with the two weighting schemes are shown in Figure 3. In both schemes, initially, a number of global steps are taken. Since global steps minimize the objective function for every country, it is attractive initially. However, once the commonality among the tasks has been captured by the global steps, they are no longer very attractive. The algorithm takes many local steps from that point onwards. Furthermore, with the unweighted scheme, the countries with signiﬁcantly more data dominate the objective. Thus, the multi-task algorithm takes signiﬁcantly more steps in such countries. On the other hand, in the weighted scheme, smaller countries are relatively easier to ﬁt than bigger ones and a lot of steps are taken in these countries. Although we presented two extreme weighting schemes, other weighting schemes with speciﬁc weights to each country are possible. Finding appropriate groups of countries: Finding an appropriate grouping of countries that is beneﬁcial to each country so that the tasks in that group can help each other is a nontrivial task. Since we wanted to ﬁnd out the best group of countries that is most beneﬁcial to each country, What we mean here is that improvements reported in web search ranking papers are typically of the order of 1%; see for instance Figure 1 of [19].
2

Table 3: Percentage improvement over independent for the best countries found on the validation set.

we searched all possible combinations of countries. Specifically we explored 211 combinations of possible groupings for eleven countries and found the best group of countries that helps a given country based on the validation data set. Then we tested this best model on the test set to observe its performance. Since this experiment involves a very large number of combinations, we have ﬁxed to some default values the learning parameters of the gradient boosted decision tree algorithm (number of nodes, shrinkage and number of trees). Table 3 shows the experimental results for this task. Each row shows the DCG-5 gain of the best grouped multitask ranking model over the independent ranking model for each country. We can see that the multi-task ranking model improves the performance in every single country over the country-speciﬁc ranking model.

6. LARGE SCALE EXPERIMENTS
In this section we present the experimental results by testing the multi-task algorithms on the large scale web search ranking data sets with complete feature sets. We illustrate that our methods help to customize the ranking models on a number of country-speciﬁc data sets. For Sections 6.1, 6.2 and 6.3, we ﬁxed the parameters such as number of trees, number of nodes per tree and compare multi-task ranking models with independent models on the validation set. In Section 6.4 and 6.5, we did complete model selection on the validation set and report the results on the test set. Note that we have two diﬀerent experimental setting in this paper. Most of the experiments are in a reranking setting, where a ﬁxed set of documents with relevance grades are exposed to the ranking models for each query. This is the traditional setting used in almost all of the learning to rank papers. On the other hand, in Sections 6.4 and 6.5, we present results based on web-scale experimental setting, where all the documents3 in the web index are exposed to the ranking models. To our knowledge, we are the ﬁrst to provide results in such a setting. It also serves as a further validation to the results obtained with the reranking experimental setting.

3 To be precise, it would be infeasible to score all the documents in the index and only the potentially relevant documents – as determined by a basic ranking function – are scored.

1194

unweighted Market A Market B Market C Market E Market J Market K Global 300 250
Steps taken

weighted Market A Market B Market C Market E Market J Market K Global

250 200
Steps taken

200 150 100 50 0

150 100 50 0

200

400

600 800 1000 Number of iterations

1200

1400

200

400 600 800 Number of iterations

1000

1200

Figure 3: Steps taken by the multi-task algorithm with the two weighting schemes. The country labels A-K are sorted by increasing data set sizes. The unweighted (left) version takes more steps for the larger data sets, whereas the weighted (right) variation focusses more on countries with less data. Country A B C D E F G H I J K N Mean Multi-GBDT
Unweighted Weighted

Multi-GBRank
Unweighted Weighted

+1.53 +1.81 +0.92 +4.14 -1.37 +0.57 +4.34 +0.34 -0.50 +0.10 +2.37 +0.53 +1.23

+0.72 +1.58 +0.52 +3.62 -1.45 +1.80 +4.68 +0.96 -0.80 -0.69 +2.38 -1.23 +1.01

+0.69 +2.22 +0.92 +1.77 -0.18 +1.67 +1.74 +0.52 -0.07 +0.74 +3.40 +0.51 +1.16

+0.75 +1.64 +0.01 +1.84 -0.91 +2.22 +0.75 +0.85 +0.32 -0.64 +2.01 -0.92 +0.66

For each country, DCG-5 gain compared to the independent ranking model for each country is shown for all the groups in which it was involved. The underlying learning algorithm in this experiment was GBDT and we used the unweighted scheme of our multi-task learning algorithm. We organized the countries into eight groups based on continents and language of the countries and we anonymized the group names. Each of these groups involve a set of countries which are indicated in the columns of Table 5. It can be noted that diﬀerent groups are beneﬁcial to each country and ﬁnding an appropriate grouping that is beneﬁcial to a speciﬁc country is challenging. The negative numbers for some countries indicate that none of the groups we selected were improving that particular country and the independent ranking model is still better than various groupings we tried.

6.3 Comparison over adaption models
Table 4: DCG-5 gains with Multi-GBDT and MultiGBRank learning algorithms in two diﬀerent weighting settings. The gains are over independent-GBDT and independent-GBRank respectively. In this section we present comparison results of our multiboost method with the domain adaptation method described in [8]. In this domain adaptation setting, the key idea is to utilize the source data to help and adapt to the target domain. We chose the source data as the simple combination of data from all of the countries and varied the target domain. In addition, the adaptation method also requires an additional parameter in terms of the number of trees that are added at the end of the base model to train with the target domain data. We ﬁxed the number of base model trees to be 1000 and the number of additive trees as 200. Figure 4 shows the DCG-5 gains of our method over the adaptation method in [8]. The multi-task method outperforms the adaptation method in most of the countries, while the adaptation method is better in a couple of countries. Since the multi-task method allows the domains to learn from each other, it fosters better interaction among the domains than the adaptation method. Also a key diﬀerence with the adaption method is that multi-task automatically decides the number of steps taken towards each of the domains by choosing the domain that minimizes the objective function at each step. Moreover, since the steps are interleaved among the domains, the target data is introduced earlier to the model than the adaptation method.

6.1 Effect of the loss function
As indicated earlier, our method can be applied to a any loss function and in particular it can be combined with a pairwise ranking model (see Section 4). In this section, we compare the results of pointwise and pairwise ranking schemes, which we refer to as Multi-GBDT and Multi-GBRank methods, to the independent ranking models in each country. Table 4 shows the results of both learning algorithms with the two weighting schemes discussed in Section 5. It can be seen that in both pointwise and pairwise ranking schemes, multitask ranking models have better average performance over the independent models.

6.2 Grouping Related Tasks
An important aspect of the multi-task learning algorithms that we proposed is that if we can group the tasks so that they are related and beneﬁt each other, we can boost the performance of the individual tasks. To demonstrate the beneﬁts of grouping the related tasks, we grouped the related countries into several groups and present results in Table 5.

1195

Country A B C D E F G H I J K L N O

Group 1 +0.33 +1.87 +1.81 +2.90 -1.02 +1.82 +1.51 +0.00 -0.85 +0.76

Group 2 +1.53 +1.81 +0.92 +4.14 +0.57

Group 3

Group 4 +1.60

Group 5

Group 6 +0.46 +0.64

Group 7

Group 8

+2.29 -1.37 -0.20 +4.34 +3.59 +0.08 -0.97 +0.10 +2.37 † +0.53 +0.38 +1.98 † † -0.09 † -1.24

+0.34 -0.50

†

Table 5: Improvement of multi-task models with various groupings of countries over independent ranking models. Each column in the table indicates a group that includes a subset of countries and each row in the table corresponds to a single country. The numbers in the cell are ﬁlled only when the corresponding country is part of the corresponding group. The symbol † indicates that this country was included for training but has not been tested.

3 2.5
% improvedment (DCG5)

2 1.5 1 0.5 0 −0.5 −1 A C D E F G H Country I J K N

Country A C D E F G H I J K N

Bestvalid +2.99 +2.31 -1.03 -0.02 +2.12 +4.80 +3.27 +0.10 +4.04 +6.85 +2.11

Besttest +3.02 +3.73 +0.27 +0.07 +3.27 +4.80 +3.27 +1.38 +4.20 +9.39 +2.11

Figure 4: DCG-5 gains of multi-task models over the adaption models. The dashed line represents the mean gain, 0.65%.

Table 6: Web scale results obtained by judging all the urls retrieved by the top 3 multi-task models as well as the independent model. Bestvalid refers to DCG-5 gain with the best model on the validation set while Besttest refers to the highest DCG-5 gain on the test set.

6.4 Web Scale Experiments
The experimental results with web-scale experimental setting are shown in Table 6. To perform this test, several multi-task models were ﬁrst trained with various parameters including the grouping and weighting as some of the parameters in addition to the standard GBDT parameters such as number of trees, number of nodes per tree and the shrinkage rate [7]. Of these models, we picked the model with the highest DCG-5 on the validation set as the Bestvalid model. We also selected the top three models on the validation set as well as the independent model to be evaluated in the web-scale experimental setting. This means that all the top documents retrieved by these models were sent for editorial judgements. Of these three selected models, the one achieving the highest DCG-5 on the test set is denoted Besttest . Table 6 shows the improvements with both Bestvalid and Besttest models.

The results indicate that the small tasks have much to beneﬁt from the other big tasks where the training data size is large. It can also be noticed that the diﬀerence between reranking and web scale results is also dependent on the size of the validation or reranking data set. When the size of the validation set is large, there is more conﬁdence on the results from the reranking results.

6.5 Experiments with Global Models
As discussed in Section 3, a byproduct of our multi-task learning algorithm with multiple countries is a global model, F 0 that is learned with data from all the countries. This global model can be utilized to deploy to the countries where there is no editorial data available at all, which could serve as a good generic model. Table 7 shows the results of the global models in the same web-scale setting as in the previous section. For each country we present the DCG-5 gains

1196

Country A C D E F G H I J K N mean

Improvements with F 0 +2.94 -0.20 -0.17 -0.33 +0.83 +0.49 +1.18 +0.73 +4.83 +0.85 -1.48 + 0.88

Table 7: DCG-5 gains of global models trained with multi-task approach compared with simple data combination from of all countries.

of the multi-task global models over that baseline ranking model F 0 . A key diﬀerence between these two models is that the multi-task global model primarily learns the commonalities in the countries while simple data combination model could learn both commonalities and the country speciﬁc idiosyncrasies. While these country speciﬁc idiosyncrasies are helpful for the that speciﬁc country, it might actually hurt other countries. Although the global model does not perform well in a few countries, the average performance of the multi-task global ranking model is better than the simple data combination model.

7.

CONCLUSIONS

In this paper we introduced a novel multi-task learning algorithm based on gradient boosted decision trees that is speciﬁcally designed with web search ranking in mind. We model the problem of learning the ranking model for various countries in a multi-task learning framework. The customization of the ranking models to each country happens naturally by modeling both the characteristics of the local countries and the commonalities separately. We provided a thorough evaluation of multi-task web search ranking on large scale real world data. Our multi-task learning method lead to reliable improvements in DCG, especially after speciﬁcally selecting sub-sets that are learned jointly. The results in this paper validate that multi-task learning has a natural application in web-search ranking. As future work, we want to apply this multi-task approach to other ranking adaptation problems where the tasks could involve various query types such as navigational and informational queries. Our proposed multi-task learning framework could learn a ranking model for all of these query types jointly while still learning a generic global learning model that can work across all types of queries. Furthermore, we could apply our algorithm to other machine learning tasks beyond ranking for which boosted decision trees are well suited.

8.

REFERENCES

[1] L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classiﬁcation and regression trees. Chapman & Hall/CRC, 1984.

[2] R. Caruana. Multitask learning. In Machine Learning, pages 41–75, 1997. [3] D. Chen, Y. Xiong, J. Yan, G.-R. Xue, G. Wang, and Z. Chen. Knowledge transfer for cross domain learning to rank. Information Retrieval, 2009. [4] R. Collobert and J. Weston. A uniﬁed architecture for NLP: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM New York, NY, USA, 2008. [5] W. Dai, Q. Yang, G. Xue, and Y. Yu. Boosting for transfer learning. In Proceedings of the 24th international conference on Machine learning, pages 193–200. ACM, 2007. [6] T. Evgeniou and M. Pontil. Regularized multi–task learning. In KDD, pages 109–117, 2004. [7] J. Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29:1189–1232, 2001. [8] J. Gao, Q. Wu, C. Burges, K. Svore, Y. Su, N. Khan, S. Shah, and H. Zhou. Model adaptation via model interpolation and boosting for web search ranking. In EMNLP, pages 505–513, 2009. [9] R. Herbrich, T. Graepel, and K. Obermayer. Large margin rank boundaries for ordinal regression, pages 115–132. MIT Press, Cambridge, MA, 2000. [10] K. Jarvelin and J. Kekalainen. IR evaluation methods for retrieving highly relevant documents. In SIGIR, pages 41–48. New York: ACM, 2002. [11] P. Li, C. J. C. Burges, and Q. Wu. Mcrank: Learning to rank using multiple classiﬁcation and gradient boosting. In NIPS, 2007. [12] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225–231, 2009. [13] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent in function space. In Neural information processing systems, volume 12, pages 512–518, 2000. [14] S. Rosset, J. Zhu, T. Hastie, and R. Schapire. Boosting as a regularized path to a maximum margin classiﬁer. Journal of Machine Learning Research, 5:941–973, 2004. [15] R. Schapire, Y. Freund, P. Bartlett, and W. Lee. Boosting the margin: A new explanation for the eﬀectiveness of voting methods. The annals of statistics, 26(5):1651–1686, 1998. [16] B. Sch¨ olkopf and A. Smola. Learning with kernels. MIT press Cambridge, MA, 2002. [17] X. Wang, C. Zhang, and Z. Zhang. Boosted multi-task learning for face veriﬁcation with applications to web image and video search. In Proceedings of IEEE Computer Society Conference on Computer Vision and Patter Recognition, 2009. [18] K. Weinberger, A. Dasgupta, J. Attenberg, J. Langford, and A. Smola. Feature hashing for large scale multitask learning. In ICML, 2009. [19] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen, and G. Sun. A general boosting method and its application to learning ranking functions for web search. In NIPS, 2007.

1197

