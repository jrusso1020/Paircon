Feature-Cost Sensitive Learning with Submodular Trees of Classifiers
Matt J. Kusner, Wenlin Chen, Quan Zhou, Zhixiang (Eddie) Xu, Kilian Q. Weinberger, Yixin Chen
Washington University in St. Louis, 1 Brookings Drive, MO 63130
Tsinghua University, Beijing 100084, China
{mkusner, wenlinchen, zhixiang.xu, kilian, ychen25}@wustl.edu
zhouq10@mails.tsinghua.edu.cn

Abstract
During the past decade, machine learning algorithms have become commonplace in large-scale real-world industrial applications. In these settings, the computation time to train and
test machine learning algorithms is a key consideration. At
training-time the algorithms must scale to very large data set
sizes. At testing-time, the cost of feature extraction can dominate the CPU runtime. Recently, a promising method was proposed to account for the feature extraction cost at testing time,
called Cost-sensitive Tree of Classifiers (CSTC). Although
the CSTC problem is NP-hard, the authors suggest an approximation through a mixed-norm relaxation across many classifiers. This relaxation is slow to train and requires involved
optimization hyperparameter tuning. We propose a different
relaxation using approximate submodularity, called Approximately Submodular Tree of Classifiers (ASTC). ASTC is
much simpler to implement, yields equivalent results but requires no optimization hyperparameter tuning and is up to
two orders of magnitude faster to train.

Introduction
Machine learning is rapidly moving into real-world industrial settings with widespread impact. Already, it has
touched ad placement (Bottou et al., 2013), web-search
ranking (Zheng et al., 2008), large-scale threat detection
(Zhang et al., 2013), and email spam classification (Weinberger et al., 2009). This shift, however, often introduces resource constraints on machine learning algorithms. Specifically, Google reported an average of 5.9 billion searches
executed per day in 2013. Running a machine learning algorithm is impractical unless it can generate predictions in
tens of milliseconds. Additionally, a large scale corporation
may want to limit the amount of carbon consumed through
electricity use. Taking into account the needs of the learning
setting is a step machine learning must make if it is to be
widely adopted outside of academic settings.
In this paper we focus on the resource of test-time CPU
cost. This test-time cost is the total cost to evaluate the classifier and to extract features. In industrial settings, where
linear models are prevalent, the feature extraction cost dominates the test-time CPU cost. Typically, this cost must be
strictly within budget in expectation. For example, an email
Copyright c 2014, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

spam classifier may have 10 milliseconds per email, but can
spend more time on difficult cases as long as it can be faster
on easier ones. A key insight to facilitate this flexibility is to
extract different features for different test instances.
Recently, a model for the feature-cost efficient learning
setting was introduced that demonstrates impressive testtime cost reductions for web search (Xu et al., 2014). This
model, called Cost-Sensitive Tree of Classifiers (CSTC), partitions test inputs into subsets using a tree structure. Each
node in the tree is a sparse linear classifier that decides
what subset a test input should fall into, by passing these
inputs along a path through the tree. Once an input reaches
a leaf node it is classified by a specialized leaf classifier. By
traversing the tree along different paths, different features
are extracted for different test inputs. While CSTC demonstrates impressive real-world performance, the training time
is non-trivial and the optimization procedure involves several sensitive hyperparameters.
Here, we present a simplification of CSTC, that takes advantage of approximate submodularity to train a near optimal tree of classifiers. We call our model Approximately
Submodular Tree of Classifiers (ASTC). We make a number
of novel contributions: 1. We show how the CSTC optimization can be formulated as an approximately submodular set
function optimization problem, which can be solved greedily. 2. We reduce the training complexity of each classifier
within the tree to scale linearly with the training set size. 3.
We demonstrate on several real-world datasets that ASTC
matches CSTC’s cost/accuracy trade-offs, yet it is significantly quicker to implement, requires no additional hyperparameters for the optimization and is up to two orders of
magnitude faster to train.

Resource-efficient learning
In this section we formalize the general setting of resourceefficient learning and introduce our notation. We are given a
training dataset with inputs and class labels {(xi , yi )}ni=1 =
(X, y) ∈ R(n,d) × Y n (in this paper we limit our analysis to
the regression case). Without loss of generality we assume
for each feature vector xf ∈ Rn that kxα k2 = 1, for all α =
1, . . . , d, and that kyk2 = 1, all with zero mean. Our aim is
to learn a linear classifier hβ (x) = x> β that generalizes well
to unseen data and uses the available resources as effectively
as possible. We assume that each feature α incurs a cost of

c(α) during extraction. Additionally, let `(·) be a convex loss
function and let B be a budget on its feature cost. Formally,
we want to solve the following optimization problem,
X
min `(y; X, β) subject to
c(α) ≤ B, (1)
β

α:|β|>0

β1 , θ1
β0 , θ0
x

v

v1

x� β 0 > θ 0
0

x� β 1 ≤ θ 1

P

where
α:|β|>0 c(α) sums over the (used) features with
non-zero weight in β.

Cost-sensitive tree of classifiers (CSTC)

Combinatorial optimization. There are two road-blocks
to learning the CSTC classifiers β k . First, because instances
traverse different paths through the tree, the optimization is
a complex combinatorial problem. In Xu et al. (2014), they
fix this by probabilistic tree traversal. Specifically, each classifier x> β k is trained using all instances, weighted by the
probability that instances reach v k . This probability is derived by squashing node predictions using the sigmoid function: σ(x> β) = 1/(1 + exp(−x> β)).
To make the optimization more amenable to gradientbased methods, Xu et al. (2014) convert the constrained optimization problem in eq. (1) to the Lagrange equivalent and
minimize the expected classifier loss plus the expected feature cost, where the expectation is taken over the probability
of an input reaching node v k ,
n

1X k
k 2
k
k
min
pi (yi − x>
i β ) +ρkβ k1 + λ E[C(β )]. (2)
| {z }
βk n
i=1
exp. feature cost
|
{z
}
exp. squared loss

Here pki is the probability that instance xi traverses to v k and
ρ is the regularization constant to control overfitting. The last
term is the expected feature cost of β k ,
"
#
X
X
X
k
l
j
E[C(β )] =
p
c(α)
|βα |
. (3)
v l ∈P k

α

v j ∈π l

0
k

The outer-most sum is over all leaf nodes P affected by
β k and pl is the probability of any input reaching such a
leaf node v l . The remaining terms describe the cost incurred
for an input traversing to that leaf node. CSTC makes the
assumption that, once a feature is extracted for an instance
it is free for future requests. Therefore, CSTC sums over all
features α and if any classifier along the path to leaf node
v l uses feature α, it is paid for exactly once (π l is the set of
nodes on the path to v l ).

β3

v4

x� β 4

v5

β5

v6

β6

v2
β2 , θ2

Recent work in resource-budgeted learning (Xu et al., 2014)
(CSTC) shows impressive results by learning multiple classifiers from eq. (1) which are arranged in a tree (depth
D
D) β 1 , . . . , β 2 −1 . The CSTC model is shown in figure 1
(throughout the paper we consider the linear classifier version of CSTC). Each node v k is a classifier whose predictions x> β k for an input x are thresholded by θk . The threshold decides whether to send x to the upper or lower child of
v k . An input continues through the tree in this way until arriving at a leaf node, which predicts its label.

v3

Figure 1: The CSTC tree (depth 3). Instances x are sent
along a path through the tree (e.g., in red) based on the predictions of node classifiers β k . If predictions are above a
threshold θk , x is sent to an upper child node, otherwise it is
sent to a lower child. The leaf nodes predict the class of x.
`0 norm and differentiability. The second optimization
road-block is that this feature cost term is non-continuous,
and is thus hard to optimize. Their solution is to derive a
continuous relaxation of the `0 norm using the mixed-norm
(Kowalski, 2009). The final optimization is non-covex and
not differentiable and the authors present a variational approach, introducing auxiliary variables for both `0 and `1
norms so that the optimization can be solved with cyclic
block-coordinate descent.
There are a number of practical difficulties that arise when
using CSTC for a given dataset. First, optimizing a nonleaf node in the CSTC tree affects all descendant nodes via
the instance probabilities. This slows the optimization and
is difficult to implement. Second, the optimization is sensitive to gradient learning rates and convergence thresholds,
which require careful tuning. In the same vein, selecting
appropriate ranges for hyperparameters λ and ρ may take
repeated trial runs. Third, because CSTC needs to reoptimize all classifier nodes the training time is non-trivial for
large datasets, making hyperparameter tuning on a validation set time-consuming. Additionally, the stationary point
reached by block coordinate descent is initialization dependent. These difficulties may serve as significant barriers to
entry, potentially preventing practitioners from using CSTC.

Model
In this paper we propose a vastly simplified variant of the
CSTC classifier, called Approximately Submodular Tree of
Classifiers (ASTC). Instead of relaxing the expected cost
term into a continuous function, we reformulate the entire
optimization as an approximately submodular set function
optimization problem.
ASTC nodes. We begin by considering an individual classifier β k in the CSTC tree, optimized using eq. (2). If we
ignore the effect of β k on descendant leaf nodes P k and
previous nodes on its path π k , the feature cost changes:
X
E[C(β k )] =
c(α)kβαk k0 .
(4)
α

This combined with the loss term is simply a weighted
classifier with cost-weighted `0 -regularization. We propose
to greedily select features based on their performance/cost
trade-off and to build the tree of classifiers top-down, starting from the root. We will solve one node at a time and set
features ‘free’ that are used by parent nodes (as they need
not be extracted twice). Figure 2 shows a schematic of the
difference between the optimization of ASTC and the reoptimization of CSTC.

CSTC optimization

Resource-constrained submodular optimization. An alternative way to look at the optimization of a single CSTC
node is as an optimization over sets of features. Let Ω be
the set of all features. Define the loss function for node v k ,
`k (A), over a set of features A ⊆ Ω as such,

v6

n
1X k
`k (A) = min
pi (yi − δA (xi )> β k )2
βk n
i=1

(5)

where we treat probabilities pki as indicator weights: pki = 1
if input xi is sent to v k , and to 0 otherwise. Define δA (x) as
an element-wise feature indicator function that returns feature xa if a ∈ A and 0 otherwise. Thus, `k (·) is the squared
loss of the optimal model using only (a) inputs that reach
v k and (b) the features in set A. Our goal is to select a set
of features A that have low cost, and simultaneously have a
low optimal loss `k (A).
Certain problems in constrained set function optimization
have very nice properties. Particularly, a class of set functions, called submodular set functions, have been shown to
admit simple near-optimal greedy algorithms (Nemhauser,
Wolsey, and Fisher, 1978). For the resource-constrained
case, each feature (set element) a has a certain resource cost
c(a), and we would like to ensure that the cost of selected
features fall under some resource budget B. For a submodular function s that is non-decreasing and non-negative the
resource-constrained set function optimization,
X
max s(A) subject to
c(a) ≤ B
(6)
A⊆Ω

a∈A

can be solved near-optimally by greedily selecting set elements a ∈ Ω that maximize s as such,
#
"
s(Gj−1 ∪ a) − s(Gj−1 )
.
(7)
gj = argmax
c(a)
a∈Ω
Where we define the greedy ordering Gj−1
=
(g1 , g2 , . . . , gj−1 ). To find gj we evaluate all remaining set elements a ∈ Ω \ Gj−1 and pick the element gj = â
for which s(Gj−1 ∪ a) increases the most over s(Gj−1 )
per cost. Let GhT i = (g1 , . . . , gm ) be the largest feasible
Pm
greedy set, having total cost T (i.e., j=1 c(gj ) = T ≤ B
and T + c(gm+1 ) > B). Streeter and Golovin (2007)
prove that for any non-decreasing and non-negative submodular function s and some budget B, eq. (7) gives an
approximation ratio of (1 − e−1 ) ≈ 0.63 with respect to
∗
the optimal set with cost T ≤ B. Call this set ChT
i . Then,
−1
∗
s(GhT i ) ≥ (1 − e )s(ChT i ) for the optimization in eq. (6).

ASTC optimization

v3
v

v1

1

v4
v0

2

2

(β , θ )
v

v

v0

5

v2

2

θ2

β 2 = (X� X)−1 X� y

Figure 2: The optimization schemes of CSTC and ASTC.
Left: When optimizing the classifier and threshold of node
v 2 , (β 2 , θ2 ) in CSTC, it affects all of the descendant nodes
(highlighted in blue). If the depth of the tree is large (i.e.,
larger than 3), this results in a complex and expensive gradient computation. Right: ASTC on the other hand optimizes each node greedily using the familiar ordinary least
squares closed form solution (shown above). θ2 is set by binary search to send half of the inputs to each child node.

Optimization
In this section we demonstrate that optimizing a single
CSTC node, and hence the CSTC tree, greedily is approximately submodular. We begin by introducing a modification to the set function (5). We then connect this to the approximation ratio of the greedy cost-aware algorithm eq. (7),
demonstrating that it produces near-optimal solutions. The
resulting optimization is very simple to implement and is
described in Algorithm 1.
Approximate submodularity. To make `k (·) amenable to
resource-constrained set function optimization (6) we convert the loss minimization problem into an equivalent label
‘fit’ maximization problem. Define the set function zk ,
zk (A) =

Var(y; pk ) − `k (A)
Var(y; pk )

(8)

P
where Var(y; pk ) = i pki (yi − ȳ)2 is the variance of the
training label vector y multiplied by 0/1 probabilities pki
(ȳ is the mean predictor). It is straightforward to show that
maximizing zk (·) is equivalent to minimizing `k (·). In fact,
the following approximation guarantees hold for zk (·) constructed from a wide range of loss functions (via a modification of Grubb and Bagnell (2012)). As we are interested
in developing a new method for CSTC training, we focus
purely on the squared loss. Note that zk (·) is always nonnegative (as the mean predictor is a worse training set predictor than the OLS solution using one feature, assuming it
takes on more than one value). To see that it is also nondecreasing note that zk (·) is precisely the squared multiple correlation R2 (Diekhoff, 1992), (Johnson and Wichern,
2002), which is known to be non-decreasing.
If the features are orthogonal then zk (·) is submodular
(Krause and Cevher, 2010). However, if this is not the case

it can be shown that zk (·) is approximately submodular and
has a submodularity ratio, defined as such:
Definition 2.1 (Submodularity Ratio) (Das and Kempe,
2011). Any non-negative set function z(·) has a submodularity ratio γU,i for a set U ⊆ Ω and i ≥ 1,
i
h
i
Xh
z(L ∪ {s}) − z(L) ≥ γU,i z(L ∪ S) − z(L) ,
s∈S

for all L ⊂ U , and S such that |S| ≤ i and S ∩ L = ∅.
The submodularity ratio ranges from 0 (z(·) is not submodular) to 1 (z(·) is submodular) and measures how close
a function z(·) is to being submodular. As is done in Grubb
and Bagnell (2012), for simplicity we will only consider the
smallest submodularity ratio γ ≤ γU,i for all U and i.
The submodularity ratio in general is non-trivial to compute. However, we can take advantage of work by Das and
Kempe (2011) who show that the submodularity ratio of
zk (·), defined in eq. (8), is further bounded. Define CkA as
the covariance matrix of X̃, where x̃i = pki δA (xi ) (inputs weighted by the probability of reaching v k , using only
the features in A). Das and Kempe (2011) show in Lemma
2.4 that for zk (·), it holds that γ ≥ λmin (CkA ), where
λmin (CkA ) is the minimum eigenvalue of CkA .
Approximation ratio. As in the submodular case, we
can optimize zk (·) subject to the resource constraint that
the cost of selected features must total less than a resource
budget B. This optimization can be done greedily using the
rule described in eq. (7). The following theorem—which
is proved for any non-decreasing, non-negative, approximately submodular set function in Grubb and Bagnell
(2012)—gives an approximation ratio for this greedy rule.
Theorem 2.2 (Grubb and Bagnell, 2012). The greedy algorithm selects an ordering G such that,
∗
zk (GhT i ) > (1 − e−γ )zk (ChT
i)

where GhT i = (g1 , g2 , . . . , gm ) is the greedy sequence
Pm
truncated at cost T , such that i=1 c(gi ) = T ≤ B and
∗
ChT
i is the set of optimal features having cost T .
Thus, the approximation ratio depends directly on the submodularity ratio of zk (·). For each node in the CSTC tree we
greedily select features using the rule described in (7). If we
are not at the root node, we set the cost of features used by
the parent of v k to 0, and select them immediately (as we
have already paid their cost). We fix a new-feature budget
B—identical for each node in the tree—and then greedily
select new features up to cost B for each node. By setting
probabilities pki to 0 or 1 depending on if xi traverses to v k ,
learning each node is like solving a unique approximately
submodular optimization problem, using only the inputs sent
to that node. Finally, we set node thresholds θk to send half
of the training inputs to each child node.
We call our approach Approximately Submodular Tree of
Classifiers (ASTC), which is shown in Algorithm 1. The optimization is much simpler than CSTC.

Algorithm 1 ASTC in pseudo-code.
1: Inputs: {X, y}; tree depth D; node budget B, costs c
2: Set the initial costs c1 = c
3: for k = 1 to 2D − 1 nodes do
4:
Gk = ∅
P
5:
while budget not exceeded: g∈Gk c(g) ≤ B do
6:
Select feature a ∈ Ω via eq. (7)
7:
Gk = Gk ∪ {a}
8:
end while
9:
Solve β k using weighted ordinary least squares
10:
if v k is not a leaf node, with children v l and v u then
11:
Set child probabilities:


1 if pki > θk l
1 if pki ≤ θk
pui =
pi =
0 otherwise
0 otherwise
12:
Set new feature costs: cu = cl = ck
13:
Free used features: cu (Gk ) = cl (Gk ) = 0
14:
end if
15: end for
D
16: Return {β 1 , β 2 , . . . β 2 −1 }

Fast greedy selection
Equation (7) requires solving an ordinary least squares problem, eq. (5), when selecting the feature that improves zk (·)
the most. This
 requires a matrix inversion which typically
takes O d3 time. However, because we only consider selecting one feature at a time we can avoid the inversion for
zk (·) altogether using the QR decomposition. Let Gj =
(g1 , g2 , . . . , gj ) be our current set of greedily-selected features. For simplicity let XGj = δGj (X), the data masked so
that only features in Gj are non-zero. Computing zk (·) requires computing the weighted squared loss, eq. (5), which,
after the
q requires no inverse. Redefine
q QR decomposition
pk
pk
i
i
xi =
n xi and yi =
n yi , then we have,
`k (Gj ) = min(y − XGj β k )> (y − XGj β k ).
βk

(9)

Let XGj = QR be the QR decomposition of XGj . Plugging
in this decomposition, taking the gradient of `k (Gj ) with
respect to β k , and solving at 0 yields (Hastie, Tibshirani,
and Friedman, 2001),
β k = R−1 Q> y
The squared loss for the optimal β k is,
`k (Gj ) = (y − QRR−1 Q> y)> (y − QRR−1 Q> y)
= (y − QQ> y)> (y − QQ> y)
= y> y − y> QQ> y.

(10)

Imagine we have extracted j features and we are considering
selecting a new feature a. The immediate approach would be
to recompute Q including this feature and then recompute
the squared loss (10). However, computing qj+1 (the column corresponding to feature a) can be done incrementally

Complexity. Computing the ordinary least squares solution the naive way for the (j +1)th feature: (X> X)−1 X> y
requires O n(j + 1)2 + (j + 1)3 for the covariance multiplication and inversion. This must be done d − j times to
compute zk (·) for every remaining feature. Using
 the QR
decomposition, computing qj+1
requires
O
nj
time and

computing eq. (11) takes O n time. As before, this must
be done d − j times for all remaining features, but as mentioned above both steps can be done in parallel.

Results
In this section, we evaluate our approach on a real-world
feature-cost sensitive ranking dataset: the Yahoo! Learning to Rank Challenge dataset. We begin by describing the
dataset and show Precision@5 per cost compared against
CSTC (Xu et al., 2014) and another cost-sensitive baseline.
We then present results on a diverse set of non-cost sensitive
datasets, demonstrating the flexibility of our approach. For
all datasets we evaluate the training times of our approach
compared to CSTC for varying tree budgets.
Yahoo! learning to rank. To judge how well our approach
performs in a particular real-world setting, we test ASTC
on the binary Yahoo! Learning to Rank Challenge data set
(Chen et al., 2012). The dataset consists of 473, 134 web
documents and 19, 944 queries. Each input xi is a querydocument pair containing 519 features, each with extraction
costs in the set {1, 5, 20, 50, 100, 150, 200}. The unit of cost
is in weak-learner evaluations (i.e., the most expensive feature takes time equivalent to 200 weak-learner evaluations).
We remove the mean and normalize the features by their `2

0.3
0.29

test error

0.115

0.28

0.11

0.27

0.105

cost-weighted l1-classifier r

0.26

CSTC

0.1

ASTC

0.25

ASTC, soft
0.24

0.09
0

500

1000

1500

CIFAR

0.44

0.23
0

10

20

30

40

50

40

50

MiniBooNE

0.24
0.22

0.42

0.2
0.4

test error

test error

zk (Gj ∪ a) − zk (Gj )
−`k (Gj ∪ a) + `k (Gj )
=
c(a)
Var(y; pk )c(a)
>
>
y> Q1:j+1 Q>
1:j+1 y − y QQ y
=
Var(y; pk )c(a)
>
(qj+1 y)2
(11)
=
Var(y; pk )c(a)


where Q1:j+1 = Q, qj+1 . The first two equalities follow
from the definitions of zk (·) and `k (·). The third equality
follows because Q and Q1:j+1 are orthogonal matrices.
We can compute all of the possible qj+1 columns, corresponding to all of the remaining features a in parallel, call
this matrix Qremain . Then we can compute eq. (11) vectorwise on Qremain and select the feature with the largest corresponding value of zk (·).

0.31

0.095

where q1 = Xg1 /kXg1 k2 (recall g1 is the first greedilyselected feature). Finally, in order to select the best next feature using eq. (7), for each feature a we must compute,

Forest

0.32

0.12

(higher is better)

Xa − QQ> Xa
=
kXa − QQ> Xa k2

Yahoo!

0.13
0.125

Precision @ 5

using the Gram–Schmidt process:
Pj
Xa − m=1 (X>
a qm )qm
qj+1 =
Pj
>
kXa − m=1 (Xa qm )qm k2

0.18

0.38

0.16

0.36

0.14
0.34

0.32
0

0.12

50

100

150

feature cost

200

250

0.1
0

10

20

30

feature cost

Figure 3: Plot of ASTC, CSTC, and a cost-sensitive baseline on on real-world feature-cost sensitive dataset (Yahoo!)
and three non-cost sensitive datasets (Forest, CIFAR, MiniBooNE). ASTC demonstrates roughly the same error/cost
trade-off as CSTC, sometime improving upon CSTC. For
Yahoo! circles mark the CSTC points that are used for training time comparison, otherwise, all points are compared.
norm, as is assumed by the submodularity ratio bound analysis. We use the Precision@5 metric, which is often used for
binary ranking datasets.
Figure 3 compares the test Precision@5 of CSTC with
the greedy algorithm described in Algorithm 1 (ASTC). For
both algorithms we set a maximum tree depth of 5. We also
compare against setting the probabilities pki using the sigmoid function σ(x> β k ) = 1/(1 + exp(−x> β k )) on the
node predictions as is done by CSTC (ASTC, soft). Specifically, the probability of an input x traversing from parent
node v k to its upper child v u is σ(x> β k − θk ) and to its
lower child v l is 1 − σ(x> β k − θk ). Thus, the probability of
x reaching node v k from the root is the product of all such
parent-child probabilities from the root to v k . Unlike CSTC,
we disregard the effect β k has on descendant node probabilities (see Figure 2). Finally, we also compare against a single
cost-weighted `1 -regularized classifier.
We note that the ASTC methods perform just as good, and
sometimes slightly better, than state-of-the-art CSTC. All of
the techniques perform better than the single `1 classifier, as
it must extract features that perform well for all instances.
CSTC and ASTC instead may select a small number of expert features to classify small subsets of test inputs.
Forest, CIFAR, MiniBooNE. We evaluate ASTC on three
very different non-cost sensitive datasets in tree type and image classification (Forest, CIFAR), as well as particle identification (MiniBooNE). As the feature extraction costs are
unknown we set the cost of each feature α to c(α) = 1. As
before, ASTC is able to improve upon CSTC.
Training time speed-up. Tables 1 and 2 show the speedup of our approaches over CSTC for various tree budgets.

Table 1: Training speed-up of ASTC over CSTC for different tree budgets on Yahoo! and Forest datasets.
YAHOO !
F OREST
C OST B UDGETS
10
52
86
169
468
800
1495
3
5
8
13
23
ASTC
119 X 52 X 41 X 21 X 15 X 9.2 X 6.6 X
8.4 X 7.0 X 6.3 X 4.9 X 3.1 X
ASTC, SOFT
121 X 48 X 46 X 18 X 15 X 8.2 X 6.4 X
8.0 X 6.4 X 5.7 X 4.5 X 2.8 X

C OST B UDGETS
ASTC
ASTC, SOFT

Table 2: Training speed-up of ASTC over CSTC for CIFAR and MiniBooNE datasets.
CIFAR
M INI B OO NE
9
24
76
180
239
4
5
12
14
18
5.6 X 2.3 X 0.68 X 0.25 X 0.14 X
7.4 X 7.9 X 5.5 X 5.2 X 4.1 X
5.3 X 2.3 X 0.62 X 0.27 X 0.13 X
7.2 X 6.2 X 5.9 X 4.2 X 4.3 X

For a fair speed comparison, we first learn a CSTC tree for
different values of λ, which controls the allowed feature extraction cost (the timed settings on the Yahoo! dataset are
marked with black circles on Figure 3, whereas all points are
timed for the other datasets). We then determine the cost of
unique features extracted at each node in the learned CSTC
tree. We set these unique feature costs as individual node
budgets B k for ASTC methods and greedily learn tree features until reaching the budget for each node. We note that
on the real-world feature-cost sensitive dataset Yahoo! the
ASTC methods are consistently faster than CSTC. Of the
remaining datasets ASTC is faster in all settings except for
three parameter settings on CIFAR. One possible explanation for the reduced speed-ups is that the training set of these
datasets are much smaller (Forest: n = 36, 603 d = 54; CIFAR: n = 19, 761 d = 400; MiniBooNE: n = 45, 523 d = 50)
than Yahoo! (n = 141, 397 and d = 519). Thus, the speed-ups
are not as pronounced and the small, higher dimensionality
CIFAR dataset trains slightly slower than CSTC.

Related work
Prior to CSTC (Xu et al., 2014), a natural approach to controlling feature resource cost is to use `1 -regularization to
obtain a sparse set of features (Efron et al., 2004). One
downside of these approaches is that certain inputs may only
require a small number of cheap features to compute, while
other inputs may require a number of expensive features.
This scenario motivated the development of CSTC (Xu et
al., 2014). There are a number of models that use similar
decision-making schemes to speed-up test-time classification. This includes Dredze, Gevaryahu, and Elias-Bachrach
(2007) who build feature-cost sensitive decision trees, BusaFekete et al. (2012) who use a Markov decision process to
adaptively select features for each instance, and Wang and
Saligrama (2012) who learn subset-specific classifiers.
Feature selection has been tackled by a number of submodular optimization papers (Krause and Cevher, 2010; Das
and Kempe, 2011; Das, Dasgupta, and Kumar, 2012; Krause
and Guestrin, 2012). Surprisingly, until recently, there were
relatively few papers addressing resource-efficient learning.
Grubb and Bagnell (2012) greedily learn weak learners that
are cost-effective using (orthogonal) matching pursuit. Work
last year (Zolghadr et al., 2013) considers an online setting
in which a learner can purchase features in ‘rounds’. Most
similar to our work is Golovin and Krause (2011) who learn

33
3.1 X
2.5 X

50
1.4 X
1.5 X

47
2.0 X
1.7 X

a policy to adaptively select features to optimize a set function. Differently, their work assumes the set function is fully
submodular and every policy action only selects a single element (feature). To our knowledge, this work is the first treebased model to tackle resource-efficient learning using approximate submodularity.

Conclusion
We have introduced Approximately Submodular Tree of
Classifiers (ASTC), using recent developments in approximate submodular optimization to develop a practical nearoptimal greedy method for feature-cost sensitive learning.
The resulting optimization yields an efficient update scheme
for training ASTC up to 120 times faster than CSTC.
One limitation of this approach is that the approximation
guarantee does not hold if features are preprocessed. Specifically, for web-search ranking, it is common to first perform
gradient boosting to generate a set of limited-depth decision
trees. The predictions of these decision trees can then be
used as features, as in non-linear of CSTC (Xu et al., 2013).
Additionally, a set of features may cost less than the sum
of their individual costs. One common example is image descriptors for object detection such as HOG features (Dalal
and Triggs, 2005). A descriptor can be thought of as a group
of features. Once a single feature from the group is selected,
the remaining features in the group become ‘free’, as they
were already computed for the descriptor. Extending ASTC
to these features would widen the scope of the approach.
Overall, by presenting a simple, near-optimal method for
feature-cost sensitive learning we hope to bridge the gap
between machine learning models designed for real-world
industrial settings and those implemented in such settings.
Without the need for expert tuning and with faster training
we believe our approach can be rapidly used in an increasing
number of large-scale machine learning applications.
Acknowledgements. KQW, MK, and ZX are supported
by NIH grant U01 1U01NS073457-01 and NSF grants
1149882 and 1137211. YC and WC are supported by NSF
grants CNS-1017701, CCF-1215302, and IIS-1343896. QZ
is partially supported by the Key Technologies Program
of China grant 2012BAF01B03 and by the NSFC grant
61273233. Computations were performed via the Washington University Center for High Performance Computing,
partially provided by grant NCRR 1S10RR022984-01A1.

References
Bottou, L.; Peters, J.; Quiñonero-Candela, J.; Charles, D. X.;
Chickering, D. M.; Portugaly, E.; Ray, D.; Simard, P.; and Snelson, E. 2013. Counterfactual reasoning and learning systems:
the example of computational advertising. The Journal of Machine Learning Research 14(1):3207–3260.

Wang, J., and Saligrama, V. 2012. Local supervised learning
through space partitioning. In Advances in Neural Information
Processing Systems, 91–99.
Weinberger, K.; Dasgupta, A.; Langford, J.; Smola, A.; and Attenberg, J. 2009. Feature hashing for large scale multitask learning.
In International Conference on Machine Learning, 1113–1120.

Busa-Fekete, R.; Benbouzid, D.; Kégl, B.; et al. 2012. Fast classification using sparse decision dags. In International Conference
on Machine Learning.

Xu, Z.; Kusner, M. J.; Chen, M.; and Weinberger, K. Q. 2013.
Cost-sensitive tree of classifiers. In International Conference on
Machine Learning.

Chen, M.; Weinberger, K. Q.; Chapelle, O.; Kedem, D.; and Xu,
Z. 2012. Classifier cascade for minimizing feature evaluation
cost. In International Conference on Artificial Intelligence and
Statistics, 218–226.

Xu, Z.; Kusner, M. J.; Weinberger, K. Q.; Chen, M.; and Chapelle,
O. 2014. Budgeted learning with trees and cascades. Journal of
Machine Learning Research.

Dalal, N., and Triggs, B. 2005. Histograms of oriented gradients
for human detection. In Computer Vision and Pattern Recognition, volume 1, 886–893.
Das, A., and Kempe, D. 2011. Submodular meets spectral: Greedy
algorithms for subset selection, sparse approximation and dictionary selection. International Conference on Machine Learning.
Das, A.; Dasgupta, A.; and Kumar, R. 2012. Selecting diverse
features via spectral regularization. In Advances in Neural Information Processing Systems, 1592–1600.
Diekhoff, G. 1992. Statistics for the social and behavioral sciences: univariate, bivariate, multivariate. Wm. C. Brown Publishers Dubuque, IA.
Dredze, M.; Gevaryahu, R.; and Elias-Bachrach, A. 2007. Learning
fast classifiers for image spam. In Conference on Email and
Anti-Spam.
Efron, B.; Hastie, T.; Johnstone, I.; and Tibshirani, R. 2004. Least
angle regression. The Annals of Statistics 32(2):407–499.
Golovin, D., and Krause, A. 2011. Adaptive submodularity: Theory and applications in active learning and stochastic optimization. Journal of Artificial Intelligence Research 42(1):427–486.
Grubb, A., and Bagnell, J. A. 2012. Speedboost: Anytime prediction with uniform near-optimality. In International Conference
on Artificial Intelligence and Statistics.
Hastie, T.; Tibshirani, R.; and Friedman, J. 2001. The elements of
statistical learning: data mining, inference and prediction. New
York: Springer-Verlag 1(8):371–406.
Johnson, R. A., and Wichern, D. W. 2002. Applied multivariate
statistical analysis, volume 5. Prentice hall Upper Saddle River,
NJ.
Kowalski, M. 2009. Sparse regression using mixed norms. Applied
and Computational Harmonic Analysis 27(3):303–324.
Krause, A., and Cevher, V. 2010. Submodular dictionary selection for sparse representation. In International Conference on
Machine Learning, 567–574.
Krause, A., and Guestrin, C. E. 2012. Near-optimal nonmyopic value of information in graphical models. arXiv preprint
arXiv:1207.1394.
Nemhauser, G. L.; Wolsey, L. A.; and Fisher, M. L. 1978. An analysis of approximations for maximizing submodular set functions
i. Mathematical Programming 14(1):265–294.
Streeter, M., and Golovin, D. 2007. An online algorithm for maximizing submodular functions. Technical report, DTIC Document.

Zhang, D.; He, J.; Si, L.; and Lawrence, R. 2013. Mileage: Multiple
instance learning with global embedding. International Conference on Machine Learning.
Zheng, Z.; Zha, H.; Zhang, T.; Chapelle, O.; Chen, K.; and Sun, G.
2008. A general boosting method and its application to learning ranking functions for web search. In Advances in Neural
Information Processing Systems. 1697–1704.
Zolghadr, N.; Bartók, G.; Greiner, R.; György, A.; and Szepesvári,
C. 2013. Online learning with costly features and labels. In Advances in Neural Information Processing Systems, 1241–1249.

